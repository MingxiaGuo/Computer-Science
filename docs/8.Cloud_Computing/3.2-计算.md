## 3.1 计算虚拟化


### 3.1.2 虚拟化实现方式：

#### 3.1.2.1 Hypervisor虚拟化


因内部体系结构和具体实现不同，Hypervisor所呈现的功能会有很大的差异。以下从CPU、内存、设备虚拟化3个方面，简单介绍Hypervisor内部的实现方式。

* **CPU虚拟化**

  X86操作系统被设计为直接在硬件上运行，很自然，操作系统会认为它们拥有硬件的所有控制权。x86架构为操作系统和应用程序提供了 4个权限级别（ring 0、ring，ring 2和ring 3）来管理对硬件的访问。用户程序一般运行在ring 3,而操作系统需要直接访问内存和硬件，所以必须在ring 0执行特权指令。虚拟化X86架构需要在操作系统（原本运行于最高权限ring 0）与硬件之间再增加一个虚拟层，用于为创建和管理虚拟机提供共享资源。某些敏感指令在非ringO下执行时具有不同的语义，而不能很好地被虚拟化，这使得x86虚拟化的实现更加复杂。运行时陷入并翻译这些敏感指令和特权指令是一个巨大的挑战，这使得X86架构的虚拟化起初看起来像是"不可完成的任务"。

  虚拟化发展多年，但业界还没有一个开放的标准来定义和管理虚拟化。每个公司可以根据自己的情况，设计不同的虚拟化方案来应对虚拟化的技术挑战。而处理敏感和特权指令，以实现X86架构上CPU虚拟化的技术，大体可归纳为以下3种：

  * 使用二进制翻译的全虚拟化
  * 操作系统辅助或半虚拟化
  * 硬件辅助的虚拟化

  （1）使用二进制翻译的全虚拟化
  这种方法对内核代码进行翻译，将不可虚拟化的指令替换为一串新的指令，而这串指令对虚拟化硬件可达到预期效果，如图2・3所示。同时，将用户级的代码直接运行在物理处理器上以保证虚拟化的高性能。虚拟机监控器为虚拟机提供真实的物理系统的所有服务，包括虚拟BIOS.虚拟设备和虚拟内存管理等。

  ![](./assets/使用二进制翻译的全虚拟化.jpg)

  由于虚拟机操作系统被完全抽象，通过虚拟化层与底层硬件彻底解耦，所以，二进制翻译和直接指令执行的结合提供了全虚拟化。虚拟机操作系统完全意识不到虚拟化，不需要对虚拟机系统做任何的修改。Hypervisor在运行过程中动态翻译操作系统指令，并将结果缓存以备后续使用。而用户级指令无需修改就可以运行，具有和物理机一样的执行速度。

  全虚拟化为虚拟机提供最佳的隔离性和安全性。由于同样的虚拟机实例可以运行在虚拟化环境或真实物理硬件上，所以全虚拟化也简化了虚拟机的可移植性。VMware的虚拟化产品和微软的Virtual Server就采用了全虚拟化。

  (2)操作系统辅助虚拟化或半虚拟化

  相对于全虚拟化，半虚拟化是指通过虚拟机系统和Hypervisor间的交互来改善性能和效率。半虚拟化涉及修改虚拟机操作系统内核，将不可虚拟化的指令替换为超级调用Hypercall,以便直接与虚拟化层通信，如图2-4所示。Hypervisor也为其他关键的系统操作，如内存管理、中断处理、计时等，提供超级调用接口。

  ![](./assets/半虚拟化.jpg)

  在全虚拟化中，未经修改的虚拟机系统并不知道自身被虚拟化，敏感系统调用陷入进行二进制翻译。与全虚拟化不同，半虚拟化的价值在于减少了虚拟化开销。但是半虚拟化相对于全虚拟化的性能优势与工作负载有很大的关系。由于半虚拟化不支持未经修改的操作系统(如 Windows 2OOO/XP),它的兼容性和可移植性较差。由于需要对系统内核进行深度修改，很明显，在生产环境中半虚拟化在技术支持和维护上会引入很多问题。开源的Xen项目是半虚拟化的一个例子，它使用一个经过修改的Linux内核来虚拟化处理器，而用另外一个定制的虚拟机系统的设备驱动来虚拟化l/0ₒ

  (3)硬件辅助虚拟化

  硬件厂商也迅速拥抱虚拟化并开发出硬件的新特性来简化虚拟化技术，这其中包括Intel虚拟化技术(VT-x)和AMD的AMD-V两者都为特权指令增加了新的CPU执行模式，以允许VMM在ring 0新增根模式下运行。如图2-5所示，特权和敏感调用自动陷入Hypervisor,而不需要进行二进制翻译或半虚拟化。虚拟机状态保存在虚拟机控制结构(VMCS, VT-x)或虚拟机控制块(VMCB, AMD-V)中。支持VT-x和AMD-V特性的第一代硬件辅助特性处理器在2006年发布，但第一代硬件辅助的实现采用的编程模型僵化，导致Hypervisor到虚拟机的切换开销很高，以至于硬件辅助虚拟化性能低于某些优化后的二进制翻译性能。

  ![](./assets/硬件辅助虚拟化.jpg)

  第二代硬件辅助技术做了很大的改进，包括硬件支持的内存虚拟化，如AMD的NPT(Nested Page Table)和 Intel 的 EPT (Extended Page Table),以及硬件支持的设备，和 I/O 虚拟化 Intel VT-d、AMD l0MMUₒ
* **内存虚拟化**

  内存虚拟化涉及共享系统物理内存和为虚拟机动态分配物理内存。内存虚拟化和现代操作系统对虚拟内存的支持很相似。应用程序看到的连续地址空间与底层真实物理内存并不一一对应，操作系统在页表中保存了虚拟页号与物理页号的映射关系。当前，所有的x86 CPU都采用内存管理单元(MMU)和页表缓存部件(TLB),以优化虚拟内存的性能。

  为了在一个系统上运行多个虚拟机，还需要另外一层内存虚拟化。换句话说，需要虚拟化MMU来支持虚拟机系统。虚拟机系统仍然控制虚拟机中虚拟地址到内存物理地址的映射，但虚拟机系统不能直接访问真实物理机器内存。

  在硬件辅助虚拟化出现之前，Hypervisor负责将虚拟机物理内存映射到真实的机器内存，并使用影子页表来加速映射过程。Hypervisor使用硬件中的TLB将虚拟内存直接映射到机器内存，以避免每次访问时所进行的两级转换。影子页表的引入意味着Hypervisor需要为每个客户机的每个进程的页表都维护一套相应的影子页表，这会带来内存上较大的额外开销。此外，客户机页表和和影子页表的同步也比较复杂。当虚拟机改变了虚拟内存到物理内存的映射时，Hypervisor需要更新影子页表，以备后续可以直接查找。

  Intel EPT技术和AMD NPT技术都对内存虚拟化提供了硬件支持。这两种技术原理类似，都是通过硬件方式加速客户机虚拟地址到宿主机物理地址之间的转换。以EPT为例，EPT页表实现客户机物理地址到宿主机物理地址的映射，这样就将客户机虚拟地址到宿主机物理地址的转换分解为客户机虚拟地址到客户机物理地址映射和客户机物理地址到宿主机物理地址的映射，而这两步映射都由硬件自动完成。当客户机运行时，客户机页表被载入CR3,而EPT页表被载入专门的EPT页表指针寄存器EPTPO EPT页表对地址的映射机制与客户机页表对地址的映射机制相同。EPT实现方式不需要为每个客户机的每个进程维护一套页表来进行转换映射，EPT比影子页表实现简单，且由于采用硬件实现，虚拟化性能、效率也得到大幅提升。
* **设备虚拟化**

  设备和I/O虚拟化涉及对虚拟设备和共享物理设备之间的I/O请求路径的管理。

  相较于物理硬件直通(direct pass-through),基于软件的I/O虚拟化和管理提供了更丰富的特性和更简化的管理方式。以网络为例，虚拟网卡和虚拟交换机在虚拟机之间创建虚拟网络，而不需要消耗物理网络的带宽。网卡绑定允许将多个物理网卡展现为一块网卡，在网卡故障时可以对虚拟机做到透明切换，并且，虚拟机可以通过热迁移在不同的系统间无缝迁移，而保持MAC地址不变。高效I/O虚拟化的关键是要在维持虚拟化优势的同时，最小化CPU消耗。

  Hypervisor虚拟化物理硬件为每个虚拟机呈现标准的虚拟设备。这些虚拟设备有效地模拟了所熟知的硬件，并将虚拟机的请求转换到系统物理硬件。所有虚拟机可以配置为运行在相同的虚拟硬件上，而与底层真实的系统物理硬件无关。设备驱动的一致性和标准化也进一步推动了虚拟机的标准化，增强了虚拟机在不同平台间的可移植性。

#### 3.1.2.2 容器虚拟化

容器虚拟化：充分利用操作系统自身机制和特性

容器技术：新一代虚拟化技术

- Docker(工具：Machine、Compose、Swarm，容器平台：Kubernetes，Mesos，CoreOS)
- chroot(1982,UNIX) -> Linux Container(LXC,集成到Linux内核中) -> Docker(提供容器管理工具，分层文件系统，镜像；早起Docker完全基于LXC，之后开发了libcontainer，之后Dokcer推动runC项目，使容器不局限于Linux操作系统，而是更安全、更具扩展性。)

容器也是对服务器资源进行隔离，包括CPU份额、网络I/O、带宽、存储I/O、内存等。同一台主机上的多台容器之间可以公平友好地共享资源，而不互相影响。

如今，容器是云计算的一个热门话题。在同一台服务器上部署容器，其密度相较于虚拟机可以提升约10倍。但是容器并不是一个新的技术，它至少可以追溯到2000年FreeBSD jails 的出现，而 FreeBSD jails 则是基于 1982 年 BSD UNIX 的 chroot C 命令。再往前，chroot最早源于1979年UNIX7版本。通过chroot可以改变进程和子进程所能看到的根目录，这意味着可以在指定的根目录下独立运行程序，所以说从早期的chroot中就可以看出容器的踪迹。但是chroot仅适合于运行简单的应用，往往只是一个shell程序。虽然chroot会为程序创造一个jail, jail通过虚拟对文件系统、网络等的访问，限制用户的行为，但是还是有些方法很容易发生"越狱"这使得chroot很难应用于大型复杂系统。

SUN利用了 jail的概念，将其演化成Solaris Zones。但这一技术是Solaris特有的，所以虽然我们可以在Zone中运行Solaris应用或者一个更早版本的Solaris,但是无法在AIX或者Linux中运用这一技术。

在 Solaris 基于 FreeBSD jail 开发 Solaris Zone 的同时，Google、RedHat、 Canonical等公司也基于Linux进行了容器的相关研究。Parallels在2001年研发了 Virtuozzo,并获得了一定的商业成功。Parallels Virtuozzo在2005年演变为OpenVZ 其后又作为LXC开源进入Linux内核。而Google于2013年开源了 Imctfy项目，虽然Google容器项目开源得很晚，但事实上，Parallels. RedHat以及Google自身的Imctfy项目都是依托于Google的cgroup技术。cgroup技术使得开发者可以进一步抽象系统资源，增强了 Linux系统安全性。Google内部也一直在使用容器支持日常的公司运作，甚至支持Google Doc、Gmailₛ Google Search等商业应用。Google每周要运行约20亿个容器。

但是，对于大部分公司，容器还是一个神秘甚至有些令人畏惧的技术。直到Docker的出现才改变了业界开发、运维模式。Docker使得人们认识了又一个开源容器项目libcontainer, Docker自身也成为了 Linux容器的事实标准。

容器虚拟化和Hypervisor虚拟化的差别在于，容器虚拟化没有Hypervisor层，容器间相互隔离，但是容器共享操作系统，甚至bins/libs,如图2·6所示。每个容器不是独立的操作系统，所以容器虚拟化没有冗余的操作系统内核及相应的二进制库等，这使得容器部署、启动的开销几乎为零，且非常迅速。

![](../../Cloud_Computing/assets/容器虚拟化原理.jpg)

容器是非常轻量的。容器内进程可以是一个Linux操作系统，可以是一个三层架构的Web应用，也可以是一个数据库后端，甚至是一个简单的hello world程序。

容器使用的主要内核特性如下。

* namespace

  容器虚拟化利用namespace提供容器间的隔离。当运行容器时，容器虚拟化为容器创建一组namespaceₒ容器在namespace中运行，不会超出namespace指定的范围。

  容器使用了以下namespaceₒ

  * Pid namespace：提供进程隔离。
  * Net namespace：管理容器网络接口，实现网络隔离。
  * IPC namespace：提供容器进程间通信隔离，例如不同的容器采用不同的消息队列。
  * Mnt namespace：允许不同的容器看到的文件目录不同。
  * Uts namespace：允许容器有独立的hostname和domain nmme,这使得容器在网络上可以作为一个独立节点而非一个进程呈现。
* cgroup

  cgroup是Google贡献的一个项目，目的是通过内核技术对共享资源进行分配、限制及管理。容器利用cgroup为每个容器分配CPU、内存以及blkio等资源，并对其使用容量进行限制。通过引入cgroup,可以在同一主机的多台容器间公平、友好地共享资源，避免了因某些容器资源滥用而导致其他虚拟机甚至主机性能受到显著影响。

容器得到了一些初创公司的关注，如Docker. CoreOS. Shippable等公司，同时也受到了很多大公司的热捧。Google基于之前Borg系统的经验开发了 Kubernetes容器管理系统，IBM在其BlueMix PaaS平台支持Docker, Amazon在其弹性云上开放容器服务，HP、微软、RedHat也在容器领域做了相应工作。

容器面临的最大的挑战是安全问题。目前已经有一些安全产品，如RedHat的SELinux增强Linux安全级别，Ubuntu的AppArmor针对应用设定访问控制属性。但是还需要进一步加强内核安全，在多租户环境中将入侵者阻挡在容器外。

#### 3.1.2.3 大型主机虚拟化：IBM

### 3.1.3 KVM

KVM最初是由一个名为Qumranet的以色列小公司开发的，2008年9月由RedHat收购。但当时的虚拟化市场上主要以VMware为主，KVM没有受到太多关注。2011年，为了避免VMware 一家独大, IBM和RedHat,联合惠普和英特尔成立了开放虚拟化联盟(Open Virtualization Alliance),使得KVM快速发展起来。

KVM全称为Kernel based virtual machine,如图2-7所示。从命名可以看出，KVM采用的是裸金属架构，并且是基于Linux内核的。KVM利用Linux操作系统 并在其上扩展了一个**kvm.ko内核模块，由它提供虚拟化能力。**

通过KVM可以创建和运行多个虚拟机。而在KVM架构中，虚拟机被实现为一个普通的**Qemu-kvm进程**，由Linux标准调度器进行调度。事实上**每个虚拟CPU都呈现为一个常规Linux进程**，这就使得KVM可以使用Linux内核的所有特性。

**设备模拟通过一个修改后的Qemu提供**，包括对BIOS、PCI总线、USB总线，以及标准设备集如IDE和SCSI磁盘控制器、网卡等的模拟。

在KVM中通过用户态与内核协作完成虚拟化。从用户角度看，KVM是一个典型的Linux字符设备。用户可以通过ioctl函数对/dev/kvm设备节点进行操作，如创建、运行虚拟机。

![](./assets/KVM架构.jpg)

通过/dev/kvm提供的操作如下：

* 创建虚拟机。
* 为虚拟机分配内存。
* 读写虚拟CPU寄存器。
* 向虚拟CPU注入中断。
* 运行虚拟机。

KVM采用硬件辅助虚拟化技术，CPU运行时有3种模式：用户模式、内核模式和客户模式。

客户操作流程如图2-8所示。描述如下：

![](../../Cloud_Computing/assets/客户操作流程.jpg)

1） 在最外层，用户首先通过ioctl （）调用内核，触发CPU进入客户（Guest）模式，执行客户机代码，直到I/O指令或者外部事件（如网络包到达、定时器超时等）发生。对于CPU,外部事件表现为中断。

2） 在内核层，内核引起硬件进入客户模式。如果由于外部中断、影子页表缺失等事件，CPU退出客户模式，内核执行必要的处理，之后继续客户机执行。如果退出原因为I/O指令或者队列中断信号，则内核退出到用户态。

3） 在最内层，CPU执行客户代码，直到由于指令退出客户态。

KVM在Linux内核的基础上添加了虚拟机管理模块，由于**借用原生Linux CPU调度和内存管理机制，KVM实现非常轻量**。并且在kernel机制向前发展的同时，KVM也能获益。**由于采用了最新的Linux机制，并且依赖于x86硬件辅助虚拟化，通过CPU vt-x、内存ept技术等，KVM的性能也呈现出了较好的表现。**





### 3.1.6 Docker

随着互联网的发展，分布式应用越来越普遍，应用提供商们期望，不管有多少用户，不论在何种设备上，这些程序都能随时随地可用和运行。至于应用程序，它们必须具备足够的弹性、互操作性，并可以被大规模扩展。此外，越来越多的应用提供商不仅要求满足当前需求，还期望建立复杂且可
被广泛采用的下一代应用程序。

这个工作最大的挑战在于一应用程序不再只运行在一台计算机上。解 决方案层面要求将逻辑软件组件和底层基础设施拆分。当主机故障、升级或 者在其他环境中重新部署时，服务独立于之前的基础设施环境，始终随处可 用。

而其首要解决的问题就是封装和分发，若没有一致的封装方法，将同一个软件部署在各类不同的操作系统、设备、数据中心，必然会产生大量不稳定因素。因此，需要通过现存技术打造一个标准格式：一个始终一致的容器 —可以移动，可随时运行的Docker容器。第二个问题就是如何在不同的机器上执行这些容器，并产生一致的、可预期的结果。这两个问题定义了Docker发布前的大部分工作，也说明了 Docker成功带来的价值。

Docker 采用 Client-Server 架构，如图 2-11 所示。Docker Deamon 作为服务器，部署在每台主机上，支持主机上所有容器的分发、创建和运行。Docker客户端与Docker Deamon通过Socket或者Restful API进行交互。Docker客户端和Daemon可以部署在同一台主机上，也可以甶Docker客户
端远程访问Docker Daemon。
<<<<<<< HEAD:docs/Cloud_Computing/3.2-计算.md
`<img src="/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/assets/docker架构.png" style="zoom:50%;" />`
=======
<img src="../../Cloud_Computing/assets/docker架构.png" style="zoom:50%;" />
>>>>>>> 3d58ba2deaf818cf931014cb0dda15ebbfcf4894:docs/Cloud_Computing/3-云计算技术/3.2-计算.md

Docker包括以下主要部件。

1. Docker Daemon
   Docker Daemon运行在服务器上，但用户并不直接和Docker Daemon通信，而是通过
   Docker 客户端与 Docker Daemon 交互。
2. Docker客户端
   Docker客户端是Docker的主要用户接口，Docker客户端接收用户请求，交给后端Docker Daemon，并将响应返回给用户。
3. Docker 镜像
   Docker镜像和虛拟机镜像一样，是一个只读模板。一个Web迨用的Docker锖像可以是一个已经安装了 Apache和Web应用的Ubuntu操作系统。Docker镜像可以用于创建Docker容器。Docker提供了简单的方式构建新镜像和更新已有镜像，ffl户也可以从Docker Registry下载其用户已创建好的镜像。Docker镜像是Docker的构建组件。
4. Docker Registry
   Docker Registry存储并管理Docker镜像。用户可以M公共或者私有存储上传或T载Docker镜像。公共 Docker Registry 成为 Docker Hub。Docker Registry提供大量已构建好的镜像供用户使用，这些镜像可以是由用户自己创建的，也可以是由其他用户创建并共享的。 Docker Registry是Docker的分发部件。
5. Docker 容器
   Docker容器像一个目录。Docker容器包含了运行一个应用需要的所有环境。每个容器从
   Docker镜像创建而产生。Docker容器的生命周期包括运行、启动、停止、移动和删除。每个容器是一个隔离的、安全的应用平台。Docker容器是Docker的运行部件。

下面，我们以Docker run命令为例对Docker流程进行描述。

1. 用 户 在Docker客户端执行以下命令：
   ``$ docker run -i -t ubuntu /bin/bash`` Docker 客户端与 Docker Daemon 交互。
2. Docker Daemon检查宿主机上是否有Ubuntu镜像，如果没有则从Docker registries下载 Docker Ubuntu 镜像
3. Docker Daemon 从 Ubuntu 镜像创建Docker容器，并为其分配文件系统， mount —个可读写镜像层，之后Docker Daemon为容器分配网卡资源，设置IP。
4. Docker Daemon 启动应用，自此 Docker容器完成启动过程。

用户通过Docker客户端管理容器，也可以与容器中的应用进行交互。当容器运行结束后，用户通过Docker客户端删除容器。

Docker所依赖的底层技术除了 namespace、cgroup特性，还有Netlink、Capability等其他内核特性。所以，Docker本质上是一个上层管理工具，如图2-12所示。通过对Linux内核的namespace、cgroup、netlink 等特性的调用和整合，实现了应用逻辑与底层基础设施的解耦。



![](./assets/docke与linux内核关系.png)
Docker让容器虚拟化真正走入人们的视线，而Docker的成功离不开对联合文件系统（union file system)的引入。联合文件系统通过创建镜像层，将具有不同文件结构的镜像层进行叠加挂载，使它们看上去像一个文件系统。由于通过多个镜像层叠加挂载，所以联合文件系统非常轻量。通过联合文件系统，在大规模部署场景，Docker可以对容器快速复制、重建及更新。Docker可利用的联合文件系统包括AUFS、btrfs、vfs、devicemapper等。目前AUFS提供的功能最全面，使用最广泛。

### 3.1.7 计算虚拟化技术对比及应用场景

Hypervisor虚拟化技术对比：

![](./assets/hypervisor技术对比.jpg)

容器与hypervisor虚拟化技术的核心技术相去甚远

![](./assets/容器与hypervisor对比.jpg)

