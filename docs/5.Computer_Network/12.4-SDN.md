# 软件定义网络

13，14年提出的网络概念，现已成熟，实现方法现在也有很多，IBM使用overlay、underlay技术实现

上一节我们说到，使用原生的VLAN和Linux网桥的方式来进行云平台的管理，但是这样在灵活性、隔离性方面都显得不足，而且整个网络缺少统一的视图、统一的管理。

可以这样比喻，云计算就像大家一起住公寓，要共享小区里面的基础设施，其中网络就相当于小区里面的电梯、楼道、路、大门等，大家都走，往往会常出现问题，尤其在上班高峰期，出门的人太多，对小区的物业管理就带来了挑战。

物业可以派自己的物业管理人员，到每个单元的楼梯那里，将电梯的上下行速度调快一点，可以派人将隔离健身区、景色区的栅栏门暂时打开，让大家可以横穿小区，直接上地铁，还可以派人将多个小区出入口，改成出口多、入口少等等。等过了十点半，上班高峰过去，再派人都改回来。

## 软件定义网络（SDN）

这种模式就像传统的网络设备和普通的Linux网桥的模式，配置整个云平台的网络通路，你需要登录到这台机器上配置这个，再登录到另外一个设备配置那个，才能成功。

如果物业管理人员有一套智能的控制系统，在物业监控室里就能看到小区里每个单元、每个电梯的人流情况，然后在监控室里面，只要通过远程控制的方式，拨弄一个手柄，电梯的速度就调整了，栅栏门就打开了，某个入口就改出口了。

这就是软件定义网络（SDN）。它主要有以下三个特点。

![img](https://static001.geekbang.org/resource/image/34/f9/346fe3b3dbe1024e7119ec4ffa9377f9.jpg)

- **控制与转发分离**：转发平面就是一个个虚拟或者物理的网络设备，就像小区里面的一条条路。控制平面就是统一的控制中心，就像小区物业的监控室。它们原来是一起的，物业管理员要从监控室出来，到路上去管理设备，现在是分离的，路就是走人的，控制都在监控室。
- **控制平面与转发平面之间的开放接口**：控制器向上提供接口，被应用层调用，就像总控室提供按钮，让物业管理员使用。控制器向下调用接口，来控制网络设备，就像总控室会远程控制电梯的速度。这里经常使用两个名词，前面这个接口称为**北向接口**，后面这个接口称为**南向接口**，上北下南嘛。
- **逻辑上的集中控制**：逻辑上集中的控制平面可以控制多个转发面设备，也就是控制整个物理网络，因而可以获得全局的网络状态视图，并根据该全局网络状态视图实现对网络的优化控制，就像物业管理员在监控室能够看到整个小区的情况，并根据情况优化出入方案。

## OpenFlow和OpenvSwitch

SDN有很多种实现方式，我们来看一种开源的实现方式。

OpenFlow是SDN控制器和网络设备之间互通的南向接口**协议**，OpenvSwitch用于创建软件的虚拟交换机。OpenvSwitch是支持OpenFlow协议的，当然也有一些硬件交换机也支持OpenFlow协议。它们都可以被统一的SDN控制器管理，从而实现物理机和虚拟机的网络连通。

![img](https://static001.geekbang.org/resource/image/38/d0/383710afd8e2b7e99ba8ccfef69c02d0.jpg)

SDN控制器是如何通过OpenFlow协议控制网络的呢？

![img](https://static001.geekbang.org/resource/image/ed/f7/ed8939e33728c6118db3c0283b1dbdf7.jpg)

在OpenvSwitch里面，有一个流表规则，任何通过这个交换机的包，都会经过这些规则进行处理，从而接收、转发、放弃。

那流表长啥样呢？其实就是一个个表格，每个表格好多行，每行都是一条规则。每条规则都有优先级，先看高优先级的规则，再看低优先级的规则。

![img](https://static001.geekbang.org/resource/image/91/77/91a7f011885ec48ca57d61940b748477.jpg)

对于每一条规则，要看是否满足匹配条件。这些条件包括，从哪个端口进来的，网络包头里面有什么等等。满足了条件的网络包，就要执行一个动作，对这个网络包进行处理。可以修改包头里的内容，可以跳到任何一个表格，可以转发到某个网口出去，也可以丢弃。

通过这些表格，可以对收到的网络包随意处理。

![img](https://static001.geekbang.org/resource/image/34/22/342ac21bda24e20d8d78f47ca8415a22.jpg)

具体都能做什么处理呢？通过上面的表格可以看出，简直是想怎么处理怎么处理，可以覆盖TCP/IP协议栈的四层。

对于物理层：

- 匹配规则包括由从哪个口进来；
- 执行动作包括从哪个口出去。

对于MAC层：

- 匹配规则包括：源MAC地址是多少？（dl_src），目标MAC是多少？（dl_dst），所属vlan是多少？（dl_vlan）；
- 执行动作包括：修改源MAC（mod_dl_src），修改目标MAC（mod_dl_dst），修改VLAN（mod_vlan_vid），删除VLAN（strip_vlan），MAC地址学习（learn）。

对于网络层：

- 匹配规则包括：源IP地址是多少？(nw_src)，目标IP是多少？（nw_dst）。
- 执行动作包括：修改源IP地址（mod_nw_src），修改目标IP地址（mod_nw_dst）。

对于传输层：

- 匹配规则包括：源端口是多少？（tp_src），目标端口是多少？（tp_dst）。
- 执行动作包括：修改源端口（mod_tp_src），修改目标端口（mod_tp_dst）。

总而言之，对于OpenvSwitch来讲，网络包到了我手里，就是一个Buffer，我想怎么改怎么改，想发到哪个端口就发送到哪个端口。

OpenvSwitch有本地的命令行可以进行配置，能够实验咱们前面讲过的一些功能。我们可以通过OpenvSwitch的命令创建一个虚拟交换机。然后可以将多个虚拟端口port添加到这个虚拟交换机上。

```
ovs-vsctl add-br ubuntu_br
```

## 实验一：用OpenvSwitch实现VLAN的功能

下面我们实验一下通过OpenvSwitch实现VLAN的功能，在OpenvSwitch中端口port分两种。

第一类是access port：

- 这个端口配置tag，从这个端口进来的包会被打上这个tag；
- 如果网络包本身带有的VLAN ID等于tag，则会从这个port发出；
- 从access port发出的包不带VLAN ID。

第二类是trunk port：

- 这个port不配置tag，配置trunks；
- 如果trunks为空，则所有的VLAN都trunk，也就意味着对于所有VLAN的包，本身带什么VLAN ID，就是携带者什么VLAN ID，如果没有设置VLAN，就属于VLAN 0，全部允许通过；
- 如果trunks不为空，则仅仅带着这些VLAN ID的包通过。

我们通过以下命令创建如下的环境：

```
ovs-vsctl add-port ubuntu_br first_br
ovs-vsctl add-port ubuntu_br second_br
ovs-vsctl add-port ubuntu_br third_br
ovs-vsctl set Port vnet0 tag=101
ovs-vsctl set Port vnet1 tag=102
ovs-vsctl set Port vnet2 tag=103
ovs-vsctl set Port first_br tag=103
ovs-vsctl clear Port second_br tag
ovs-vsctl set Port third_br trunks=101,102
```

另外要配置禁止MAC地址学习。

```
ovs-vsctl set bridge ubuntu_br flood-vlans=101,102,103
```

![img](https://static001.geekbang.org/resource/image/6e/b4/6e1ddc1eb92c85cda32f40b62dd9fcb4.jpg)

创建好了环境以后，我们来做这个实验。

1. 从192.168.100.102来ping 192.168.100.103，然后用tcpdump进行抓包。first_if收到包了，从first_br出来的包头是没有VLAN ID的。second_if也收到包了，由于second_br是trunk port，因而出来的包头是有VLAN ID的，third_if收不到包。
2. 从192.168.100.100来ping 192.168.100.105, 则second_if和third_if可以收到包，当然ping不通，因为third_if不属于某个VLAN。first_if是收不到包的。second_if能够收到包，而且包头里面是VLAN ID=101。third_if也能收到包，而且包头里面是VLAN ID=101。
3. 从192.168.100.101来ping 192.168.100.104， 则second_if和third_if可以收到包。first_if是收不到包的。second_br能够收到包，而且包头里面是VLAN ID=102。third_if也能收到包，而且包头里面是VLAN ID=102。

通过这个例子，我们可以看到，通过OpenvSwitch，不用买一个支持VLAN的交换机，你也能学习VLAN的工作模式了。

## 实验二：用OpenvSwitch模拟网卡绑定，连接交换机

接下来，我们来做另一个实验。在前面，我们还说过，为了高可用，可以使用网卡绑定，连接到交换机，OpenvSwitch也可以模拟这一点。

在OpenvSwitch里面，有个bond_mode，可以设置为以下三个值：

- active-backup：一个连接是active，其他的是backup，当active失效的时候，backup顶上；
- balance-slb：流量安装源MAC和output VLAN进行负载均衡；
- balance-tcp：必须在支持LACP协议的情况下才可以，可根据L2, L3, L4进行负载均衡。

我们搭建一个测试环境。

![img](https://static001.geekbang.org/resource/image/8a/6c/8a1956cb5bbf03de7d6cbaa2e706046c.jpg)

我们使用下面的命令，建立bond连接。

```
ovs-vsctl add-bond br0 bond0 first_br second_br
ovs-vsctl add-bond br1 bond1 first_if second_if
ovs-vsctl set Port bond0 lacp=active
ovs-vsctl set Port bond1 lacp=active
```

默认情况下bond_mode是active-backup模式，一开始active的是first_br和first_if。

这个时候我们从192.168.100.100 ping 192.168.100.102，以及从192.168.100.101 ping 192.168.100.103的时候，tcpdump可以看到所有的包都是从first_if通过。

如果把first_if设成down，则包的走向会变，发现second_if开始有流量，对于192.168.100.100和192.168.100.101似乎没有收到影响。

如果我们通过以下命令，把bond_mode设为balance-slb。然后我们同时在192.168.100.100 ping 192.168.100.102，在192.168.100.101 ping 192.168.100.103，我们通过tcpdump发现包已经被分流了。

```
ovs-vsctl set Port bond0 bond_mode=balance-slb
ovs-vsctl set Port bond1 bond_mode=balance-slb
```

通过这个例子，我们可以看到，通过OpenvSwitch，你不用买两台支持bond的交换机，也能看到bond的效果。

那OpenvSwitch是怎么做到这些的呢？我们来看OpenvSwitch的架构图。

![img](https://static001.geekbang.org/resource/image/d8/14/d870e5bfcad8ec45d146c3226cdccb14.jpg)

OpenvSwitch包含很多的模块，在用户态有两个重要的进程，也有两个重要的命令行工具。

- 第一个进程是OVSDB进程。ovs-vsctl命令行会和这个进程通信，去创建虚拟交换机，创建端口，将端口添加到虚拟交换机上，OVSDB会将这些拓扑信息保存在一个本地的文件中。
- 第一个进程是vswitchd进程。ovs-ofctl命令行会和这个进程通信，去下发流表规则，规则里面会规定如何对网络包进行处理，vswitchd会将流表放在用户态Flow Table中。

在内核态，OpenvSwitch有内核模块OpenvSwitch.ko，对应图中的Datapath部分。在网卡上注册一个函数，每当有网络包到达网卡的时候，这个函数就会被调用。

在内核的这个函数里面，会拿到网络包，将各个层次的重要信息拿出来，例如：

- 在物理层，in_port即包进入的网口的ID；
- 在MAC层，源和目的MAC地址；
- 在IP层，源和目的IP地址；
- 在传输层，源和目的端口号。

在内核中，有一个内核态Flow Table。接下来内核模块在这个内核流表中匹配规则，如果匹配上了，则执行操作、修改包，或者转发或者放弃。如果内核没有匹配上，则需要进入用户态，用户态和内核态之间通过Linux的一个机制Netlink相互通信。

内核通过upcall，告知用户态进程vswitchd在用户态Flow Table里面去匹配规则，这里面的规则是全量的流表规则，而内核Flow Table里面的只是为了快速处理，保留了部分规则，内核里面的规则过一阵就会过期。

当在用户态匹配到了流表规则之后，就在用户态执行操作，同时将这个匹配成功的流表通过reinject下发到内核，从而接下来的包都能在内核找到这个规则。

这里调用openflow协议的，是本地的命令行工具，也可以是远程的SDN控制器，一个重要的SDN控制器是OpenDaylight。

下面这个图就是OpenDaylight中看到的拓扑图。是不是有种物业管理员在监控室里的感觉？

![img](https://static001.geekbang.org/resource/image/27/a8/274442ba251fdc63c88bc5dbfc6183a8.jpg)

我们可以通过在OpenDaylight里，将两个交换机之间配置通，也可以配置不通，还可以配置一个虚拟IP地址VIP，在不同的机器之间实现负载均衡等等，所有的策略都可以灵活配置。

## 如何在云计算中使用OpenvSwitch？

OpenvSwitch这么牛，如何用在云计算中呢？

![img](https://static001.geekbang.org/resource/image/24/59/24b09861632f7ba7211073e2829d4f59.jpg)

我们还是讨论VLAN的场景。

在没有OpenvSwitch的时候，如果一个新的用户要使用一个新的VLAN，还需要创建一个属于新的VLAN的虚拟网卡，并且为这个租户创建一个单独的虚拟网桥，这样用户越来越多的时候，虚拟网卡和虚拟网桥会越来越多，管理非常复杂。

另一个问题是虚拟机的VLAN和物理环境的VLAN是透传的，也即从一开始规划的时候，就需要匹配起来，将物理环境和虚拟环境强绑定，本来就不灵活。

而引入了OpenvSwitch，状态就得到了改观。

首先，由于OpenvSwitch本身就是支持VLAN的，所有的虚拟机都可以放在一个网桥br0上，通过不同的用户配置不同的tag，就能够实现隔离。例如上面的图，用户A的虚拟机都在br0上，用户B的虚拟机都在br1上，有了OpenvSwitch，就可以都放在br0上，只是设置了不同的tag。

另外，还可以创建一个虚拟交换机br1，将物理网络和虚拟网络进行隔离。物理网络有物理网络的VLAN规划，虚拟机在一台物理机上，所有的VLAN都是从1开始的。由于一台机器上的虚拟机不会超过4096个，所以VLAN在一台物理机上如果从1开始，肯定够用了。

例如在图中，上面的物理机里面，用户A被分配的tag是1，用户B被分配的tag是2，而在下面的物理机里面，用户A被分配的tag是7，用户B被分配的tag是6。

如果物理机之间的通信和隔离还是通过VLAN的话，需要将虚拟机的VLAN和物理环境的VLAN对应起来，但为了灵活性，不一定一致，这样可以实现分别管理物理机的网络和虚拟机的网络。好在OpenvSwitch可以对包的内容进行修改。例如通过匹配dl_vlan，然后执行mod_vlan_vid来改进进出出物理机的网络包。

尽管租户多了，物理环境的VLAN还是不够用，但是有了OpenvSwitch的映射，将物理和虚拟解耦，从而可以让物理环境使用其他技术，而不影响虚拟机环境，这个我们后面再讲。

## 小结

好了，这一节就到这里了，我们来总结一下：

- 用SDN控制整个云里面的网络，就像小区保安从总控室管理整个物业是一样的，将控制面和数据面进行了分离；
- 一种开源的虚拟交换机的实现OpenvSwitch，它能对经过自己的包做任意修改，从而使得云对网络的控制十分灵活；
- 将OpenvSwitch引入了云之后，可以使得配置简单而灵活，并且可以解耦物理网络和虚拟网络。

最后，给你留两个思考题：

1. 在这一节中，提到了通过VIP可以通过流表在不同的机器之间实现复杂均衡，你知道怎样才能做到吗？
2. 虽然OpenvSwitch可以解耦物理网络和虚拟网络，但是在物理网络里面使用VLAN，数目还是不够，你知道该怎么办吗？



1. 可以通过ovs-ofctl下发流表规则，创建group，并把端口加入group中，所有发现某个地址的包在两个端口之间进行负载均衡。

```
sudo ovs-ofctl -O openflow11 add-group br-lb "group_id=100 type=select selection_method=dp_hash bucket=output:1 bucket=output:2"
sudo ovs-ofctl -O openflow11 add-flow br-lb "table=0,ip,nw_dst=192.168.2.0/24,actions=group:100"
```



请问刘老师，大规模环境中的sdn控制器是一个独立实例或独立集群来实现么？主流控制器有哪些？另外流表要在每一台宿主机保存么？那大小限制的问题如何解决？能否独立集中存放流表 SDN控制器是什么东西？

SDN控制器是一个独立的集群，主要是在管控面，因为要实现一定的高可用性。

主流的开源控制器有OpenContrail、OpenDaylight等。当然每个网络硬件厂商都有自己的控制器，而且可以实现自己的私有协议，进行更加细粒度的控制，所以江湖一直没有办法统一。

流表是在每一台宿主机上保存的，大小限制取决于内存，而集中存放的缺点就是下发会很慢。





---

## 精选留言

- 

  勤劳的小胖子-libo

  等二个是通过overlay技术，比如vxlan, 但需要使用tun 而不是tap

  2018-07-13 15:52

- 

  Jobs

  从 192.168.100.100 来 ping 192.168.100.105,为何ping不通，third_if却能收到包？
  另外这里为啥要禁止mac地址学习？

  

  2018-11-13 15:25

- 

  Jobs

  刘老师您好！我现在工作中正在研究Linux 上的VM，即qemu-kvm，职业方向是不是也可以不断往云计算去进阶呢？这两年顺着你这几天及将来的文章不断深入细节去研究就可以了吗

  2018-07-13 09:15

  作者回复

  我的只是个开端

  2018-07-13 17:17

- 

  我是谁

  first_if 这个是什么，怎么产生的？

  2018-12-07 20:09

- 

  许森森

  实验一中的配置文件在哪里下啊？
  ovs-vsctl set Port vnet0 tag=101

  2018-12-03 15:29

- 

  楊_宵夜

  超哥, 两个问题:
  在 [实验一 用 OpenvSwitch 实现 VLAN 的功能] 中;

  -- 问题一&引用一
  \2. 从192.168.100.100来ping192.168.100.105, 则second_if和third_if可以收到包; 当然ping不通,
  因为third_if不属于某个VLAN...
  \--
  我读上下文, 没有发现third_if是属于哪个VLAN呢?
  并且也没看出来 100.103 和 100.104 的VLAN分别是什么;

  --问题二&引用一
  ovs-vsctl add-port ubuntu_br first_br
  ovs-vsctl set Port vnet0 tag=101
  ovs-vsctl set Port first_br tag=103
  \--
  可以看到端口first_br是需要先add的;
  为什么vnet0不需要先add呢?

  2018-11-20 15:38

  作者回复

  在配置文件里面配置了，vnet创建完毕就已经在ovs上了

  2018-11-20 17:13

- 

  我爱探索

  OpenDaylight和openvswtch是否自由开源并很好的扩展Linux系统的网络能力

  2018-10-18 21:41

- 
# My notes

网络虚拟化技术：
是一种将物理网络资源划分为多个虚拟网络的技术。它可以将一组物理网络资源（例如路由器、交换机、链路和带宽）划分为多个独立的、虚拟的网络实例，每个实例都具有自己的逻辑拓扑、路由策略和安全策略。这种虚拟化技术可以帮助网络管理员更高效地利用网络资源，提高网络的可扩展性、可靠性和安全性。

NFV：


以下是几种常见的网络虚拟化技术：
1.  虚拟局域网（VLAN）：VLAN是一种在物理网络上创建多个逻辑网络的技术。它通过将不同的网络设备和端口分组到不同的虚拟网络中来实现隔离和管理。
2.  虚拟专用网络（VPN）：VPN是一种利用公共网络（如Internet）建立安全的私有网络的技术。它可以将不同的物理网络资源（例如路由器、交换机、链路和带宽）组合成一个虚拟网络，从而为不同的用户提供安全的网络连接。
3.  虚拟路由器：虚拟路由器是一种在单个物理路由器上创建多个逻辑路由器的技术。它可以将路由器的硬件资源（如CPU和内存）划分为多个独立的虚拟实例，从而为不同的用户提供独立的路由器功能和管理。
4.  虚拟交换机：虚拟交换机是一种在单个物理交换机上创建多个逻辑交换机的技术。它可以将交换机的端口、带宽和其他资源划分为多个独立的虚拟实例，从而为不同的用户提供独立的交换机功能和管理。
5.  软件定义网络（SDN）：SDN是一种基于虚拟化的网络架构，通过将网络控制平面和数据平面分离来实现网络虚拟化。SDN可以将不同的物理网络资源组合成一个虚拟网络，从而为不同的用户提供独立的网络功能和管理。

SDN：Software Defined Network， 一种新型网络架构，一种**基于虚拟化**的网络架构，通过将网络控制面与数据面分离，使网络管理人员能够更加灵活地控制网络。从而使网络管理人员能够在一个集中的控制器上对整个网络进行统一管理和控制，大大简化了网络管理的复杂度。SDN还具有其他优点，例如可编程性、灵活性、可扩展性和安全性等。它已经被广泛应用于数据中心网络、企业网络和电信网络等领域。

SDN架构：
1.  中央控制器：中央控制器是SDN网络的核心组件，它负责管理整个网络，包括控制器本身、网络拓扑、流表、策略等。
2.  数据面设备：数据面设备是指网络中的交换机或路由器等网络设备，它们负责实际的数据传输和处理。
3.  协议：SDN网络需要一些协议来实现数据面和控制面之间的通信，例如OpenFlow、NETCONF、YANG等。
4.  应用程序接口（API）：API是SDN应用程序与中央控制器之间的接口，通过API，应用程序可以访问控制器中的网络状态、流表和策略等信息，并对其进行操作。
![img](https://static001.geekbang.org/resource/image/34/f9/346fe3b3dbe1024e7119ec4ffa9377f9.jpg)


SDN的实现方式：
1.  基于OpenFlow的SDN：这是最常见的SDN实现方式，它使用OpenFlow协议来实现控制面和数据面之间的通信。
2.  基于NETCONF和YANG的SDN：这种SDN实现方式使用NETCONF协议来管理网络设备，并使用YANG来描述网络配置和状态信息。
3.  基于BGP的SDN：这种SDN实现方式使用BGP协议来实现网络控制，通过BGP扩展来定义网络策略和路由规则。
4. 总的来说，SDN的实现方式非常多样化，不同的实现方式适用于不同的网络场景和需求。
## OpenFlow
OpenFlow:
* 一种开放式协议，南向接口的一种协议。它允许网络管理员通过控制器来配置和管理网络中的交换机或路由器等数据面设备。实现数据面和控制面之间的通信, 主要为了实现二层交换和三层路由，是一种集中式路由控制协议，与之可比较的分布式协议是BGP或OSPF协议，

## Open vSwitch
Open vSwitch:
* 一个开源的虚拟交换机，支持OpenFlow协议和其他虚拟化技术。它是一个多层的虚拟交换机，能够在虚拟机和物理机之间提供网络

可以用于在虚拟化环境中管理虚拟机之间的网络通信。它提供了一系列的网络功能，包括二层交换、虚拟机间的流量控制、VLAN支持、GRE和VXLAN隧道等。Open vSwitch可以在虚拟化平台中与KVM、Xen和VMware等虚拟化技术集成使用。

Open vSwitch是由开源社区开发的，其代码库和技术文档都是公开的。它支持多种开源虚拟化平台，包括OpenStack和CloudStack等。Open vSwitch可以与其他网络设备和服务集成，例如路由器、负载均衡器、防火墙和监控工具等。它还提供了一些高级的网络功能，例如流量镜像、QoS、流量分类和流量过滤等，使得网络管理员可以更加灵活地控制虚拟机之间的网络通信。

Open vSwitch的优点在于它的灵活性和可扩展性。它可以用于管理大规模虚拟机的网络通信，并提供了一些高级的网络功能，同时还支持多种虚拟化平台和开源工具的集成。它的开源性和可自定义性也使得它成为了一个广泛使用的虚拟交换机。

Open vSwitch 是一种软件交换机，它的原理是将物理服务器上的多个虚拟机连接起来，形成一个虚拟交换机，从而实现虚拟机之间的通信。

具体来说，Open vSwitch 可以在主机上创建一个虚拟交换机，将多个虚拟机连接到这个交换机上，从而形成一个虚拟网络。当虚拟机需要与其他虚拟机通信时，Open vSwitch 会通过虚拟端口将虚拟机之间的网络流量进行转发。

## DPDK
网络协议栈：
* arp
* ether
* ip
* icmp
* udp
* tcp
dpdk组件
* mp
* aci
* kni
* timer
* bpf
* mnpf/mempool
dpdk经典项目：
* DNS
* gateway
* nat
* firewall
* switch
* pktgen
dpdk 框架
* vpp
* ovs
* nff-go (golang)
* snabb (lua)
* spdk (c)
dpdk源码
* 内核驱动
	* lgb_uio
	* vfio
	* kni
* 内存
	* mempool
	* mbuf
* 协议
	* ipsec
	* bpf
	* pci
	* flow_classify
* 虚拟化
	* vhost
	* virtio
* cpu
	* affinity
	* rcu
	* sched
* 安全
	* security
	* cryptodev
	* compressdev
dpdk性能
* 性能指标
	* 吞吐量bps
	* 折链/建链 pps
	* 并发
	* 时延
	* 丢包率
* 测试方法
	* 测试用例
	* vpp sandbox
	* perf3灌包
	* rfc2544
* 测试工具
	* perf3
	* trex
	* testpmd
	* pktgen-dpdk

# DPDK
DPDK: Date plane development kit，是一个用来进行包数据处理加速的软件库。DPDK是INTEL公司开发的一款高性能的网络驱动组件，主要技术有用户态、轮询取代中断、零拷贝、网卡RSS、访存DirectIO等。
提供了一个用户空间的数据包处理框架，实现高性能的数据包处理，通常用于构建高性能网络应用程序。
通过使用大页内存，绕过操作系统内核，直接与硬件设备交互，从而提供了极高的数据包处理性能和低延迟。DPDK适用于网络设备如网卡、交换机、路由器等的数据包处理，以及各种网络应用程序如防火墙、负载均衡、虚拟化、CDN等的网络数据流量处理

## DPDK特点
与传统的数据包处理相比，DPDK具有以下特点：
* [轮询](https://so.csdn.net/so/search?q=%E8%BD%AE%E8%AF%A2&spm=1001.2101.3001.7020)：在包处理时避免中断上下文切换的开销，
* 用户态驱动：规避不必要的[内存](https://so.csdn.net/so/search?q=%E5%86%85%E5%AD%98&spm=1001.2101.3001.7020)拷贝和系统调用，便于快速迭代优化
* 亲和性与独占：特定任务可以被指定只在某个核上工作，避免线程在不同核间频繁切换，保证更多的cache命中
* 降低访存开销：利用内存大页HUGEPAGE降低TLB miss，利用内存多通道交错访问提高内存访问有效带宽
* 软件调优：cache行对齐，预取数据，多元数据批量操作
## DPDK框架