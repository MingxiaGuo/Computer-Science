# [Kubernetes](https://kubernetes.io/)

- kubernetes - [Tutorials - Kuberneteshttps://kubernetes.io/docs/tutorials/](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=11&cad=rja&uact=8&ved=2ahUKEwjD9dHigubfAhWO1FkKHUMkA_kQFjAKegQIFBAB&url=https%3A%2F%2Fkubernetes.io%2Fdocs%2Ftutorials%2F&usg=AOvVaw10KC1WIqEEhUnwj_VnX0g6)

## 1. kubernetes overview

### 1.1 what is Kubernetes

[Kubernetes](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/) is an open source **container orchestration engine** for **automating deployment**, **scaling**, and **management of containerized applications(workloads and services)** based on Borg and Omega, that facilitates both declarative configuration and automation. The open source project is hosted by the Cloud Native Computing Foundation ([CNCF](https://www.cncf.io/about)).

The name Kubernetes originates from **Greek**, meaning **helmsman(舵手)** or **pilot(飞行员)**.

Kubernetes不仅仅支持Docker，还支持Rocket，这是另一种容器技术。

Kubernetes 为什么要用“舵手”来命名呢？大家可以看一下这张图：

<img src="./assets/轮船.png" style="zoom:50%;" />

这是一艘载着一堆集装箱的轮船，轮船在大海上运着集装箱奔波，把集装箱送到它们该去的地方。我们之前其实介绍过一个概念叫做 container，container 这个英文单词也有另外的一个意思就是“集装箱”。Kubernetes 也就借着这个寓意，希望成为运送集装箱的一个轮船，来帮助我们管理这些集装箱，也就是管理这些容器。

* **Container orchestrator**
  * Runs and manages containers
  * Unified API for deploying web applications, batch jobs, and databases
  * Maintains and tracks the global view of the cluster
  * Supports multiple cloud and bare-metal environments
* **Manage applications, not machines**
  * Rolling updates, canary deploys, and blue-green deployments
* **Designed for extensibility**
  * Rich ecosystem of plug-ins for scheduling, storage, networking
* **Open source project managed by the Linux Foundation**
  * Inspired and informed by Google's experiences and internal systems
  * 100% open source, written in Go

### 1.2 why k8s

Going back in time: https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/#going-back-in-time

### 1.2 Kubernetes feature

* Intelligent Scheduling: 把一个容器放到一个集群的某一个机器上，
* Self-healing: 在一个集群中，经常会出现宿主机的问题或者说是 OS 的问题，导致容器本身的不可用，Kubernetes 会自动地对这些不可用的容器进行恢复. Kubernetes will **ALWAYS** try and steer the cluster to its desired state.
* Horizontal scaling: scale up and down dynamically
* Service discovery & load balancing
* Automated rollouts and rollbacks:
* Secret and configuration management

实际上，使用Kubernetes只需一个[部署文件](https://github.com/kubernetes/kubernetes/blob/master/examples/guestbook/all-in-one/guestbook-all-in-one.yaml)，使用一条命令就可以部署多层容器（前端，后台等）的完整集群：kubectl是和Kubernetes API交互的命令行程序。现在介绍一些核心概念。

下面，我们希望以三个例子跟大家更切实地介绍一下 Kubernetes 的能力。

#### 1.2.1调度

<img src="./assets/调度.png" style="zoom: 33%;" />

Kubernetes 可以把用户提交的容器放到 Kubernetes 管理的集群的某一台节点上去。Kubernetes 的调度器是执行这项能力的组件，它会观察正在被调度的这个容器的大小、规格。

比如说它所需要的 CPU以及它所需要的 memory，然后在集群中找一台相对比较空闲的机器来进行一次 placement，也就是一次放置的操作。在这个例子中，它可能会把红颜色的这个容器放置到第二个空闲的机器上，来完成一次调度的工作。

#### 1.2.2 自动修复

<img src="./assets/自动修复.png" style="zoom: 33%;" />

Kubernetes 有一个节点健康检查的功能，它会监测这个集群中所有的宿主机，当宿主机本身出现故障，或者软件出现故障的时候，这个节点健康检查会自动对它进行发现。

下面 Kubernetes 会把运行在这些失败节点上的容器进行自动迁移，迁移到一个正在健康运行的宿主机上，来完成集群内容器的一个自动恢复。

#### 1.2.3 水平伸缩

<img src="./assets/水平伸缩.png" style="zoom:50%;" />

Kubernetes 有业务负载检查的能力，它会监测业务上所承担的负载，如果这个业务本身的 CPU 利用率过高，或者响应时间过长，它可以对这个业务进行一次扩容。

比如说在下面的例子中，黄颜色的过度忙碌，Kubernetes 就可以把黄颜色负载从一份变为三份。接下来，它就可以通过负载均衡把原来打到第一个黄颜色上的负载平均分到三个黄颜色的负载上去，以此来提高响应的时间。

以上就是 Kubernetes 三个核心能力的简单介绍。

### 1.3. Kubernetes Strengths

* **Kubernetes has a clear governance model** managed by the Linux Foundation. Google is actively driving the product features and roadmap, while allowing the rest of the ecosystem to participate.
* **A growing and vibrant Kubernetes ecosystem** provides confidence to enterprises about its long-term viability.  IBM, Huawei, Intel, and Red Hat are some of the companies making prominent contributions to the project.
* **The commercial viability of Kubernetes makes it an interesting choice for vendors.**  We expect to see new offerings announced over the next several months.
* **Despite the expected growth in commercial distributions, Kubernetes avoids dependency and vendor lock-in** through active community participation and ecosystem support.
* **Kubernetes supports a wide range of deployment options.** Customers can choose between bare metal, virtualization, private, public, and hybrid cloud deployments. It enjoys a wide range of delivery models across on-premises and cloud-based services.
* **The design of Kubernetes is more operations-centric** than developer-orientated, which makes it the first choice of DevOps teams.

### 1.4 Kubernetes Certified Service Providers(KCSP)

The KCSP program is a vetted tier of service providers who have deep experience helping enterprises successfully adopt Kubernetes. KCSP partners offer Kubernetes support, consulting, professional services and training for organizations embarking on their Kubernetes journey.

* KCSP is managed by the Cloud Native Computing Foundation
* IBM is a KCSP Partner

  * IBM Container Service is a managed k8s environment with built-in cluster security and isolation while leveraging services including Waston, IoT, Weather, etc.

## 2. K8s Cluster Architecture

### 2.1 Architecture Overview

![](./assets/architecture_cluster.png)

![](./assets/architecture.png)

* **Cluster**: 集群是一组节点，这些节点可以是物理服务器或者虚拟机，之上安装了Kubernetes平台. A cluster is a set of resources, worker nodes, networks, and storage devices that keep apps highly available
* [Node](https://kubernetes.io/docs/concepts/architecture/nodes/): a node is a worker machine in k8s.
* [Pods](https://kubernetes.io/docs/concepts/workloads/pods/): A pod represents a set of running containers in your cluster. The worker node(s) host the Pods that are the components of the application workload
* The [Control Plane](https://kubernetes.io/docs/reference/glossary/?all=true#term-control-plane) manages the worker nodes and the Pods in the cluster. Control Plane=**Master node** that manages the cluster
  * Scheduling, replication & control
  * Multiple nodes for HA
* **Worker nodes**
  * Node where pods are run
  * Docker engine
  * kubelet agent accepts & executes commands from the master to manage pods
  * cAdvisor – Container Advisor provides resource usage and performance statistics
  * kube-proxy – routes inbound or ingress traffic

### 2.2 Control Plane Components

Kubernetes 的 **Master Node** 包含四个主要的组件：API Server、Controller manager、Scheduler 以及 etcd。Control Plane = Master Node

<img src="./assets/control-plane-components.png" alt="image-20220418175945770" style="zoom:30%;" />

* **Kube-apiserver**: 顾名思义是用来**处理 API 操作**的，**Kubernetes 中所有的组件都会和 API Server 进行连接**，组件与组件之间一般不进行独立的连接，都**依赖于 API Server 进行消息的传送**；

  * Exposes REST API for managing Kubernetes(kubernetes control plane and datastore)
  * Used by kubectl CLI
  * All clients and other applications interact with kubernetes **strictly** through the API Server.
  * Acts as the gatekeeper to the cluster by handling authentication and authorization, request validation, mutation, and admission control in addition to being the front-end to the backing datastore.
  * You can run several instances of kube-apiserver and balance traffic between those instances.
* **Etcd**: A highly-available key value store. All cluster data is stored here. https://etcd.io/

  * etcd acts as the cluster Distributed backing datastore for kubernetes.
  * provide a strong, consistent and **highly available key-value store** for persisting cluster state.
  * Stores objects and config information.
  * The apiserveris the only thing that talks to it
  * Uses *“Raft Consensus”* among a quorum of systems to create a fault-tolerant consistent *“view”* of the cluster. https://raft.github.io/
  * <img src="./assets/etcd.png" alt="image-20220418180240031" style="zoom:50%;" />
* **kube-scheduler**: handesplacement :: 是调度器，“调度器”顾名思义就是完成调度的操作，就是我们刚才介绍的第一个例子中，把一个用户提交的 Container，依据它对 CPU、对 memory 请求大小，找一台合适的节点，进行放置；

  * Component on the master that **watches newly created pods** that have no node assigned, and **selects a worker node for them to run on. **(Selects the worker node each pods runs in)
  * Factors taken into account for scheduling decisions include individual and collective resource requirements, hardware/software/policy constraints, affinity and anti-affinity specifications, data locality, inter-workload interference and deadlines.
* **Kube-controller-manager**: 是控制器，它用来完成**对集群状态的一些管理**。比如刚刚我们提到的两个例子之中，第一个自动对容器进行修复、第二个自动进行水平扩张，都是由 Kubernetes 中的 Controller 来进行完成的

  * The home of the core controllers,
  * Daemon that runs controllers, which are the background threads that handle routine tasks in the cluster
  * Monitors the cluster state via the apiserver and **steers the cluster towards the desired state**. Does NOT handle scheduling,
  * Some types of these controllers are:
    * **Node Controller**: Responsible for noticing and responding when nodes go down.
    * **Job controller**: Watches for Job objects that represent one-off tasks, then creates Pods to run those tasks to completion.
    * **Endpoints Controller:** Populates the Endpoints object (that is, joins Services & Pods).
    * **Service Account & Token Controllers**: Create default accounts and API access tokens for new namespaces.
* **cloud-controller-manager**

  * The following controllers can have cloud provider dependencies:

    * **Node controller**: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding

    - **Route controller**: For setting up routes in the underlying cloud infrastructure
    - **Service controller**: For creating, updating and deleting cloud provider load balancers
    - **Volume Controller**: For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes

我们刚刚提到的 API Server，它本身在部署结构上是一个可以水平扩展的一个部署组件；Controller 是一个可以进行热备的一个部署组件，它只有一个 active，它的调度器也是相应的，虽然只有一个 active，但是可以进行热备。

### 2.2 Node Components

Kubernetes 的 **Node** 包含四个主要的组件：kubelet, kube-proxy, Conntainer Runtime Engine:

<img src="./assets/node-components.png" alt="image-20220418181335418" style="zoom:40%;" />

<img src="./assets/k8s-node.png" style="zoom: 33%;" />

* **kubelet**

  * An agent that runs on each [node](https://kubernetes.io/docs/concepts/architecture/nodes/) in the cluster. It makes sure that [containers](https://kubernetes.io/docs/concepts/containers/) are running in a [Pod](https://kubernetes.io/docs/concepts/workloads/pods/).
  * The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which were not created by Kubernetes
* **kube-proxy**

  * kube-proxy is a network proxy that runs on each [node](https://kubernetes.io/docs/concepts/architecture/nodes/) in your cluster, implementing part of the Kubernetes [Service](https://kubernetes.io/docs/concepts/services-networking/service/) concept.
  * [kube-proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) maintains network rules on each nodes. These network rules allow network communication to your Pods from network sessions inside or outside of your cluster.
  * kube-proxy uses the operating system packet filtering layer if there is one and it's available. Otherwise, kube-proxy forwards the traffic itself.
  * Performs connection forwarding or load balancing for Kubernetes cluster services.
* **Container runtime Engine**

  * The **container runtime** is the CRI compatible application(software) that is responsible for running and managing containers.
  * Kubernetes supports container runtimes such as [containerd](https://containerd.io/docs/), [CRI-O](https://cri-o.io/#what-is-cri-o), and any other implementation of the [Kubernetes CRI (Container Runtime Interface)](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-node/container-runtime-interface.md).

    * Containerd (docker)
    * Cri-o
    * Rkt
    * Kata (formerly clear and hyper)
    * Virtlet (VM CRI compatible runtime)

**Kubernetes 的 Node** 是真正运行业务负载的，每个业务负载会以 Pod 的形式运行。等一下我会介绍一下Pod 的概念。一个 Pod 中运行的一个或者多个容器，真正去运行这些 Pod 的组件的是叫做 **kubelet**，也就是 Node 上最为关键的组件，它通过 API Server 接收到所需要 Pod 运行的状态，然后提交到我们下面画的这个 Container Runtime 组件中。

在 OS 上去创建容器所需要运行的环境，最终把容器或者 Pod 运行起来，也需要对存储跟网络进行管理。Kubernetes 并不会直接进行网络存储的操作，他们会靠 Storage Plugin 或者是网络的 Plugin 来进行操作。用户自己或者云厂商都会去写相应的 **Storage Plugin** 或者 **Network Plugin**，去完成存储操作或网络操作。

在 Kubernetes 自己的环境中，也会有 Kubernetes 的 Network，它是为了提供 Service network 来进行搭网组网的。（等一下我们也会去介绍“service”这个概念。）真正完成 service 组网的组件的是 **Kube-proxy**，它是利用了 **iptable** 的能力来进行组建 Kubernetes 的 Network，就是 cluster network，以上就是 Node 上面的四个组件。

Kubernetes 的 Node 并不会直接和 user 进行 interaction，它的 interaction 只会通过 Master。而 User 是通过 Master 向节点下发这些信息的。Kubernetes 每个 Node 上，都会运行我们刚才提到的这几个组件。

### 2.3 Addons

* DNS
* Web UI
* Container Resource Monitoring
* Cluster-level Logging

### 2.4 Communication of components

![](./assets/k8s例子.png)

下面我们以一个例子再去看一下 Kubernetes 架构中的这些组件，是如何互相进行 interaction 的。

 用户可以通过 UI 或者 CLI 提交一个 Pod 给 Kubernetes 进行部署，这个 Pod 请求首先会通过 CLI 或者 UI 提交给 Kubernetes API Server，下一步 API Server 会把这个信息写入到它的存储系统 etcd，之后 Scheduler 会通过 API Server 的 watch 或者叫做 notification 机制得到这个信息：有一个 Pod 需要被调度。

这个时候 Scheduler 会根据它的内存状态进行一次调度决策，在完成这次调度之后，它会向 API Server report 说：“OK！这个 Pod 需要被调度到某一个节点上。”

这个时候 API Server 接收到这次操作之后，会把这次的结果再次写到 etcd 中，然后 API Server 会通知相应的节点进行这次 Pod 真正的执行启动。相应节点的 kubelet 会得到这个通知，kubelet 就会去调 Container runtime 来真正去启动配置这个容器和这个容器的运行环境，去调度 Storage Plugin 来去配置存储，network Plugin 去配置网络。

这个例子我们可以看到：这些组件之间是如何相互沟通相互通信，协调来完成一次Pod的调度执行操作的。

## 3. K8s Core Objects and API

https://kubernetes.io/docs/concepts/overview/working-with-objects/

### 3.1 k8s API

https://kubernetes.io/docs/concepts/overview/kubernetes-api/

Kubernetes API 是由 **HTTP+JSON** 组成的：用户访问的方式是 HTTP，访问的 API 中 content 的内容是 JSON/YAML 格式的。Kubernetes 的 **kubectl** 也就是 command tool，**Kubernetes UI**，或者有时候用 **curl**，直接与 Kubernetes 进行沟通，都是使用 HTTP + JSON 这种形式。

下面有个例子：比如说，对于这个 Pod 类型的资源，它的 HTTP 访问的路径，就是 API，然后是 apiVesion: V1, 之后是相应的 Namespaces，以及 Pods 资源，最终是 Podname，也就是 Pod 的名字。

如果我们去提交一个 Pod，或者 get 一个 Pod 的时候，它的 content 内容都是用 JSON 或者是 YAML 表达的。在这个 yaml file 中，对 Pod 资源的描述也分为几个部分。

### 3.2 k8s object

kubernetes资源对象: 资源元信息

* apiVesion: V1 ; API 的 **version**
* kind: 描述我在操作哪个资源
* Namespaces
* Metadata:
  * Name: 资源的名字
  * Labels：用于识别资源
  * Annotations：用于描述资源，对资源的额外的一些用户层次的描述。https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/
  * OwnerReference：用于多个资源相互关系
* Spec: 期望的状态。
  * Containers: 内部需要有哪些 container 被运行
    * name:
    * image: 它的 image 是什么
    * ports: 它暴露的 port 是什么？
* Status：观测到的状态，资源当前的状态

### 3.1 Namespaces

Namespaces are a logical cluster or environment, and are the primary method of partitioning a cluster or scoping access.

![image-20220418184210798](./assets/namespace.png)

Namespace 是用来做一个集群内部的逻辑隔离的，它包括鉴权、资源管理等。Kubernetes 的每个资源，比如刚才讲的 Pod、Deployment、Service 都属于一个 Namespace，同一个 Namespace 中的资源需要命名的唯一性，不同的 Namespace 中的资源可以重名。

<img src="./assets/k8s-namespace.png" style="zoom:50%;" />

Namespace 一个用例，比如像在阿里巴巴，我们内部会有很多个 business units，在每一个 business units 之间，希望有一个视图上的隔离，并且在鉴权上也不一样，在 cuda 上面也不一样，我们就会用 Namespace 来去给每一个 BU 提供一个他所看到的这么一个看到的隔离的机制。

### 3.2 Finalizer

https://kubernetes.io/docs/concepts/overview/working-with-objects/finalizers/

### 3.2 Pods

 Pod 是 Kubernetes 的一个**最小调度以及资源单元**。用户可以通过 Kubernetes 的 Pod API 生产一个 Pod，让 Kubernetes 对这个 Pod 进行调度，也就是把它放在某一个 Kubernetes 管理的节点上运行起来。一个 Pod 简单来说是对一组容器的抽象，它里面会**包含一个或多个容器。**

比如像下面的这幅图里面，它包含了两个容器，每个容器可以指定它所需要资源大小。比如说，一个核一个 G，或者说 0.5 个核，0.5 个 G。

<img src="./assets/k8s-pod.png" style="zoom:50%;" />

当然在这个 Pod 中也可以包含一些其他所需要的资源：比如说我们所看到的 Volume 卷这个存储资源；比如说我们需要 100 个 GB 的存储或者 20GB 的另外一个存储。

在 Pod 里面，我们也可以去定义容器所需要运行的方式。比如说运行容器的 Command，以及运行容器的环境变量等等。Pod 这个抽象也给这些容器提供了一个共享的运行环境，它们会共享同一个网络环境，这些容器可以用 localhost 来进行直接的连接。而 **Pod 与 Pod 之间，是互相有 isolation 隔离的。**

A group of co-located containers: Pods are **one or MORE containers** that share volumes and namespace

* Smallest deployment unit – runs containers
* Each pod has its own IP
* Shares a PID namespace, network, and hostname, volumes
* **They are also ephemeral!**

<img src="./assets/pod.png" style="zoom:50%;" />

Pod examples

![image-20220418184313988](./assets/pod-example.png)

**Key Pod Container Attributes**

* **name** - The name of the container
* **image** - The container image
* **ports** - array of ports to expose. Can be granted a friendly name and protocol may be specified
* **env** - array of environment variables
* **command** - Entrypoint array (equiv to Docker ENTRYPOINT)
* **args** - Arguments to pass to the command (equiv to Docker CMD)

```
containers:

name: nginx
image: nginx:stable-alpine
ports:
  - containerPort: 80
    name: http
    protocol: TCP
env:
  - name: MYVAR
    value: isAwesome
command: [“/bin/sh”, “-c”]
args: [“echo ${MYVAR}”]
```

![image-20220418184551481](./assets/key-pod-attribute.png)

**Pod Template**

- Workload Controllers manage instances of Pods based off a provided template.
- Pod Templates are Pod specs with limited metadata.
- Controllers use Pod Templates to make actual pods.
- ![image-20220418184658595](./assets/pod-template.png)

### 3.3 Labels

A label is a key/value pair that is attached to a resource, such as a pod, to convey a user-defined identifying attribute.

- **key-value** pairs that are used to i**dentify,** **describe** and **group** together related sets of objects or resources.
- **NOT** characteristic of uniqueness.
- Have a strict syntax with a slightly limited character set.
- https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set

<img src="./assets/labels.png" alt="image-20220418184741120" style="zoom:30%;" />

作用：

* 用于筛选资源: 可以使用selector查询：类似于SQL ‘select * where ...
* 唯一的组合资源的方法
* 这些 label 是可以被 selector，也就是选择器所查询的。

通过 label，kubernetes 的 API 层就可以对这些资源进行一个筛选，那这些筛选也是 kubernetes 对资源的集合所表达默认的一种方式。

例如说，我们刚刚介绍的 Deployment，它可能是代表一组的 Pod，它是一组 Pod 的抽象，一组 Pod 就是通过 label selector 来表达的。当然我们刚才讲到说 service 对应的一组 Pod，就是一个 service 要对应一个或者多个的 Pod，来对它们进行统一的访问，这个描述也是通过 label selector 来进行 select 选取的一组 Pod。

所以可以看到 label 是一个非常核心的 kubernetes API 的概念

<img src="./assets/k8s-api-label.png" style="zoom:50%;" />

<img src="./assets/k8s-labels.png" style="zoom:50%;" />

上面是四个常用的标签。前三个标签打在pod上，最后一个标签打在node对象上。

### 3.4 Resource Model

* **Request**: amount of a resource allowed to be used, with a strong guarantee of availability.
  * CPU(seconds/second), RAM(bytes)
  * Scheduler will not over-commit requests
* **Limit**: max amount of a resource that can be used, regardless of guarantees.
  * scheduler ignores limits

<img src="./assets/resouece-model.png" alt="Screen Shot 2022-04-18 at 6.57.10 PM" style="zoom:90%;" />

### 3.5 Selectors

Selectors use labels to filter or select objects, and are used throughout Kubernetes.

<img src="./assets/selectors.png" alt="image-20220418185913626" style="zoom:50%;" />

<img src="./assets/selectors-dia.png" alt="image-20220418185944036" style="zoom:40%;" />

**Selector Types**

<img src="./assets/selector-types.png" alt="image-20220418190042433" style="zoom:50%;" />

### 3.6 Service

Service 提供了一个或者多个 Pod 实例的稳定访问地址。

<img src="./assets/k8s-service.png" style="zoom:50%;" />

比如在上面的例子中，我们看到：一个 Deployment 可能有两个甚至更多个完全相同的 Pod。对于一个外部的用户来讲，访问哪个 Pod 其实都是一样的，所以它希望做一次负载均衡，在做负载均衡的同时，我只想访问某一个固定的 VIP，也就是 Virtual IP 地址，而不希望得知每一个具体的 Pod 的 IP 地址。

我们刚才提到，这个 pod 本身可能 terminal go（终止），如果一个 Pod 失败了，可能会换成另外一个新的。

对一个外部用户来讲，提供了多个具体的 Pod 地址，这个用户要不停地去更新 Pod 地址，当这个 Pod 再失败重启之后，我们希望有一个抽象，把所有 Pod 的访问能力抽象成一个第三方的一个 IP 地址，实现这个的 Kubernetes 的抽象就叫 Service。

实现 Service 有多种方式，Kubernetes 支持 Cluster IP，上面我们讲过的 kuber-proxy 的组网，它也支持 nodePort、 LoadBalancer 等其他的一些访问的能力。

A service defines a set of pods and a means by which to access them, such as single stable IP address, static namespaces and corresponding DNS name. 1. Collection of pods exposed as an endpoint. 2. state and networking info propagated to all worker nodes

- **Unified method of accessing** the exposed workloads of Pods.
- **Durable resource** (unlike Pods)  , **NOT Ephemeral!**
  - static cluster-unique IP
  - static namespaced DNS name

* \<service name\>.\<namespace\>.svc.cluster.local

- Target Pods using **equality based selectors**.
- Uses **kube-proxy** to provide simple load-balancing.
- **kube-proxy** acts as a daemon that creates **local entries** in the host’s iptables for every service.

* Types of service exposure

  * **ClusterIP** (Default)– Exposes cluster-internal IP
  * **NodePort** – Exposes the service on each Node’s IP at a static port
  * **LoadBalancer** – Exposes externally using a cloud provider’s load balancer
  * **ExternalName** – Maps to an external name (such as foo.bar.example.com)

  <img src="./assets/services-2.png" alt="image-20220418175332944" style="zoom:30%;" />

  ![img](./assets/services.png)

#### 3.6.1 **ClusterIP Service**

**ClusterIP** services exposes a service on a strictly cluster internal virtual IP.

<img src="./assets/clusrerip-service.png" alt="image-20220418191400923" style="zoom:50%;" />

<img src="./assets/cluster-ip-dia.png" alt="image-20220418191650711" style="zoom:50%;" />

**ClusterIP Service Without Selector**

<img src="./assets/cluster-ip-no-selector.png" alt="image-20220418191745221" style="zoom:50%;" />

#### 3.6.2 **NodePort Service**

- **NodePort** services extend the **ClusterIP** service.
- Exposes a port on every node’s IP.
- Port can either be statically defined, or dynamically taken from a range between 30000-32767.

<img src="./assets/nodeport-service.png" alt="image-20220418191922107" style="zoom:50%;" />

<img src="./assets/nodeport-service-example.png" alt="image-20220418191922107" style="zoom:50%;" />

#### 3.6.3 **LoadBalancer Service**

- **LoadBalancer** services extend **NodePort.**
- Works in conjunction with an external system to map a cluster external IP to the exposed service.

apiVersion: v1
kind: Service
metadata:
  name: example-prod
spec:
  type: LoadBalancer
  selector:
    app: nginx
    env: prod
  ports:
    protocol: TCP
    port: 80
    targetPort: 80

<img src="./assets/load-balance-service.png" alt="image-20220418192321568" style="zoom:50%;" />

<img src="./assets/load-balance-service-2.png" alt="image-20220418192321568" style="zoom:50%;" />

#### 3.6.4 **ExternalName Service**

- **ExternalName** is used to reference endpoints **OUTSIDE** the cluster.
- Creates an internal **CNAME** DNS entry that aliases another.

### 3.7 **Ingress – Name Based Routing**

- An API object that manages external access to the services in a cluster
- Provides load balancing, SSL termination and name/path-based virtual hosting
- Gives services externally-reachable URLs

### 3.8 Exploring the Core

Lab - github.com/mrbobbytables/k8s-intro-tutorials/blob/master/core

## 4 Workloads

### 4.1 **ReplicaSet**

- Primary method of managing pod replicas and their lifecycle.
- Includes their scheduling, scaling, and deletion.
- Their job is simple: **Always ensure the desired number of pods are running.**
- 
- ![image-20220418213338290](./assets/ReolicaSet.png)

![image-20220418213422819](./assets/ReplicaSet-example.png)

![image-20220418213531098](./assets/ReplicaSet-2.png)

### 4.2 **Deployment**

- Way of managing Pods via **ReplicaSets.**
- Provide rollback functionality and update control.
- Updates are managed through the **pod-template-hash** label.
- Each iteration creates a unique label that is assigned to both the **ReplicaSet** and subsequent Pods.

![image-20220418213619710](./assets/Deployment.png)

Deployment 是在 Pod 这个抽象上更为上层的一个抽象，它可以定义一组 Pod 的副本数目、以及这个 Pod 的版本。一般大家用 Deployment 这个抽象来做应用的真正的管理，而 Pod 是组成 Deployment 最小的单元。

<img src="./assets/k8s-deployment.png" style="zoom:50%;" />

Kubernetes 是通过 Controller，也就是我们刚才提到的控制器去维护 Deployment 中 Pod 的数目，它也会去帮助 Deployment 自动恢复失败的 Pod。

比如说我可以定义一个 Deployment，这个 Deployment 里面需要两个 Pod，当一个 Pod 失败的时候，控制器就会监测到，它重新把 Deployment 中的 Pod 数目从一个恢复到两个，通过再去新生成一个 Pod。通过控制器，我们也会帮助完成发布的策略。比如说进行滚动升级，进行重新生成的升级，或者进行版本的回滚。

### 4.3 **DaemonSet**

- Ensure that all nodes matching certain criteria will run an instance of the supplied Pod.
- Are ideal for cluster wide services such as log forwarding or monitoring.

![image-20220418213752435](/Users/guomingxia/Documents/workspace/Computer-Science/docs/Cloud_Computing/assets/DaemonSet.png)

### 4.4 **StatefulSet**

A StatefulSet is a Controller that provides a unique identity to its Pods. It provides guarantees about the ordering of deployment and scaling.

- Tailored to managing Pods that must persist or maintain state.
- Pod lifecycle will be ordered and follow consistent patterns.
- Assigned a unique ordinal name following the convention of ‘*`<statefulset name>`-`<ordinal index>`*’.

### 4.5 **Job**

### 4.6 **CronJob**

An extension of the Job Controller, it provides a method of executing jobs on a cron-like schedule.

## 5. Security

![image-20220418182117151](./assets/access-control.png)

**Authentication**

- **X509 Client Certs** (CN used as user, Org fields as group) No way to revoke them!! – wip ☺
- **Static Password File** (password,user,uid,"group1,group2,group3")
- **Static Token File** (token,user,uid,"group1,group2,group3")
- **Bearer Token** (Authorization: Bearer 31ada4fd-ade)
- **Bootstrap Tokens** (Authorization: Bearer 781292.db7bc3a58fc5f07e)
- **Service Account Tokens** (signed by API server’s private TLS key or specified by file)

**Role - Authorization**

![image-20220418183514597](/Users/guomingxia/Documents/workspace/Computer-Science/docs/Cloud_Computing/assets/role-auth.png)

**RoleBinding - Authorization**

`<img src="/Users/guomingxia/Library/Application Support/typora-user-images/image-20220418182604251.png" alt="image-20220418182604251" style="zoom:50%;" />`

`<img src="/Users/guomingxia/Library/Application Support/typora-user-images/image-20220418182837085.png" alt="image-20220418182837085" style="zoom:55%;" />`

**Admission Control**:

- AlwaysPullImages
- DefaultStorageClass
- DefaultTolerationSeconds
- DenyEscalatingExec
- EventRateLimit
- ImagePolicyWebhook
- LimitRanger/ResourceQuota
- PersistentVolumeClaimResize
- PodSecurityPolicy

`<img src="/Users/guomingxia/Library/Application Support/typora-user-images/image-20220418182931121.png" alt="image-20220418182931121" style="zoom:50%;" />`

`<img src="/Users/guomingxia/Library/Application Support/typora-user-images/image-20220418182957776.png" alt="image-20220418182957776" style="zoom:50%;" />`

**Request/Response**

`<img src="/Users/guomingxia/Library/Application Support/typora-user-images/image-20220418183041455.png" alt="image-20220418183041455" style="zoom:50%;" />`

## 6. Networking

**Fundamental Networking Rules**

- All containers within a pod can communicate with each other unimpeded.
- All Pods can communicate with all other Pods without NAT.
- All nodes can communicate with all Pods (and vice-versa) without NAT.
- The IP that a Pod sees itself as is the same IP that others see it as.

**Fundamentals Applied**

- **Container-to-Container**

  - Containers within a pod exist within the **same network namespace** and share an IP.
  - Enables intrapod communication over *localhost*.
- **Pod-to-Pod**

  - Allocated **cluster unique IP** for the duration of its life cycle.
  - Pods themselves are fundamentally ephemeral.

* **Pod-to-Service**

  * managed by **kube-proxy** and given a **persistent cluster unique IP**
  * exists beyond a Pod’s lifecycle.
* **External-to-Service**

  * Handled by **kube-proxy**.
  * Works in cooperation with a cloud provider or other external entity (load balancer).

## 7. Storage

Pods by themselves are useful, but many workloads require **exchanging data** between containers, or **persisting** some form of **data**.

For this we have 4 ‘types’ of storage

* **Volumes**
* **PersistentVolumes**
* **PersistentVolumeClaims**
* **StorageClasses**

> ●PV’s and PVCs work great together, but still require some manual provisioning under the hood.
>
> ●What if we could do all that dynamically? Well, you can with StorageClasses
>
> ●Act as an abstraction on top of a external storage resource that has dynamic provisioning capability
>
> ○usually the cloud providers, but others work like ceph

### 7.1 Volumes

Volume 就是卷的概念，它是用来**管理 Kubernetes 存储的**，是**用来声明在 Pod 中的容器可以访问文件目录的**，一个卷可以被挂载在 Pod 中一个或者多个容器的指定路径下面。

<img src="./assets/k8s-volume.png" style="zoom:50%;" />

而 Volume 本身是一个抽象的概念，一个 Volume 可以去**支持多种的后端的存储。**比如说 Kubernetes 的 Volume 就支持了很多存储插件，它可以支持**本地的存储**，可以支持**分布式的存储**，比如说像 ceph，GlusterFS ；它也可以支持云存储，比如说阿里云上的云盘、AWS 上的云盘、Google 上的云盘等等。

A volume is a directory, possibly with some data in it, which is accessible to a Container as part of its  filesystem.

* Storage that is tied to the **Pod’s Lifecycle**. A Volume is defined in the pod Spec and tied to its lifecycle
* A pod can have one or more types of volumes attached to it.
* Can be consumed by any of the containers within the pod

  * web server
  * pre-process data
  * socket files
  * logs
* Survive Pod restarts; however their durability beyond that is dependent on the Volume Type.
* **Volume Types**

  * Current list of in-tree volume types supported, with more being added through the “Container Storage Interface”
    * essentially storage plugin system supported by multiple container orchestration engines (k8s and mesos)
  * The ones in blue support persisting beyond a pod’s lifecycle, but generally are not mounted directly except through a persistent volume claim
  * emptyDir is common for scratch space between pods
  * configMap, secret, downward API, and projected are all used for injecting information stored within Kubernetes directly into the Pod
  * <img src="./assets/volume-type.png" alt="image-20220418205851972" style="zoom:50%;" />
* Volume Example
* <img src="./assets/volume-example.png" alt="image-20220418210718174" style="zoom:50%;" />

### 7.2 **Persistent Volumes**

* A **PersistentVolume** (PV) are k8s objects that represent underlying storage resource.
* They don’t belong to any namespace, they are a global resource.
* PVs are a **cluster wide resource** linked to a backing storage provider: NFS, GCEPersistentDisk, RBD etc.
* Generally provisioned by an administrator. users aren’t usually the ones creating PVs
* Their lifecycle is handled independently from a pod
* **CANNOT** be attached to a Pod directly. Relies on a **PersistentVolumeClaim****.** users aren’t usually the ones creating PVs

<img src="./assets/persistentvolume.png" alt="image-20220418211757526" style="zoom:50%;" />

**PV Phases**: If you get or describe a PV, you will see that they have several different states.

* Available - The PV is ready and available to be consumed.
* Bound - The PV has been bound to a PVC and is not available.
* Released - The binding PVC has been deleted. The PV is in a state that (depending on reclaimPolicy) requires manual intervention
* Failed - For some reason, Kubernetes could not reclaim the PV from the stale or deleted PVC

### 7.3 **Persistent Volume Claims**

* A **PersistentVolumeClaim** (PVC) is a **namespaced** request for storage. a PVC is based within a namespace (with the pod)
* Satisfies a set of requirements instead of mapping to a storage resource directly. It is a one-to-one mapping of a user claim to a global storage offer.
* Ensures that an application’s ‘*claim*’ for storage is portable across numerous backends or providers.
* A user can define specific requirements for their PVC so it gets matched with a specific PV.

`<img src="/Users/guomingxia/Library/Application Support/typora-user-images/image-20220418212016521.png" alt="image-20220418212016521" style="zoom:50%;" />`

**PVs and PVCs with Selectors**

* This example defines a PV with 2Gi of space and a label.
* The PV is using the hostPath volume type, and thus they have a label of “type: hostpath”
* The PVC is looking for 1Gi of storage and is looking for a PV with a label of “type: hostpath”
* They will match and bind because the labels match, the access modes match, and the PVC storage capacity is <= PV storage capacity

<img src="./assets/PVs-and-PVCs.png" alt="image-20220418212331384" style="zoom:50%;" />

### 7.4 **StorageClass**

- Storage classes are an abstraction on top of an external storage resource (PV)
- Work hand-in-hand with the external storage system to enable **dynamic provisioning** of storage by eliminating the need for the cluster admin to pre-provision a PV
- <img src="./assets/storageclass-example.png" alt="image-20220418204704687" style="zoom:50%;" />

* **Available** **StorageClasses**

<img src="./assets/available-storageclass.png" alt="image-20220418204900166" style="zoom:50%;" />

### 7.5 Storageclass and PVC

* A PVC is created with a storage request and supplies the StorageClass ‘standard’
* The storageClass ‘standard’ is configured with some information to connect and interact with the API of an external storage provider
* The external storage provider creates a PV that strictly satisfies the PVC request. Really, it means it’s the same size as the request

  * The PV will be named pvc-\<uid of the pvc\>
* The PV is then bound to the requesting PVC

<img src="./assets/StorageClass-and-PVC.png" alt="image-20220418212844175" style="zoom:50%;" />

### 7.6 PV and PVC

**Persistent Volumes and Claims**

* PVCs can be named the same to make things consistent but point to different storage classes
* In this example we have a dev and prod namespace. These PVCs are named the same, but reside within different namespaces and request different classes of storage.

<img src="./assets/PV-and-PVCs.png" alt="PV-and-PVCs" style="zoom:50%;" />

### 7.5 **Working with Volumes**

Lab - github.com/mrbobbytables/k8s-intro-tutorials/blob/master/storage

## 8. **Configuration**

Kubernetes has an integrated pattern for decoupling configuration from application or container.

This pattern makes use of two Kubernetes components: **ConfigMaps** and **Secrets.**

> There are instances in your application where you want to be able to store runtime-specific information.
> Things like connection info, SSL certs, certain flags that you will inevitably change over time.
> The Kubernetes takes that into account and has a solution with ConfigMaps and Secrets.

### 8.1 **ConfigMap**

* **ConfigMap**

  * Externalized data stored within kubernetes. Configmap data is stored “outside” of the cluster (in etcd)
* **ConfigMap Example**

  * **data**: Contains key-value pairs of ConfigMap contents.
    * <img src="./assets/ConfigMap-example.png" alt="image-20220418195736096" style="zoom:40%;" >
* Can be created from a manifest, literals, directories, or files directly.

  * <img src="./assets/create-ConfigMap.png" alt="image-20220418200250041" style="zoom:67%;" />
* Can be referenced through several different means:

  * environment variable at runtime
  * a command line argument (via env var)
  * injected as a file into a volume mount

### 8.2 **Secret**

* **Secret**

  * Functionally identical to a ConfigMap.
  * Stored as **base64 encoded content.**
  * Encrypted at rest within etcd (**if configured!**).
  * Stored on each worker node in tmpfs directory.
  * Ideal for username/passwords, certificates or other sensitive information that should not be stored in a container.
  * and can be created the same way as configmaps
* **Secret Example**

<img src="./assets/secret-example.png" alt="image-20220418201946677" style="zoom:50%;" />

* **Secret Creation Example**
  * Manifest , literals, directories, or files.

<img src="./assets/create-secret.png" alt="image-20220418201802353" style="zoom:50%;" />

### 8.3 **Injecting Configuration**

<img src="./assets/inject-as-env.png" alt="inject-as-env" style="zoom:50%;" />

<img src="./assets/inject-in-command.png" alt="inject-in-command" style="zoom:50%;" />

<img src="./assets/inject-as-volume.png" alt="inject-as-volume" style="zoom:50%;" />

## 9. Metrics and Monitoring

**Metrics API server**

* Metric server collects metrics such as **CPU** and **Memory** by each pod and node from the Summary API, exposed by [Kubelet](https://kubernetes.io/docs/admin/kubelet/) on each node.
* Metrics Server registered in the main API server through [Kubernetes aggregator](https://kubernetes.io/docs/concepts/api-extension/apiserver-aggregation/), which was introduced in Kubernetes 1.7

**HPA (horizontal pod** **autoscaler**)

<img src="./assets/hpa.png" alt="image-20220418194256223" style="zoom:50%;" />

<img src="./assets/HPA-example.png" alt="image-20220418194429824" style="zoom:70%;" />

**Prometheus** & **Grafana (dashboards)**

![image-20220418194546676](./assets/prometheus.png)

**Fluentd** **(log shipping)**

![image-20220418194623430](./assets/fluentd.png)

![image-20220418194703160](./assets/logging-33.png)

## 8. K8s CRD

Custom Resource Definitions

## 9 Extend k8s

Kubernetes is highly configurable and extensible.

**Kubernetes extension points**

1. **kubectl plugins**: Users often interact with the Kubernetes API using `kubectl`. [Kubectl plugins](https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/) extend the kubectl binary. They only affect the individual user's local environment, and so cannot enforce site-wide policies.
2. **API server extensions** : The apiserver handles all requests. Several types of extension points in the apiserver allow authenticating requests, or blocking them based on their content, editing content, and handling deletion. These are described in the API Access Extensions section.
3. **Custom resources** : The apiserver serves various kinds of resources. Built-in resource kinds, like pods, are defined by the Kubernetes project and can't be changed. You can also add resources that you define, or that other projects have defined, called Custom Resources, as explained in the Custom Resources section. Custom Resources are often used with API Access Extensions.
4. **Scheduler extensions**: The Kubernetes scheduler decides which nodes to place pods on. There are several ways to extend scheduling. These are described in the Scheduler Extensions section.
5. **Controller extensions**: Much of the behavior of Kubernetes is implemented by programs called Controllers which are clients of the API-Server. Controllers are often used in conjunction with Custom Resources.
6. **kubelet network plugins**: The kubelet runs on servers, and helps pods appear like virtual servers with their own IPs on the cluster network. Network Plugins allow for different implementations of pod networking.
7. **kubelet storage plugins**: The kubelet also mounts and unmounts volumes for containers. New types of storage can be supported via Storage Plugins.

[Extending Kubernetes](https://kubernetes.io/docs/concepts/extend-kubernetes/)

### 9.1 API Extensions

[CustomResourceDefinitions](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#customresourcedefinition-v1beta1-apiextensions)

[Extend the Kubernetes API with CustomResourceDefinitions](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/)

[Custom Resources concept guide](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/).

**kubernetes二次开发（主要是开发满足自己业务的api）: client-go**

使用 client-go 控制原生及拓展的 Kubernetes API | PPT 实录: https://www.kubernetes.org.cn/1309.html

如何基于Kubernetes开发自定义的Controller: [https://www.kubernetes.org.cn/3655.html](https://www.kubernetes.org.cn/3655.html)

### 9.2 Infrastructure Extensions



Go clients for talking to a [kubernetes](http://kubernetes.io/) cluster:https://github.com/kubernetes/client-go

[源码分析] client-go的使用及源码分析: https://www.huweihuang.com/article/source-analysis/client-go-source-analysis/



## References

* **Kubernetes concept** https://kubernetes.io/docs/concepts/

* **Kubernetes tutorial** –https://kubernetes.io/docs/tutorials/kubernetes-basics/
* Introduction to container orchestration –https://www.exoscale.ch/syslog/2016/07/26/container-orch/
* TNS Research: The Present State of Container Orchestration –https://thenewstack.io/tns-research-present-state-container-orchestration/
* Large-scale cluster management at Google with Borg –https://research.google.com/pubs/pub43438.html
* kubernetes 中文社区： https://www.kubernetes.org.cn/
* Free Kubernetes Courses https://www.edx.org/
* Interactive Kubernetes Tutorials https://www.katacoda.com/courses/kubernetes
* Learn Kubernetes the Hard Way https://github.com/kelseyhightower/kubernetes-the-hard-way
* Official Kubernetes Youtube Channel https://www.youtube.com/c/KubernetesCommunity
* Official CNCF Youtube Channel https://www.youtube.com/c/cloudnativefdn
* Track to becoming a CKA/CKAD (Certified Kubernetes Administrator/Application Developer) https://www.cncf.io/certification/expert/
* Awesome Kubernetes https://www.gitbook.com/book/ramitsurana/awesome-kubernetes/details
* **Kubernetes中文指南/云原生应用架构实战手册** - https://jimmysong.io/kubernetes-handbook

## Setup and Run k8s

https://kubernetes.io/docs/setup/

* [Download Kubernetes](https://kubernetes.io/releases/download/)
* [install tools](https://kubernetes.io/docs/tasks/tools/)
* Select a [container runtime](https://kubernetes.io/docs/setup/production-environment/container-runtimes/) for your new cluster

## Demo

安装Kubernetes 沙箱环境，主要有三个步骤：

- 安装一个虚拟机，在虚拟机中启动 Kubernetes。推荐用 virtualbox 作为虚拟机的运行环境；安装 VirtualBox： https://www.virtualbox.org/wiki/Downloads
- 在虚拟机中启动 Kubernetes，Kubernetes 有一个非常有意思的项目，叫 minikube，也就是启动一个最小的 local 的 Kubernetes 的一个环境。我们推荐使用阿里云版本的minikube，它和官方 minikube 的主要区别就是把 minikube 中所需要的 Google 上的依赖换成国内访问比较快的一些镜像，这样就方便了大家的安装工作；安装 MiniKube（中国版）: https://yq.aliyun.com/articles/221687
- 对 minikube 进行启动。启动命令：`minikube start —vm-driver virtualbox` . 如果大家不是 Mac 系统，其他操作系统请访问下面这个链接，查看其它操作系统如何安装 minikube 沙箱环境。 [https://kubernetes.io/docs/tasks/tools/install-minikube/](https://kubernetes.io/docs/tasks/tools/install-minikube，)

当大家安装好之后，我会跟大家一起做一个例子，来做三件事情：

1. 提交一个 nginx deployment；

   kubectl apply  -f  https://k8s.io/examples/application/deployment.yaml
2. 升级 nginx deployment，也就是改变它中间 Pod 的版本；

   kubectl apply -f  https://k8s.io/examples/application/deployment-update.yaml
3. 扩容 nginx deployment，进行一次水平的伸缩。

   kubectl apply -f  https://k8s.io/examples/application/deployment-update.yaml

首先，我们先看一下 minikube 的 status，可以看到 kubelet master 和 kubectl 都是配置好的。

```
minikube status
```

下一步我们利用 kubectl 来看一下这个集群中节选的状态，可以看到这个master 的节点已经是running状态：

```
kubectl get nodes
```

我们就以这个为节点，下面我们尝试去看一下现在集群中 Deployment 这个资源：

```
kubectl get deployments
```

可以看到集群中没有任何的 Deployment，我们可以利用 watch 这个语义去看集群中 Deployment 这个资源的变化情况。

```
kubectl get --watch deployments
```

下面我们去做刚才想要的三个操作：第一个操作是去创建一个 Deployment。可以看到下面第一个图，这是一个 API 的 content，它的 kind 是 Deployment，name 是 nginx-deployment, 有图中它的 replicas 数目是2，它的镜像版本是 1.7.9。

```
cat deploymeny.yaml
```

我们下面还是回到 kubectl 这个 commnd 来执行这次 Deployment 的真正的操作。我们可以看到一个简单的操作，就会去让 Deployment 不停地生成副本。

```
lubectl apply -f deployment.yaml
```

Deployment 副本数目是 2 个，下面也可以 describe 一下现在的 Deployment 的状态。我们知道之前是没有这个 Deployment 的，现在我们去 describe 这个 nginx-deployment。

```
kubectl describe nginx-deployment
```

下图中可以看到：有一个 nginx-deployment 已经被生成了，它的 replicas 数目也是我们想要的、selector 也是我们想要的、它的 image 的版本也是 1.7.9。还可以看到，里面的 deployment-controller 这种版本控制器也是在管理它的生成。

下面我们去升级这个 Deployment 版本，首先下载另外一个 yaml 文件 deployment-update.yaml，可以看到这里面的 image 本身的版本号从 1.7.9 升级到 1.8。

```
cat deployment-update.yaml
```

接下来我们重新 apply 新的 deployment-update 这个 yaml 文件。

```
lubectl apply -f deployment-update.yaml
```

可以看到，在另一边的屏幕上显示出了这个 Deployment 升级的一些操作，最终它的 up-to-date 值从 0 变成了 2，也就是说所有的容器都是最新版本的，所有的 Pod 都是最新版本的。我们也可以 discribe 具体去看一下是不是所有 Pod 的版本都被更新了，可以看到这个 image 的版本由 1.7.9 真正更新到了 1.8。

```
kubectl describe nginx-deployment
```

最后，我们也可以看到 controller 又执行了几次新的操作，这个控制器维护了整个 Deployment 和 Pod 状态。

最后我们演示一下给 Deployment 做水平扩张，下载另一个 yaml 文件 deployment-scale.yaml，这里面的 replicas 数目已经从 2 改成了 4。

```
cat deployment-scale.yaml
```

回到最开始的窗口，用 kubectl 去 apply 这个新的 deployment-scale.yaml 文件，在另外一个窗口上可以看到，当我们执行了 deployment-scale 操作之后，它的容器 Pod 数目从 2 变成了 4。我们可以再一次 describ 一下当前集群中的 deployment 的情况，可以看到它的 replicas 的数目从 2 变到了 4，同时也可以看到 controller 又做了几次新的操作，这个 scale up 成功了。

```
kubectl apply -f deployment-scale.yaml
kubectl describe nginx-deployment
```

最后，让我们利用 delete 操作把我们刚才生成的 Deployment 给删除掉。kubectl delete deployment，也是刚才我们本身的 deployment name，当我们把它删除掉之后，我们今天所有的操作就完成了。

```
kubectl delete deployment nginx-deployment
```

我们再去重新 get 这个 Deployment，也会显示这个资源不再存在，这个集群又回到了最开始干净的状态。

```
kubectl get deployments
```
