虚拟化技术是云计算的基石。通过虚拟化，单服务器资源被分割成多个细粒度资源，实现资源高效利用。

**计算虚拟化技术**主要包括Hypervisor虚拟化(KVM，XEN，VMWare)和容器虚拟化(Docker)。

---

## 3.1 计算虚拟化

### 3.1.1 计算虚拟化介绍

计算虚拟化通过虚拟化技术将一台计算机虚拟为多台逻辑计算机，每个逻辑计算机可运行不同的操作系统，并且应用程序可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率。CPU的虚拟化技术可以用单CPU模拟多CPU运行。

计算虚拟化使用软件的方法重新定义、划分IT资源，可以实现IT资源的动态分配、灵活调度、跨域共享、提高IT资源利用率，使IT资源能够真正成为社会基础设施，服务于各行各业中灵活多变的应用需求。

虚拟化技术的核心是运行在硬件服务器上的**Hypervisor**软件。Hypervisor是一种运行在物理服务器和操作系统之间的中间软件层，允许多个操作系统和应用共享一套基础物理硬件，因此也可以看作虚拟环境中的"元"操作系统，它可以协调访问服务器上的所有物理设备和虚拟机，也叫虚拟机监视器(Virtual Machine Monitor) ₒ **Hypervisor是所有虚拟化技术的核心**，**非中断地支持多工作负载迁移的能力**是Hypervisor的基本功能。**当服务器启动并执行Hypervisor时，它会给每一台虚拟机分配适量的内存、CPU、网络和磁盘，并加载所有虚拟机的客户操作系统。**

![](../../Cloud_Computing/assets/虚拟化技术基础架构.jpg)

### 3.1.2 虚拟化实现方式：

#### 3.1.2.1 Hypervisor虚拟化

Hypervisor虚拟化表达了服务请求与底层物理交付的分离。通过在物理硬件与操作系统间增加一层逻辑虚拟化层，计算机的各种实体资源，如CPU、内存、网络及存储等，得以被抽象分割，形成多个虚拟机实体。对于上层服务，虚拟机就是真实的计算机，它拥有独立的计算环境，拥有自己的虚拟硬件。

Hypervisor虚拟化基本原理图如下，**其中Hypervisor 层负责服务器硬件和虚拟机操作系统之间的通信**。


  <img src="/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/assets/Hypervisor虚拟化基本原理图.jpg" alt="Hypervisor虚拟化基本原理图" style="zoom:50%;" />

随着桌面和服务器处理能力逐年持续增长，Hypervisor虚拟化也被证明是一种强大的技术。虚拟化可以简化软件开发、测试，有助于服务器整合，提高数据中心的敏捷性和业务的连续性。事实表明，把操作系统和应用程序从硬件中完全抽象出来，封装成具有可移植性的虚拟机，可以带来很多单纯硬件所不具备的虚拟化特性。最常见的例子是，通过虚拟化提供的在线迁移特性，服务可以7x24小时不间断运行，即使在数据备份和硬件维护时也不需要中断服务。事实上，在一些真实的虚拟化生产环境中，客户服务器已经运行数年而没有发生宕机。

对于x86系统，按照其虚拟化实现方式，可分为两种类型：

* **裸金属架构**

  裸金属架构的虚拟化是运行在服务器硬件之上，如图2-1所示。虚拟机运行在Hypervisor层之上，而Hypervisor层直接安装在硬件之上。由于不需要通过操作系统就可以直接访问硬件资源，这种虚拟化类型更高效，并且具有更好的可扩展性和更高的安全性。目前在市场上使用这种虚拟化架构的产品有微软的**Hyper-V**、VMware vSphere的**ESXi**和Citrix的 **XenServerₒ**

* **寄居架构**

  寄居架构虚拟化层将虚拟化层以一种应用程序的方式运行在操作系统之上，只要是操作系统能支持的硬件，虚拟化层都能支持，具有很好的兼容性。

  ![](/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/3-云计算技术/3.2-计算/寄居架构.jpg)

无论采用裸金属架构还是寄居架构，虚拟化层都负责运行和管理所有虚拟机。虚拟化层为每个虚拟机实现虚拟机硬件抽象，并负责运行客户操作系统，通过分割和共享CPU、内存和I/O设备等来实现系统的虚拟化。

因内部体系结构和具体实现不同，Hypervisor所呈现的功能会有很大的差异。以下从CPU、内存、设备虚拟化3个方面，简单介绍Hypervisor内部的实现方式。

* **CPU虚拟化**

  X86操作系统被设计为直接在硬件上运行，很自然，操作系统会认为它们拥有硬件的所有控制权。x86架构为操作系统和应用程序提供了 4个权限级别（ring 0、ring，ring 2和ring 3）来管理对硬件的访问。用户程序一般运行在ring 3,而操作系统需要直接访问内存和硬件，所以必须在ring 0执行特权指令。虚拟化X86架构需要在操作系统（原本运行于最高权限ring 0）与硬件之间再增加一个虚拟层，用于为创建和管理虚拟机提供共享资源。某些敏感指令在非ringO下执行时具有不同的语义，而不能很好地被虚拟化，这使得x86虚拟化的实现更加复杂。运行时陷入并翻译这些敏感指令和特权指令是一个巨大的挑战，这使得X86架构的虚拟化起初看起来像是"不可完成的任务"。

  虚拟化发展多年，但业界还没有一个开放的标准来定义和管理虚拟化。每个公司可以根据自己的情况，设计不同的虚拟化方案来应对虚拟化的技术挑战。而处理敏感和特权指令，以实现X86架构上CPU虚拟化的技术，大体可归纳为以下3种：

    * 使用二进制翻译的全虚拟化

    * 操作系统辅助或半虚拟化

    * 硬件辅助的虚拟化

  （1）使用二进制翻译的全虚拟化
  这种方法对内核代码进行翻译，将不可虚拟化的指令替换为一串新的指令，而这串指令对虚拟化硬件可达到预期效果，如图2・3所示。同时，将用户级的代码直接运行在物理处理器上以保证虚拟化的高性能。虚拟机监控器为虚拟机提供真实的物理系统的所有服务，包括虚拟BIOS.虚拟设备和虚拟内存管理等。

  ![](/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/3-云计算技术/3.2-计算/使用二进制翻译的全虚拟化.jpg)

  由于虚拟机操作系统被完全抽象，通过虚拟化层与底层硬件彻底解耦，所以，二进制翻译和直接指令执行的结合提供了全虚拟化。虚拟机操作系统完全意识不到虚拟化，不需要对虚拟机系统做任何的修改。Hypervisor在运行过程中动态翻译操作系统指令，并将结果缓存以备后续使用。而用户级指令无需修改就可以运行，具有和物理机一样的执行速度。

  全虚拟化为虚拟机提供最佳的隔离性和安全性。由于同样的虚拟机实例可以运行在虚拟化环境或真实物理硬件上，所以全虚拟化也简化了虚拟机的可移植性。VMware的虚拟化产品和微软的Virtual Server就采用了全虚拟化。

  (2)操作系统辅助虚拟化或半虚拟化

  相对于全虚拟化，半虚拟化是指通过虚拟机系统和Hypervisor间的交互来改善性能和效率。半虚拟化涉及修改虚拟机操作系统内核，将不可虚拟化的指令替换为超级调用Hypercall,以便直接与虚拟化层通信，如图2-4所示。Hypervisor也为其他关键的系统操作，如内存管理、中断处理、计时等，提供超级调用接口。

  ![](/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/3-云计算技术/3.2-计算/半虚拟化.jpg)

  在全虚拟化中，未经修改的虚拟机系统并不知道自身被虚拟化，敏感系统调用陷入进行二进制翻译。与全虚拟化不同，半虚拟化的价值在于减少了虚拟化开销。但是半虚拟化相对于全虚拟化的性能优势与工作负载有很大的关系。由于半虚拟化不支持未经修改的操作系统(如 Windows 2OOO/XP),它的兼容性和可移植性较差。由于需要对系统内核进行深度修改，很明显，在生产环境中半虚拟化在技术支持和维护上会引入很多问题。开源的Xen项目是半虚拟化的一个例子，它使用一个经过修改的Linux内核来虚拟化处理器，而用另外一个定制的虚拟机系统的设备驱动来虚拟化l/0ₒ

  (3)硬件辅助虚拟化

  硬件厂商也迅速拥抱虚拟化并开发出硬件的新特性来简化虚拟化技术，这其中包括Intel虚拟化技术(VT-x)和AMD的AMD-V两者都为特权指令增加了新的CPU执行模式，以允许VMM在ring 0新增根模式下运行。如图2-5所示，特权和敏感调用自动陷入Hypervisor,而不需要进行二进制翻译或半虚拟化。虚拟机状态保存在虚拟机控制结构(VMCS, VT-x)或虚拟机控制块(VMCB, AMD-V)中。支持VT-x和AMD-V特性的第一代硬件辅助特性处理器在2006年发布，但第一代硬件辅助的实现采用的编程模型僵化，导致Hypervisor到虚拟机的切换开销很高，以至于硬件辅助虚拟化性能低于某些优化后的二进制翻译性能。

  ![](/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/3-云计算技术/3.2-计算/硬件辅助虚拟化.jpg)

  第二代硬件辅助技术做了很大的改进，包括硬件支持的内存虚拟化，如AMD的NPT(Nested Page Table)和 Intel 的 EPT (Extended Page Table),以及硬件支持的设备，和 I/O 虚拟化 Intel VT-d、AMD l0MMUₒ

* **内存虚拟化**

  内存虚拟化涉及共享系统物理内存和为虚拟机动态分配物理内存。内存虚拟化和现代操作系统对虚拟内存的支持很相似。应用程序看到的连续地址空间与底层真实物理内存并不一一对应，操作系统在页表中保存了虚拟页号与物理页号的映射关系。当前，所有的x86 CPU都采用内存管理单元(MMU)和页表缓存部件(TLB),以优化虚拟内存的性能。

  为了在一个系统上运行多个虚拟机，还需要另外一层内存虚拟化。换句话说，需要虚拟化MMU来支持虚拟机系统。虚拟机系统仍然控制虚拟机中虚拟地址到内存物理地址的映射，但虚拟机系统不能直接访问真实物理机器内存。

  在硬件辅助虚拟化出现之前，Hypervisor负责将虚拟机物理内存映射到真实的机器内存，并使用影子页表来加速映射过程。Hypervisor使用硬件中的TLB将虚拟内存直接映射到机器内存，以避免每次访问时所进行的两级转换。影子页表的引入意味着Hypervisor需要为每个客户机的每个进程的页表都维护一套相应的影子页表，这会带来内存上较大的额外开销。此外，客户机页表和和影子页表的同步也比较复杂。当虚拟机改变了虚拟内存到物理内存的映射时，Hypervisor需要更新影子页表，以备后续可以直接查找。

  Intel EPT技术和AMD NPT技术都对内存虚拟化提供了硬件支持。这两种技术原理类似，都是通过硬件方式加速客户机虚拟地址到宿主机物理地址之间的转换。以EPT为例，EPT页表实现客户机物理地址到宿主机物理地址的映射，这样就将客户机虚拟地址到宿主机物理地址的转换分解为客户机虚拟地址到客户机物理地址映射和客户机物理地址到宿主机物理地址的映射，而这两步映射都由硬件自动完成。当客户机运行时，客户机页表被载入CR3,而EPT页表被载入专门的EPT页表指针寄存器EPTPO EPT页表对地址的映射机制与客户机页表对地址的映射机制相同。EPT实现方式不需要为每个客户机的每个进程维护一套页表来进行转换映射，EPT比影子页表实现简单，且由于采用硬件实现，虚拟化性能、效率也得到大幅提升。

* **设备虚拟化**

  设备和I/O虚拟化涉及对虚拟设备和共享物理设备之间的I/O请求路径的管理。

  相较于物理硬件直通(direct pass-through),基于软件的I/O虚拟化和管理提供了更丰富的特性和更简化的管理方式。以网络为例，虚拟网卡和虚拟交换机在虚拟机之间创建虚拟网络，而不需要消耗物理网络的带宽。网卡绑定允许将多个物理网卡展现为一块网卡，在网卡故障时可以对虚拟机做到透明切换，并且，虚拟机可以通过热迁移在不同的系统间无缝迁移，而保持MAC地址不变。高效I/O虚拟化的关键是要在维持虚拟化优势的同时，最小化CPU消耗。

  Hypervisor虚拟化物理硬件为每个虚拟机呈现标准的虚拟设备。这些虚拟设备有效地模拟了所熟知的硬件，并将虚拟机的请求转换到系统物理硬件。所有虚拟机可以配置为运行在相同的虚拟硬件上，而与底层真实的系统物理硬件无关。设备驱动的一致性和标准化也进一步推动了虚拟机的标准化，增强了虚拟机在不同平台间的可移植性。

#### 3.1.2.2 容器虚拟化

容器虚拟化：充分利用操作系统自身机制和特性

容器技术：新一代虚拟化技术

- Docker(工具：Machine、Compose、Swarm，容器平台：Kubernetes，Mesos，CoreOS)
- chroot(1982,UNIX) -> Linux Container(LXC,集成到Linux内核中) -> Docker(提供容器管理工具，分层文件系统，镜像；早起Docker完全基于LXC，之后开发了libcontainer，之后Dokcer推动runC项目，使容器不局限于Linux操作系统，而是更安全、更具扩展性。)

容器也是对服务器资源进行隔离，包括CPU份额、网络I/O、带宽、存储I/O、内存等。同一台主机上的多台容器之间可以公平友好地共享资源，而不互相影响。

如今，容器是云计算的一个热门话题。在同一台服务器上部署容器，其密度相较于虚拟机可以提升约10倍。但是容器并不是一个新的技术，它至少可以追溯到2000年FreeBSD jails 的出现，而 FreeBSD jails 则是基于 1982 年 BSD UNIX 的 chroot C 命令。再往前，chroot最早源于1979年UNIX7版本。通过chroot可以改变进程和子进程所能看到的根目录，这意味着可以在指定的根目录下独立运行程序，所以说从早期的chroot中就可以看出容器的踪迹。但是chroot仅适合于运行简单的应用，往往只是一个shell程序。虽然chroot会为程序创造一个jail, jail通过虚拟对文件系统、网络等的访问，限制用户的行为，但是还是有些方法很容易发生"越狱"这使得chroot很难应用于大型复杂系统。

SUN利用了 jail的概念，将其演化成Solaris Zones。但这一技术是Solaris特有的，所以虽然我们可以在Zone中运行Solaris应用或者一个更早版本的Solaris,但是无法在AIX或者Linux中运用这一技术。

在 Solaris 基于 FreeBSD jail 开发 Solaris Zone 的同时，Google、RedHat、 Canonical等公司也基于Linux进行了容器的相关研究。Parallels在2001年研发了 Virtuozzo,并获得了一定的商业成功。Parallels Virtuozzo在2005年演变为OpenVZ 其后又作为LXC开源进入Linux内核。而Google于2013年开源了 Imctfy项目，虽然Google容器项目开源得很晚，但事实上，Parallels. RedHat以及Google自身的Imctfy项目都是依托于Google的cgroup技术。cgroup技术使得开发者可以进一步抽象系统资源，增强了 Linux系统安全性。Google内部也一直在使用容器支持日常的公司运作，甚至支持Google Doc、Gmailₛ Google Search等商业应用。Google每周要运行约20亿个容器。

但是，对于大部分公司，容器还是一个神秘甚至有些令人畏惧的技术。直到Docker的出现才改变了业界开发、运维模式。Docker使得人们认识了又一个开源容器项目libcontainer, Docker自身也成为了 Linux容器的事实标准。

容器虚拟化和Hypervisor虚拟化的差别在于，容器虚拟化没有Hypervisor层，容器间相互隔离，但是容器共享操作系统，甚至bins/libs,如图2·6所示。每个容器不是独立的操作系统，所以容器虚拟化没有冗余的操作系统内核及相应的二进制库等，这使得容器部署、启动的开销几乎为零，且非常迅速。

![](/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/3-云计算技术/3.2-计算/容器虚拟化原理.jpg)

容器是非常轻量的。容器内进程可以是一个Linux操作系统，可以是一个三层架构的Web应用，也可以是一个数据库后端，甚至是一个简单的hello world程序。

容器使用的主要内核特性如下。

* namespace

  容器虚拟化利用namespace提供容器间的隔离。当运行容器时，容器虚拟化为容器创建一组namespaceₒ容器在namespace中运行，不会超出namespace指定的范围。

  容器使用了以下namespaceₒ

  * Pid namespace：提供进程隔离。
  * Net namespace：管理容器网络接口，实现网络隔离。
  * IPC namespace：提供容器进程间通信隔离，例如不同的容器采用不同的消息队列。
  * Mnt namespace：允许不同的容器看到的文件目录不同。
  * Uts namespace：允许容器有独立的hostname和domain nmme,这使得容器在网络上可以作为一个独立节点而非一个进程呈现。

* cgroup

  cgroup是Google贡献的一个项目，目的是通过内核技术对共享资源进行分配、限制及管理。容器利用cgroup为每个容器分配CPU、内存以及blkio等资源，并对其使用容量进行限制。通过引入cgroup,可以在同一主机的多台容器间公平、友好地共享资源，避免了因某些容器资源滥用而导致其他虚拟机甚至主机性能受到显著影响。

容器得到了一些初创公司的关注，如Docker. CoreOS. Shippable等公司，同时也受到了很多大公司的热捧。Google基于之前Borg系统的经验开发了 Kubernetes容器管理系统，IBM在其BlueMix PaaS平台支持Docker, Amazon在其弹性云上开放容器服务，HP、微软、RedHat也在容器领域做了相应工作。

容器面临的最大的挑战是安全问题。目前已经有一些安全产品，如RedHat的SELinux增强Linux安全级别，Ubuntu的AppArmor针对应用设定访问控制属性。但是还需要进一步加强内核安全，在多租户环境中将入侵者阻挡在容器外。

#### 3.1.2.3 大型主机虚拟化：IBM

### 3.1.3 KVM

KVM最初是由一个名为Qumranet的以色列小公司开发的，2008年9月由RedHat收购。但当时的虚拟化市场上主要以VMware为主，KVM没有受到太多关注。2011年，为了避免VMware 一家独大, IBM和RedHat,联合惠普和英特尔成立了开放虚拟化联盟(Open Virtualization Alliance),使得KVM快速发展起来。 

KVM全称为Kernel based virtual machine,如图2-7所示。从命名可以看出，KVM采用的是裸金属架构，并且是基于Linux内核的。KVM利用Linux操作系统 并在其上扩展了一个**kvm.ko内核模块，由它提供虚拟化能力。**            

通过KVM可以创建和运行多个虚拟机。而在KVM架构中，虚拟机被实现为一个普通的**Qemu-kvm进程**，由Linux标准调度器进行调度。事实上**每个虚拟CPU都呈现为一个常规Linux进程**，这就使得KVM可以使用Linux内核的所有特性。

**设备模拟通过一个修改后的Qemu提供**，包括对BIOS、PCI总线、USB总线，以及标准设备集如IDE和SCSI磁盘控制器、网卡等的模拟。

在KVM中通过用户态与内核协作完成虚拟化。从用户角度看，KVM是一个典型的Linux字符设备。用户可以通过ioctl函数对/dev/kvm设备节点进行操作，如创建、运行虚拟机。

![](/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/3-云计算技术/3.2-计算/KVM架构.jpg)

通过/dev/kvm提供的操作如下：

* 创建虚拟机。
* 为虚拟机分配内存。
* 读写虚拟CPU寄存器。
* 向虚拟CPU注入中断。
* 运行虚拟机。

KVM采用硬件辅助虚拟化技术，CPU运行时有3种模式：用户模式、内核模式和客户模式。

客户操作流程如图2-8所示。描述如下：

![](/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/3-云计算技术/3.2-计算/客户操作流程.jpg)

1） 在最外层，用户首先通过ioctl （）调用内核，触发CPU进入客户（Guest）模式，执行客户机代码，直到I/O指令或者外部事件（如网络包到达、定时器超时等）发生。对于CPU,外部事件表现为中断。

2） 在内核层，内核引起硬件进入客户模式。如果由于外部中断、影子页表缺失等事件，CPU退出客户模式，内核执行必要的处理，之后继续客户机执行。如果退出原因为I/O指令或者队列中断信号，则内核退出到用户态。

3） 在最内层，CPU执行客户代码，直到由于指令退出客户态。

KVM在Linux内核的基础上添加了虚拟机管理模块，由于**借用原生Linux CPU调度和内存管理机制，KVM实现非常轻量**。并且在kernel机制向前发展的同时，KVM也能获益。**由于采用了最新的Linux机制，并且依赖于x86硬件辅助虚拟化，通过CPU vt-x、内存ept技术等，KVM的性能也呈现出了较好的表现。**

### 3.1.4 Xen

Xen最初由英国剑桥大学的研究人员以GNU的GPL授权发布的开源软件，其架构如图2-9所示。

![](/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/3-云计算技术/3.2-计算/Xen架构.jpg)

Xen虚拟化环境由以下组件组成。

1. Xen Hypervisor

   Xen Hypervisor直接部署在硬件上，负责服务器上多台虚拟机的CPU调度和内存分配。由于所有虚拟机共享同一个处理环境，所以Hypervisor不仅为虚拟机抽象硬件，还控制虚拟机的执行。Hypervisor不负责网络、外部存储设备和其他I/O功能。

2. Dom0虚拟机

   Dom0是一个以经过修改的以Linux内核作为操作系统的特权虚拟机。Dom0虚拟机运行在Xen Hypervisor 上ₒ Dom0拥有访问物理I/O资源的特权，和同一服务器上的其他虚拟机进行交互。所有Xen虚拟化环境，在启动其他虚拟机前，需要保证Dom0处于运行态。

3. DomU虚拟机

   DomU虚拟机不是特权虚拟机，没有访问物理I/O资源的权限。DomU虚拟机对I/O资源的访问必须通过Dom0虚拟机。DomU支持半虚拟化和全虚拟化，目前主要采用全虚拟化，以保障虚拟机性能。

处理器半虚拟化最大的问题在于它需要修改虚拟机操作系统，这使得虚拟机系统的运行依赖于特定的Hypervisor。例如，Xen接口实现的深度半虚拟化对Hypervisor有很强的依赖性。虚拟机操作系统和Hypervisor实现舸譽纟吉构有强耦合。Xen的Linux内核不能运行在裸机或其他的Hypervisor上，这带来了不兼容性，使kernel的发布和需要维护的版本数增加了 2倍。另外，对新的开源操纵系统来说有限制，因为对虚拟机操作系统的修改需要得到操作系统厂商的支持。最后，对Hypervisor的强依赖性阻碍了内核的独立进化。

### 3.1.5 VMware

VMware成立于1998年，是全球領先的虚拟化解决方案提供商。 VMware在1998年就开发了二进制翻译技术，使得VMM运行在ring 0以达到隔离和性能的要求，而将操作系统转移到比应用程序所在ring 3权限高，但比虚拟机监控器所在ring 0权限低的用户级。2001年，VMware发布了第一个针对x86服务器的虚拟化产品。

VMwareESXi是其底层虚拟化管理程序，目前最新版本为ESXi6。它采 用一种裸金属虚拟化架构，直接安装在物理服务器之上，并将物理服务器划 分成多个虚拟机。

VMware ESXi具有如下特性。

* 安全性高：ESXi支持内存加固、内核模块完整性校验、可信平台模 
  块。
* 磁盘footprint占用空间小。
* 可以安装在硬盘、SAN设备LUN、USB设备、SD卡、无磁盘主机上。

VMware虚拟化层为每个虚拟机实现虚拟机硬件抽象，并负责运行客户 操作系统，通过分割和共享CPU、内存和I/O设备等来实现系统的虚拟化， 同时负责主机服务器的资源共享。

VMware虚拟化层最主要的部件是VMkerneU。VMkernel负责其上所有 进程，包括管理应用和代理、虚拟机的运行。VMkemel控制服务器上所有 硬件设备，为应用管理分配资源。

VMkemel是VMware开发的一个类Posix操作系统，和其他操作系统相同，VMkemel提供一些必要的系统功能，包括进程创建和控制、信号、文件系统和线程等。对于虚拟化，VMkemel着重设计、支持运行多虚拟机，并提供资源调度、I/O堆栈、设备驱动等核心功能。

在VMkemel中，每一个虚拟机对应一个虚拟机监视器（Virtual Machine Monitor, VMM)。VMM提供虚拟机执行环境和一个VMX进程。

如图所示为ESXi 5组件架构的框图。
<img src="/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/assets/Vmware-ESXi5-架构.png" style="zoom:50%;" />

与其他虚拟化管理程序不同，其所有管理功能都可以通过远程管理工具 实现。VMwareESXi采用无代理方法，通过一种基于API的合作伙伴集成模 型来进行硬件监控和系统管理。管理任务通过vSphere Command Line Interface (vCLI)和PowerCLI提供的远程命令行执行，而Power CLI使用 Windows PowerShell cmdlet和脚本实现自动化管理。

甶于没有底层操作系统和任何服务控制台，VMwareESXi非常轻量，安 装空间低至144 MB。有些制造商直接将ESXi集成到服务器内部存储、USB key或者SD卡上。VMware ESXi的轻量性也极大地缩小了恶意软件和网络 威胁的攻击面，从而提高了可靠性和安全性。

VMware—直致力于和一些领头的技术厂商合作定义虚拟化的开放标 准。任何一个行业，开放接口和格式被证明是产品被广泛采用的一个保证， 虚拟化也不例外。VMware通过推动标准制定，带动虚拟化的增长，加速客户解决方案的交付最终形成虚拟化技术的广泛采用。虚拟化用户可以使用的 
不同虚拟化方案的产品在不断地增加，只要虚拟化解决方案兼容，客户就可以进行更大范围的访问从而受益。开放接口和格式对业界的好处是，它促进虚拟化生态系统中厂商的合作和创新，并为大家扩大了市场机会。

VMware贡献了自己现有的框架和API。这些开放接口和格式是基于VMware多年虚拟真实部署的经验，不断演进提炼出的。
这些开放接口和格式包括：

* 虚拟机接口 ： Hypervisor和虚拟机之间的API。
* 管理接口：一套致力于管理独立主机和高度变化数据中心规模虚拟化系统的标准化操作的框架
* 虚拟机磁盘格式：虚拟机磁盘格式使得虚拟机可以跨平台部署、迁移和维护。

### 3.1.6 Docker

随着互联网的发展，分布式应用越来越普遍，应用提供商们期望，不管有多少用户，不论在何种设备上，这些程序都能随时随地可用和运行。至于应用程序，它们必须具备足够的弹性、互操作性，并可以被大规模扩展。此外，越来越多的应用提供商不仅要求满足当前需求，还期望建立复杂且可 
被广泛采用的下一代应用程序。

这个工作最大的挑战在于一应用程序不再只运行在一台计算机上。解 决方案层面要求将逻辑软件组件和底层基础设施拆分。当主机故障、升级或 者在其他环境中重新部署时，服务独立于之前的基础设施环境，始终随处可 用。

而其首要解决的问题就是封装和分发，若没有一致的封装方法，将同一个软件部署在各类不同的操作系统、设备、数据中心，必然会产生大量不稳定因素。因此，需要通过现存技术打造一个标准格式：一个始终一致的容器 —可以移动，可随时运行的Docker容器。第二个问题就是如何在不同的机器上执行这些容器，并产生一致的、可预期的结果。这两个问题定义了Docker发布前的大部分工作，也说明了 Docker成功带来的价值。

Docker 采用 Client-Server 架构，如图 2-11 所示。Docker Deamon 作为服务器，部署在每台主机上，支持主机上所有容器的分发、创建和运行。Docker客户端与Docker Deamon通过Socket或者Restful API进行交互。Docker客户端和Daemon可以部署在同一台主机上，也可以甶Docker客户 
端远程访问Docker Daemon。
<img src="/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/assets/docker架构.png" style="zoom:50%;" />

Docker包括以下主要部件。

1. Docker Daemon
   Docker Daemon运行在服务器上，但用户并不直接和Docker Daemon通信，而是通过 
   Docker 客户端与 Docker Daemon 交互。

2. Docker客户端
   Docker客户端是Docker的主要用户接口，Docker客户端接收用户请求，交给后端Docker Daemon，并将响应返回给用户。

3. Docker 镜像
   Docker镜像和虛拟机镜像一样，是一个只读模板。一个Web迨用的Docker锖像可以是一个已经安装了 Apache和Web应用的Ubuntu操作系统。Docker镜像可以用于创建Docker容器。Docker提供了简单的方式构建新镜像和更新已有镜像，ffl户也可以从Docker Registry下载其用户已创建好的镜像。Docker镜像是Docker的构建组件。

4. Docker Registry
   Docker Registry存储并管理Docker镜像。用户可以M公共或者私有存储上传或T载Docker镜像。公共 Docker Registry 成为 Docker Hub。Docker Registry提供大量已构建好的镜像供用户使用，这些镜像可以是由用户自己创建的，也可以是由其他用户创建并共享的。 Docker Registry是Docker的分发部件。

5. Docker 容器
   Docker容器像一个目录。Docker容器包含了运行一个应用需要的所有环境。每个容器从 
   Docker镜像创建而产生。Docker容器的生命周期包括运行、启动、停止、移动和删除。每个容器是一个隔离的、安全的应用平台。Docker容器是Docker的运行部件。

下面，我们以Docker run命令为例对Docker流程进行描述。

1. 用 户 在Docker客户端执行以下命令：
   ```$ docker run -i -t ubuntu /bin/bash``` Docker 客户端与 Docker Daemon 交互。
2. Docker Daemon检查宿主机上是否有Ubuntu镜像，如果没有则从Docker registries下载 Docker Ubuntu 镜像
3. Docker Daemon 从 Ubuntu 镜像创建Docker容器，并为其分配文件系统， mount —个可读写镜像层，之后Docker Daemon为容器分配网卡资源，设置IP。
4. Docker Daemon 启动应用，自此 Docker容器完成启动过程。

用户通过Docker客户端管理容器，也可以与容器中的应用进行交互。当容器运行结束后，用户通过Docker客户端删除容器。

Docker所依赖的底层技术除了 namespace、cgroup特性，还有Netlink、Capability等其他内核特性。所以，Docker本质上是一个上层管理工具，如图2-12所示。通过对Linux内核的namespace、cgroup、netlink 等特性的调用和整合，实现了应用逻辑与底层基础设施的解耦。
<img src="/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/assets/docke与linux内核关系.png" style="zoom:50%;" />

Docker让容器虚拟化真正走入人们的视线，而Docker的成功离不开对联合文件系统（union file system)的引入。联合文件系统通过创建镜像层，将具有不同文件结构的镜像层进行叠加挂载，使它们看上去像一个文件系统。由于通过多个镜像层叠加挂载，所以联合文件系统非常轻量。通过联合文件系统，在大规模部署场景，Docker可以对容器快速复制、重建及更新。Docker可利用的联合文件系统包括AUFS、btrfs、vfs、devicemapper等。目前AUFS提供的功能最全面，使用最广泛。


### 3.1.7 计算虚拟化技术对比及应用场景

Hypervisor虚拟化技术对比：

![](/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/3-云计算技术/3.2-计算/hypervisor技术对比.jpg)

容器与hypervisor虚拟化技术的核心技术相去甚远

![](/Users/gmx/workspace/Computer-Science/docs/Cloud_Computing/3-云计算技术/3.2-计算/容器与hypervisor对比.jpg)

### 3.1.8 计算虚拟化作用

存储虚拟化技术可以扩大硬件的容量，简化软件的重新配置过程。CPU的虚拟化技术可以用单CPU模拟多CPU并行，允许一个平台同时运行多个操作系统，应用程序可以在相互独立的空间内运行而互不影响，从而显著提高计算机的工作效率。

虚拟化技术与多任务及超线程技术是完全不同的。多任务是指在一个操作系统中多个程序同时并行运行，而在虚拟化技术中，则可以同时运行多个操作系统，而且每一个操作系统中都有多个程序运行，每一个操作系统都运行在一个虚拟CPU或者虚拟主机上；而超线程技术只是用单CPU模拟双CPU的工作，来平衡程序运行性能，这两个模拟出来的CPU是不能分离的，只能协同工作。

### 3.1.9 计算虚拟化厂家

当前，虚拟化应用变得越来越热门，下面简单分析几大虚拟化厂商各自的优缺点。

* Citrix 公司

  Citrix公司是近两年业务增长非常快的一家公司，这得益于云计算的兴起。Citrix公司主要有三大产品：服务器虚拟化XenServer,优点是便宜，管理一般；应用虚拟化XenAPP；桌面虚拟Xendesktop ₒ后两者是目前为止最成熟的桌面虚拟化与应用虚拟化产品。企业级VDI解决方案中不少都是Citrix公司的Xendesktop与XenApp的结合使用。

* IBM
  在2007年11月的IBM虚拟科技大会上，IBM就提出了"新一代虚拟化"的概念。只是时之今日，成功的案例却并不多见，像陕西榆林地区的中国神华分公司那样的失败案例倒是不少。不过，IBM虚拟化还是具备以下两点优势：第一，IBM丰富的产品线，对自有品牌有良好的兼容性；第二，强大的研发实力，可以提供较全面的咨询方案，只是成本过高，不是每一个客户都这么富有的。加上其对第三方支持兼容较差，运维操作也比较复杂，其对于企业来说就像是把双刃剑。并且IBM所谓的虚拟化只是服务器虚拟化，而非真正的虚拟化。

* VMware
  作为业内虚拟化领先厂商的VMware公司，一直以其易用性和管理性得到广泛的认同。只是受其架构的影响限制，VMware主要是在x86平台服务器上有较大优势，而非真正的IT信息虚拟化。加上其本身只是软件方案解决商，而非像IBM与微软那样拥有各自的用户基础的厂商。所以当前，对于VMware公司来说将面临着多方面的挑战，其中包括微软、XenSource（被Citrix收购）、Parallels. IBM公司。所以，未来对于V Mware公司来说这条虚拟化之路能否继续顺风顺水下去还真不好说。

* 微软
  2008年，随着微软V irtualization的正式推出，微软已经拥有了从桌面虚拟化、服务器虚拟化到应用虚拟化、展现层虚拟化的完备的产品线。至此，其全面出击的虚拟化战略已经完全浮出水面。因为，在微软眼中虚拟化绝非简单的加固服务器和降低数据中心的成本，还意味着帮助更多的IT部门最大化R0I,并在整个企业范围内降低成本，同时强化业务的可持续性。所以微软研发了一系列的产品，用以支持整个物理和虚拟基础架构。

* KVM
  这是一个开源的系统虚拟化模块，自Linux 2.6.20之后集成在Linux的各个主要发行版本中。它使用Linux自身的调度器进行管理，所以相对于Xen,其核心源码很少。KVM目前已成为学术界的主流VMM之一。目前，大量云供应商都以KVM作为其虚拟化的底层核心，大规模使用，从而使
  KVM的影响力空前巨大。


近两年，随着虚拟化技术的快速发展，虚拟化技术已经走出了局域网，进而延伸到了整个广域网。几大厂商的代理商业越来越重视客户对虚拟化解决方案需求的分析，因此也不局限于仅代理一家厂商的虚拟化产品。

### 3.1.10 计算虚拟化解疑

当技术人员刚接触到云计算和虚拟化时，在好奇的同时，总会有这样或那样的问题，如服务器是否可以多虚一等，在这里对一些常见问题进行解答。

1. 在一台特定的服务器上，每一个虚拟机是否可以运行任何版本的操
   作系统？

    服务器虚拟化对于在每一个虚拟机上使用的Windows （或者Linux）服务器操作系统没有任何版本限制，不过，全新版本的Windows需要在安装之前检查一下它与服务器虚拟化软件的兼容性。

2. 重新启动虚拟机是否会对其他虚拟机产生影响？
   重新启动虚拟机不需要接触服务器或者服务器虚拟化软件。重新启动虚拟机对于其他虚拟机没有任何影响。虚拟机完全是相互隔离的。不过，如果重新启动物理服务器（也就是说重新启动服务器虚拟化软件），它将中断所有虚拟机的运行。

3. 如何为虚拟机分配任务？
   总的来说，最好是把繁重工作量和轻工作量的应用程序搭配起来安装到每一台物理服务器上，以便最有效地使用服务器。从性能方面看，繁重工作量的应用程序能够从在通信高峰期随时使用大量处理器和内存资源中获得益处，同时还不影响轻工作量的应用程序有效地利用服务器的剩余资源，提供服务。

4. 虚拟化和刀片服务器是否可以一起使用？

   技术上应该谨慎地结合在一起，避免"把太多的鸡蛋放在一个篮子里"。把服务器虚拟化软件安装在刀片式服务器上没有技术错误或者困难。然而，在没有认真考虑它将产生的集中的风险之前，不要这样做。例如，如果在16台刀片式服务器的每一台服务器上建立10个虚拟机，在这个刀片式服务器的机架上就一共运行了 160个应用程序。如果这个机架发生问题（如火灾或者断电），并且没有充分的备份或者冗余（机架外部的），你就会同时失去160个应用程序，并且给你的企业带来灾难。

5. 服务器虚拟化时，是否需要对存储进行改变？

   如果用户已经在数据中心建立了一个存储局域网，在不改变存储网络原有架构的情况下，只需要将原有存储网络扩展到虚拟机所在的那些服务器即可。

6. 在遇到业务中断的时候，使用虚拟化是否会使不利影响降到最低？
   提高冗余的水平以避免中断的增多影响多个应用程序或者服务，即使不担心刀片式服务器与服务器虚拟化的结合，在标准服务器上使用服务器虚拟化也是"把许多鸡蛋放在一个篮子里"。考虑到一台服务器故障可能导致10个应用程序或者数据库中断，一般来说，理想的情况是提供同样水平的冗余，允许这台服务器在主服务器发生故障时把其全部内容迅速转移到一台待机的服务器上。

7. 应用程序和数据库的实例应该放在什么地方？
   一般来说，最好指定物理服务器作为开发、测试/质量保证和生产服务器，并且相应地把应用程序实例和数据库放在这些服务器上。这个政策是安全需求推动的。在某些行业，管理部门的规定要区别对待不同的环境（特别是生产环境）

8. 虚拟化环境中，跟踪软件许可证是否会变得更加困难？
   在一个理想的数据中心，跟踪虚拟化的服务器的软件许可证并不困难。然而，现实世界的经验表明，这确实是很困难的。在虚拟化环境中，虚拟机很容易创建，而且业务部门很难发现每一个虚拟机上需要什么软件，或者已经安装了什么软件，从而使跟踪许可证的要求和许可证的使用非常困难。

9. 服务器虚拟化以后，安全管理会变得更加困难吗？
   充分地保证访问虚拟机的安全以及在虚拟机上存储信息的安全要比在传统的物理环境中复杂一些。首先，访问虚拟化软件必须要严格控制。第二，任何能够访问虚拟机的人都可以下载一个应用程序，向隔离每一个虚拟机的虚拟"墙"实施攻击。第三，对于每一个虚拟机来说，实施网络级的接入限制是更复杂的对于虚拟化的安全，在网络安全方面，云服务商已经提供了vFW、云盾等虚拟化的网络安全服务产品；在数据安全方面，目前已经有数据加密、数据位置标识、数据彻底清除等方案的支持。

10. 引进虚拟化时，管理数据库是否要重新设计？
    需要对其基础的数据库（也就是方案）进行彻底的重新设计。如果使用当前的配置管理数据库不可能进行重新设计，必须购买新的配置管理数据库软件。

    虽然有许多配置管理数据库软件供应商已经开始采用服务器虚拟化，但是，在数据中心还使用许多老版本的配置管理数据库软件。这些老版本的软件也许不能把虚拟机当作"数据入口"，不能识别"物理服务器X上虚拟机A"的关系。物理服务器上的许多东西，如安装的操作系统版本和IP地址等，现在必须与虚拟机关联起来。此外，这些事情还必须与非虚拟化的服务器关联起来。这些要求意味着配置管理数据库下面的数据库设计需要进行彻底修改，以便在已经引进虚拟化的数据中心中使用。

11. 虚拟服务器环境与传统服务器环境相比，软件成本是否会减少？
    软件许可证成本将增加，因为业务部门要求增加更多的"机器"（业务部门知道增加虚拟机很容易并且很便宜）。一旦IT部门告诉业务部门，他们能够在几个小时之内配置一个新的虚拟机，人类的贪婪本性就会占上风。业务部门会大量地提出虚拟机配置申请。但如果他们要等待批准购买物理服务器的预算、下订单、交货、安装和设置使用，那么他们在提出申请的时候就会犹豫一下。一些专家把这个结果称作"虚拟机蔓延"。更糟糕的是，当一个计划取消的时候或者开发工作完成的时候，业务部门会告诉IT部门那个相关的虚拟机可以撤销了吗？当然不会。

12. 全面应用虚拟化的数据中心和传统数据中心相比，人员是否会减
    少？
    服务器和操作系统的技术支持人员数量大体相同。数据中心完成的大部分工作是由"服务器"的数量推动的。这些服务器是虚拟的还是物理的都没有什么区别。在对待故障单、管理操作系统和应用程序更新和补丁、管理安全问题、监视性能等方面，每一个虚拟机都要与物理服务器同样对待。不必安装物理服务器所减少的少量工作量将会被在每一个新的虚拟机上安装和设置VMware等服务器虚拟化系统这类工作所抵消。

