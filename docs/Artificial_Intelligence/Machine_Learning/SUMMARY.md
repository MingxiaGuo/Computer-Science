# Summary

* [Introduction](Introduction.md)

* When Can Machines Learn? (illustrative + technical)
    *  [1 The Learning Problem](chapter1.md)
        * 1.0 Course Introduction
        * [1.1 What is Machine Learning](1.1-What-is-Machine-Learning.md)
        * [1.2 Applications of Machine Learning](1.2-applications-of-machine-learning.md)
        * [1.3 Components of Machine Learning](1.3-components-of-machine-learning.md)
        * [1.4 Machine Learning and Other Fields](机器学习与其他领域的关系.md)
    * [2 Learning to Answer Yes/No](2-learning-to-answer-yesno.md)
        * [2.1 Percetron Hypothesis Set](21-percetron-hypothesisi-set.md)
        * [2.2 Percetron Learning Algorithm: PLA](22-percetron-learning-algorithm.md)
        * [2.3 Guarantee of PLA](23-guarantee-of-pla.md)
        * [2.4 Non-Separable Data: Pocket Algorithm](24-non-separable-data.md)
    * [3 Types of Machine Learning](di-3-zhang-ji-qi-xue-xi-fen-lei.md)
        * [3.1 Learning with Different Output Space Y](di-3-zhang-ji-qi-xue-xi-fen-lei/31-learning-with-different-output-space-y.md)
        * [3.2 Learning with DIfferent Data Label $$y_n$$](di-3-zhang-ji-qi-xue-xi-fen-lei/32-learning-with-different-data-label.md)
        * [3.3 Learning with Different Protocal f: $$f =(X_n,y_n)$$](di-3-zhang-ji-qi-xue-xi-fen-lei/33-learning-with-different-protocal-f.md)
        * [3.4 Learning with Different Input Space X](di-3-zhang-ji-qi-xue-xi-fen-lei/34-learning-with-different-input-space-x.md)
    * [4 Feasibility of Learning](di-4-zhang-feasibility-of-learning.md)
        * [4.1 Learning is Impossible?](di-4-zhang-feasibility-of-learning/41-learning-is-impossible.md)
        * [4.2 Probability to the Rescue](di-4-zhang-feasibility-of-learning/4.md)
        * [4.3 Connection to Learning](di-4-zhang-feasibility-of-learning/43-connection-to-learning.md)
        * [4.4 Connnection to Real Learning](di-4-zhang-feasibility-of-learning/44-connnection-to-real-learning.md)
    
* WHy Can Machines Learn?(theoretical + illustrative)
    * [5 Training versus Testing](di-5-zhang-training-versus-testing.md)
        * [5.1 Recap and Preview](di-5-zhang-training-versus-testing/51-recap-and-preview.md)
        * [5.2 Effective Number of Lines](di-5-zhang-training-versus-testing/52-effective-number-of-lines.md)
        * [5.3 Effective Number of Hypotheses](di-5-zhang-training-versus-testing/53-effective-number-of-hypotheses.md)
        * 5.4 Break Point
    * [6 Theory of Generalization](theory-of-generalization.md)
    * [7 The vc dimension](di-7-zhang-the-vc-dimension.md)
    * 8 The Noise and Error
    * [9 线性回归Linear Regression](di-9-zhang-xian-xing-hui-gui.md)
    * 10 逻辑回归Logisitic Regression
    * 11 Linear Models for Classification
    * 12 Nolinear Transform
    * 13 Hazard of Overfitting
    * 14 Regularization
    * 15 Validation
    * 16 Three Learning Principles
    
* How Can Machines Learn?(technical + practical)

    WHy Can Machines Learn Better?(practical + theoretical)

    Embedding Numerous Features: kernel Models

    - Lecture 1: Linear Support Vector Machine
      - Large-margin Separating Hyperplane
      - Standard Large-margin Problem
      - Support Vector Machine
      - Reasons behind Large-Margin Hyperplane

    Conbining Predictive Features: Aggregation Models

    Distilling Implicit Features: Extraction Models

* [机器学习技法](ji-qi-xue-xi-ji-fa.md)

* [Linear Support Vector Machine](支持向量机svm.md)
  * [Large-Margin Separating Hyperplane](支持向量机svm/large-margin-separating-hyperplane.md)
  * [Standard Large-Margin Problem](支持向量机svm/standard-large-margin-problem.md)
  * Support Vector Machine
  * Reasons behind Large-Margin Hyperplane
  
* DualSupportVectorMachine

* KernelSupportVectorMachine

* Soft-MarginSupportVectorMachine

* KernalLogisticRegression

* [支持向量回归SVR](zhi-chi-xiang-liang-hui-gui-svr.md)

* Blending and Bagging

* Adaptive Boosting

* 决策树

* [机器学习中的数学](机器学习中的数学.md)
  
  * [1. 法向量为什么是平面方程的系数](法向量为什么是平面方程的系数.md)
  
* 深度学习

