对抗攻击：机器学习与计算机安全结合的新领域。
按照攻击者是否知道目的网络的结构参数将对抗攻击分为白盒攻击和黑盒攻击。
按照目的网络最终得到的分类结果是否是攻击者设计好的将对抗攻击分为目标攻击和非目标攻击。

对抗攻击方面的研究：
* 攻击原理
* 对抗攻击方法
* 对抗攻击防御
* 实际应用（重点）：攻击分类；检测；物理世界的攻击等

研究对抗攻击的意义： 
1. 能让机器学习模型处理大规模数据； 
2. 以“计算机速度”处理攻击威胁； 
3. 不依赖数据的明显特征，发现实际应用中的各种内在威胁； 
4. 阻止已知和未知的恶意软件； 
5. 阻止恶意软件的提前执行； 
6. 优化模型，让分类模型达到更加高的分类准确率和更加低的错误率。

对抗样本每日论文总结：[https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html](https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html)

入门资料：https://github.com/P2333/Papers-of-Robust-ML#Verification



## 综述论文
* [Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey](https://arxiv.org/abs/1801.00553)

[https://www.jiqizhixin.com/articles/2018-03-05-4](https://www.jiqizhixin.com/articles/2018-03-05-4)

## 攻击原理
[http://karpathy.github.io/2015/03/30/breaking-convnets/](http://karpathy.github.io/2015/03/30/breaking-convnets/)

 《Intriguing properties of neural networks》提出：深度神经网络模型的非线性导致的输入与输出映射的不连续性，加上不充分的模型平均和不充分的正则化导致的过拟合使得对抗攻击成为可能。 
3. 《Explaining And Harnessing Adversarial Examples》提出：高维空间中的线性就足以造成对抗样本，深度模型对对抗样本的脆弱性最主要的还是由于其线性部分的存在。通过将模型转变成非线性的RBF模型，就能减少神经网络模型对对抗攻击的脆弱性。（Ian Goodfellow对对抗样本解释的论文）

## 对抗攻击方法
目前构建对抗样本的方法很多，总结如下： 
1. 传统的梯度下降、牛顿法、BFGS、L-BFGS：这些方法在2013年发表的文章《Evasion attacks against machine learning at test time》和2014年发表的文章《Intriguing properties of neural networks》中提到并运用来生成对抗样本，同时，这两篇文章也是最早提出对抗攻击这个概念的。

(1)FGSM：《Explaining And Harnessing Adversarial Examples》（2015.3） 
这篇Goodfellow提出的FGSM方法，是比较经典的对抗样本生成方法。 -(2)Iterative version of FGSM（The Basic Iterative Method ）：《Adversarial examples in the physical world》（2017.2） 
这个方法可以视作是Fast Gradient Method的迭代应用。 这篇文章针对上一篇FGSM方法的扰动规模比较大的缺陷，提出构造规模更加受限的对抗样本。
Jacobian saliency map attack (JSMA) ：《The limitations of deep learning in adversarial settings》（2015.10） 
这篇文章提出的对抗样本构建方法，是建立在攻击者已知目标网络的结构和参数等信息的情况下。这种方法的核心思想，是通过计算神经网络前向传播过程中的导数生成对抗样本。
One Pixel Attack：《One pixel attack for fooling deep neuralnetworks》（2017）
DeepFool：《Deepfool: a simple and accurate method to fool deep neural networks》（2015.10） 
这是第一个通过计算出最小的必要扰动，并应用到对抗样本构建的方法，使用的限制扰动规模的方法是L2范数。最终得到的对抗样本效果优于前面的FGSM和JSMA方法，但是这三者都需要比较大的计算资源。
Papernot Method：《 Adversarial perturbations against deep neural networks for malware classification》（2016.6） 
论文笔记：（https://www.zybuluo.com/wuxin1994/note/854417） 
这篇论文提到的对抗样本生成方法，更多的是针对于特定的应用场景——在输入是比较离散的数据情况下如何构建对抗样本。
Universal Perturbations： 《Universal adversarial perturbations》（2016.10）：这是一种对DeepFool方法的延伸。 
论文笔记：（https://www.zybuluo.com/wuxin1994/note/847422） 
《Analysis of universal adversarial perturbations》（2017.5）
RP2： 《Robust Physical-World Attacks on Machine Learning Models》（2017.7） 
论文笔记：（https://www.zybuluo.com/wuxin1994/note/839621）
CW（The Carlini and Wagner）：《Towards evaluating the robustness of neural networks》（2017.3） 
这篇文章的作者将Szegedy在《Explaining And Harnessing Adversarial Examples》提出的攻击方式转化成了一个更加高效的优化问题，能够以添加更小扰动的代价得到更加高效的对抗样本。
Virtual adversarial examples：《Virtual adversarial training: a regularization method for supervised and semi-supervised learning》（2017.4）
《Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN》(2017.2) 
论文笔记：（https://www.zybuluo.com/wuxin1994/note/867495）
《Machine Learning as an Adversarial Service: Learning Black-Box Adversarial Examples》（2017.8） 
论文笔记：(https://www.zybuluo.com/wuxin1994/note/860472)
## 对抗攻击防御

1. 修改训练过程或修改输入样本
2. 修改网络
3. 使用附加网络



Adversarial Training（augmenting the training data with perturbed examples）：《Intriguing properties of neural networks》（2014） 
所谓的对抗训练，就是防卫者通过自己构造对抗攻击，并且将人为增加扰动的对抗样本也加入到训练数据中，从而增强训练集，让训练后得到的模型更加稳定。 
（10.5新增）《Towards deep learning models resistant to adversarial attacks 》提到对抗训练用比较弱的攻击时，往往并没有增加模型对更强的攻击的鲁棒性。（这篇文章对对抗训练的方法提出了质疑，即是这种方法是否真的可以应对未来将要遇到的更强的攻击？）
PCA whitening ：《Early methods for detecting adversarial images》（2016.8）
Defensive distillation：《Distillation as a defense to adversarial perturbations against deep neural networks》（2016.3） 
这个方法通过两个步骤完成对模型稳定性的提升：第一步是训练分类模型，其最后一层的softmax层除以一个常数T；第二步是用同样的输入训练第二个模型，但是训练数据的标签不用原始标签，而是用第一步中训练的模型最后一层的概率向量作为最后softmax层的目标。 
《Extending Defensive Distillation》（2017.5）
Feature squeezing：《Feature squeezing: Detecting adversarial examples in deep neural networks》（2017.4） 
《Feature squeezing mitigates and detects carlini/wagner adversarial examples》（2017.5）
Detection systems: （这种defence方法采取的策略是在目的网络模型前面增加一个额外的探测系统，判断输入是否是经过人为扰动的对抗样本） 
Performe statistical tests:《On the (statistical) detection of adversarial examples》（2017.2） 
Use an additional model for detection:《Adversarial and clean data are not twins》（2017.4） 
《On detecting adversarial perturbations》（2017.2） 
Apply dropout at test time:《Detecting adversarial samples from artifacts》（2017.3）
Generative Adversarial Networks (GAN):《Generative Adversarial Trainer Defense to Adversarial Perturbations with GAN》（2017.5） 
《AE-GAN: adversarial eliminating with GAN》（2017.7） 
论文笔记：（https://www.zybuluo.com/wuxin1994/note/881171）
《Efficient Defenses Against Adversarial Attacks》（2017.7） 
论文笔记：(https://www.zybuluo.com/wuxin1994/note/863551)
《Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples》
该论文中提到 发现了一种「混淆梯度」（obfuscated gradient）现象，它给对抗样本的防御带来虚假的安全感。在案例研究中，试验了 ICLR 2018 接收的 8 篇论文，发现混淆梯度是一种常见现象，其中有 7 篇论文依赖于混淆梯度，并被的这一新型攻击技术成功攻克。
## 实际应用


Attacks on classification/recognition：

《Feature Space Perturbations Yield More Transferable Adversarial Examples》（cvpr2019）
更多UAP的论文：

UAP： 《Universal adversarial perturbations》（2016.10）：这是一种对DeepFool方法的延伸。
FFF：《Fast Feature Fool: A data independent approach to universal adversarial perturbations》（2017）：这是一个破坏卷积层特征的方法。
GDUAP：《Generalizable Data-free Objective for Crafting Universal Adversarial Perturbations》：FFF的补充。（攻击分类、分割、深度估计三方面）
结合生成模型：(1)《NAG: Network for Adversary Generation》论文笔记：隅子酱：读论文|NAG: Network for Adversary Generation (2)《Learning Universal Adversarial Perturbations with Generative Models》
AAA：《Ask, Acquire, and Attack: Data-free UAP Generation using Class Impressions》引入Class Impression，论文笔记：隅子酱：读论文 | Ask, Acquire, and Attack: Data-free UAP
singular vectors：《Art of singular vectors and universal adversarial perturbations》
Attacks on Semantic Segmentation and Object Detection：

《Adversarial examples for semantic segmentation and object detection》In ICCV, 2017.
《Adversarial examples that fool detectors》
《Robust adversarial perturbation on deep proposal-based models》 In BMVC, 2018.
《Attacking object detectors via imperceptible patches on background.》
GDUAP：《Generalizable Data-free Objective for Crafting Universal Adversarial Perturbations》：FFF的补充。（攻击分类、分割、深度估计三方面）
《Transferable Adversarial Attacks for Image and Video Object Detection》（2019）笔记：隅子酱：读论文 | Feature Space Perturbations...特征空间可迁移对抗样本
Attacks in the real world：

面部识别：《Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition》（2016.10）
实际拍照图片：《Adversarial examples in the physical world》（2017.2） 
这篇文章是在实际应用中，对抗攻击往往不能将数字化的对抗样本作为目的分类器的输入，只能将对抗样本打印到纸张上，然后用拍照之类的方式得到目的网络的输入时，人为添加的扰动比较小，在拍照过程中产生了失真，不能达到攻击目的。
路标：(1)《Robust Physical-World Attacks on Machine Learning Models》（2017.7） 
论文笔记：隅子酱：Robust Physical-World Attacks (2)《Note on Attacking Object Detectors with Adversarial Stickers》
自动汽车：(1)《Concrete Problems for Autonomous Vehicle Safety: Advantages of Bayesian Deep Learning》（2017） 
论文笔记：(https://www.zybuluo.com/wuxin1994/note/843327) (2)《No need to worry about adversarial examples in object detection in autonomous vehicles》
恶意软件分类：《Adversarial Perturbations Against Deep Neural Networks for Malware Classification》（2016.6） 
论文笔记：(https://www.zybuluo.com/wuxin1994/note/854417) 
《Generating Adversarial Malware Examples for Black-Box Attacks Based on GAN》（2017.5） 
论文笔记：（https://www.zybuluo.com/wuxin1994/note/867495）
3D打印：《Synthesizing Robust Adversarial Examples》


## 其他研究

对抗攻击可移植性研究：

《Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples》（2016.5） 论文笔记：(https://www.zybuluo.com/wuxin1994/note/850755)
《Delving into Transferable Adversarial Examples and Black-box Attacks》

## 以后研究方向

根据目前学习的对抗攻击研究，我觉得以后的研究方向主要有以下几个方面： 
1. 效率更高的对抗样本构造方法； 
2. 更好的Defence策略构建； 
3. 根据特定的应用场景探究攻击和防御策略（垃圾邮件分类、恶意软件识别、人脸识别等与安全相关的领域）； 
4. 得到对抗样本，在模型的训练集中加入对抗攻击样本，可以增强神经网络的鲁棒性； 
5. 理解对抗攻击背后的数学原理，实际就是探索深度网络的原理，尝试打开这个黑盒子。 
6. （9.18更新）已经知道了根据对目的模型的了解程度可以造成不同的影响结果，那么能否根据这一点来探究各个影响因素分别的效果呢？ 
7. （10.4更新）存在这样一个问题：很难去评判一个defence方法是否是足够有效的，也很难去评价一个攻击方法是否是足够成功的。因为往往在对抗攻击研究进程中，一个提出的defence策略总是会被后来提出的攻击方法证明是不够鲁棒的。反之，一个攻击方法也往往会被后面提出的defence方法证明是无效的。这种往复的循环博弈，给研究指出了一个新的方向:可以研究一种评估攻击方法或者防御策略的有效性的评估方法。 
这一部分参考《Ground-Truth Adversarial Examples》：a defensive technique that was at first thought to produce robust networks was later shown to be susceptible to new kinds of attacks.

将GAN与对抗攻击研究相结合的方向的论文不日更新。它可以同时构建效果更加好的对抗样本和实现让模型更加鲁棒的defence策略。对抗攻击可以实现，其本质一方面是因为神经网络乃至深度学习可以实现分类和预测目的的原理还比较模糊，因此可以利用这种不确定性来混淆模型；另一方面是因为数据本身就不能按照抽取的特征得到固定的分类结果，每一个个体具有比较大的误差因素。因此，模型容易受到对抗攻击是因为模型的泛化能力不够，在处理非训练数据时容易得到错误的结果。提高模型的泛化能力才是最好的defence策略。而最近特别火的GAN网络正是一种提高网络泛化能力的手段，在多个方面都被证明能让训练得到的模型具有更好的效果。