https://www.bilibili.com/video/BV1J94y1f7u5/?spm_id_from=333.337.search-card.all.click&vd_source=50ac7e35d44afea54a236dfa228f618f


深度学习中的函数f是一个个类神经网络。
输入可以是向量(vector)，矩阵(matric, 做图像识别)，序列(语音识别，翻译)。
输出可以是一个数值(regression)，类别（classification），一段话或文章，图片。

应用：
* 计算机视觉：图像分类
* 自然语言处理
* 语音识别

Regression: stock market forecast; self-driving car; recommendation;

Supervised Learning：Training Data(labeld) ; 标注数据工作量太大
* classification
Self-supervised Learning: unlabeled data --> Pre-train(or Foundation Model) , Downstream Tasks(Fine-tune)
* GAN
Reinforcement Learning: 不知道怎么标注数据但知道什么是成功时，如下围棋

Anomaly Detection 异常检测，具备回答我不知道的能力
Explainable AI：可解释性AI，回答为什么AI知道这个答案

Model Attack
domain adaptation
Network compression

Life-long learning

Meta Learning： learn to learn

Unsupervised Abstractive Summarization
https://arxiv.org/abs/1810.02851

Unsupervised Translation
https://arxiv.org/abs/1710.04087
https://arxiv.org/abs/1710.11041


Unsupervised ASR
https://arxiv.org/abs/1804.00316
https://arxiv.org/abs/1812.09323
https://arxiv.org/abs/1904.04100
https://arxiv.org/abs/2105.11.084
## Three steps for deep learning
1. Define **Neural Network**: A neural network is a function composed of simple functions(neurons); usually we design the network structure, and let machine find parameters from data
2. **Loss Function**: Cost function evaluates how good a set of parameters is; We design the cost function based on the task
4. **Optimization**: Find the best function set (e.g. gradient descent)
5. Training
6. Validation
7. Testing

## 深度学习模型的主要组成部分

1. 数据的输入

2. 神经元模拟，其中包含参数的初始化和存储

3. 正向传播，对于给定的输入向前传播，得到最终预测输出

4. 损失计算，根据标准输出的结果计算损失的大小

5. 反向传播，根据损失量调整各个参数的值

6. 训练，不断的用给定数据重复正向传播，损失计算和反响传播调整参数的过程

7. 预测，在模型训练完成之后，通过正向传播来应用模型

8. 超参选择

# History

![](assets/DL_history.png)


# Open courses
* [李宏毅2017深度学习 ](https://www.bilibili.com/video/av9770302?from=search&seid=8726738433477857991) [ppt](http://speech.ee.ntu.edu.tw/~tlkagk/courses_MLDS17.html)

* [李宏毅2019深度学习](https://www.bilibili.com/video/av48285039?from=search&seid=18412288318936573886)





# NN
NN: Neural Netowrk

大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统。现代神经网络是一种非线性统计性数据建模工具，常用来对输入和输出间复杂的关系进行建模，或用来探索数
据的模式。

神经网络的基本组成包括输入层，隐藏层，输出层。
Fully Connected Layer：

Fully Connected Network：
Fully Connect Feedback network

## 神经元
1943年，McCulloch 和 Pitts 将生物神经网络的神经元结构与功能抽象为一个简单模型，这就是一直沿用至今的“M-P神经元模型”。结构如下图所示：
![](https://img-blog.csdnimg.cn/afd075f4121244bda2b6b728046b4ab1.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAUmF5Q2hpdV9MYWJsb3k=,size_20,color_FFFFFF,t_70,g_se,x_16)

其中图中 w或θ表示权重, 和Z是相乘求和的线性变换，和f函数是非线性变换的激活函数。

在神经元模型中，神经元接收到来自 n 个其他神经元传输过来的输入信号{x1,x2...,xn}，这些输入信号通过带权重的连接(connection)进行传递，神经元接收到的总输入值(就是图中的和Z)将与神经网络的阈值进行比较，然后通过“激活函数”(activation function)处理以产生神经元的输出。
![](https://img-blog.csdnimg.cn/310e4444a0d64a688adf960e43e8930e.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAUmF5Q2hpdV9MYWJsb3k=,size_20,color_FFFFFF,t_70,g_se,x_16)
激活函数理想的是阶跃函数，它将输入值映射为0和1输出，“1”对应神经元兴奋，“0”则是抑制。但是这种阶跃结构具有不连续、不光滑等缺点，因此实际生产常用sigmoid作为激活函数，将输入值压缩至(0,1)区间。

神经元的数学模型：
![](https://img-blog.csdnimg.cn/f5d7e5cfc44d43d5930789f2268c142b.png)
神经元的计算： 
![](https://img-blog.csdnimg.cn/23f4efc46da74ab690db642ecd45fcea.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAUmF5Q2hpdV9MYWJsb3k=,size_14,color_FFFFFF,t_70,g_se,x_16)
## 神经网络 
神经元到神经网络: 把许多这种神经元结构按照一定的层次结构连接起来就得到了神经网络。
其他常用的激活函数：
常见的激活函数 
![](https://img-blog.csdnimg.cn/10afae87b6784b3faa3e6fae721cc8ff.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAUmF5Q2hpdV9MYWJsb3k=,size_18,color_FFFFFF,t_70,g_se,x_16)
如何选择激活函数 

        sigmoid激活函数这种非线性变换一般是在输出层做分类时使用，其他的比较常用的ReLU、tanh等都是在下边说的多层、深层神经网络中的隐藏层最后使用。

        注意：当然激活函数不一定是上边非线性的，准确的来说隐藏层必须是非线性的，而最后的输出层看情况而定，分类的情况下，二分类用sigmoid(softmax也可以二分类，二分类上和sigmoid一样)，多分类用softmax，或者接多个sigmoid做多标签分类；但是如果是MSE损失的回归问题，就不用做非线性变换了，或者说什么都不用做，直接y=x输出即可。
谈一下sigmoid和softmax 

        从本质上来说两者不一样，softmax是假设数据服从多项式分布，sigmoid是假设数据服从伯努利分布。 

        softmax不适合做多标签任务,因为根据softmax公式:

         可知softmax自带了归一化操作，所有类别的概率相加为1，因此会尽可能的把一个类的概率调整的很高，因此不适合多标签任务。

        而用逻辑回归sigmoid做多标签就没有归一化这个操作(有些框架为了好看会手动添加一个归一化)，各类别概率不会互相影响。

        为什么softmax的各类输出概率会互相影响而sigmoid不会呢？这得从梯度下降的两个阶段说起，两个阶段下边会详细说。

        观察上边softmax传播图的蓝色线(蓝色线没有参数w,或者可以看作为1的w)，结合softmax公式我们发现分母是综合了所有类别信息,，因此loss包含了所有类别的信息，反向传播求梯度也会影响到所有的参数w(w1到w12)，那下一次正向传播求loss依然会综合所有的w的信息和类别信息得出loss,而sigmoid不是这样，我们看sigmoid公式：

        还有sigmoid传播图：

         sigmoid它只和当前的类别相关，因此求出的loss只包含当前类别的信息，反向求梯度也是只和自己类别相关的几个参数有关，比如最上边的类别输出只和w1、w2、w3、w4有关，那更新了这几个参数后再次正向传播求loss依然只和当前类别有关。
区分不同神经网络的三个要素：

    激活函数：将神经元的净输入信号转换为单一的输入信号，以便进一步在网络中传播

    网络拓扑：描述了模型中神经元的数量以及层数和他们的连接方式

    训练算法：指定如何设置连接权重，以便抑制或增加神经元在输入信号中的比重

## 感知机

1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字--“感知器”（Perceptron）（有的文献翻译成“感知机”，下文统一用“感知器”来指代）。

感知器是当时首个可以学习的人工神经网络。Rosenblatt现场演示了其学习识别简单图像的过程，在当时的社会引起了轰动。

人们认为已经发现了智能的奥秘，许多学者和科研机构纷纷投入到神经网络的研究中。美国军方大力资助了神经网络的研究，并认为神经网络比“原子弹工程”更重要。这段时间直到1969年才结束，这个时期可以看作神经网络的第一次高潮。

定义 

        感知机是包含两层神经元的结构，这两层分别是输入层神经元和输出层神经元，输出层就是M-P神经元，亦称阈值逻辑单元、功能单元。
![](https://img-blog.csdnimg.cn/d9797949b2f34c9cbf48e234bfaccccb.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAUmF5Q2hpdV9MYWJsb3k=,size_11,color_FFFFFF,t_70,g_se,x_16)
 
参数训练 

        与神经元模型不同，感知器中的权值是通过训练得到的。因此，根据以前的知识我们知道，感知器类似一个逻辑回归模型，可以做线性分类任务。

　　我们可以用决策分界来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，当数据的维度是3维的时候，就是划出一个平面，当数据的维度是n维时，就是划出一个n-1维的超平面。

　　下图显示了在二维平面中划出决策分界的效果，也就是感知器的分类效果。

 

         一般给定训练数据集，权重以及阈值θ(偏置项，可以看作一个固定输入为-0.1的“哑结点”（dummy node）)，则感知机权重将这样调整：

 

         其中 在(0,1)范围内，称为学习率。

        可以看出，感知机若对训练集(x,y)预测正确，即，权重不会变化，如果有误差则会调整权重。
与或非问题和线性分类器以及异或和非线性分类器
线性可分情况 

        回顾神经元公式不加 f 这个激活函数就是线性计算的结果，是的，感知器就是线性分类器。 

        假如激活函数为阶跃函数，输入数据x1、x2只能取0或1，则：

         可以看到，图中a、b、c与或非可以通过训练，一定会收敛找到合适的权重(参数)w1、w2、θ组成的超平面分界线分开样本。d图这样非线性可分的问题则不能通过感知机学习到合适的解。
线性不可分情况

        Minsky在1969年出版了一本叫《Perceptron》的书，里面用详细的数学证明了感知器的弱点，尤其是感知器对异或这样的简单分类任务都无法解决。

 

        只拥有一层功能神经元(M-P神经元)的感知机不可以解决，而两层及两层以上的功能神经元就可以完美解决了：

         如上图所示，两层感知机可以解决异或这种线性不可分问题，多了一层隐藏层，也可以说多了一层功能神经元层。相邻两层直接节点全互连，即全连接。
多层前馈神经网络 
定义： 

        隐藏层如果是多层的话就是“多层前馈神经网络”了：

单层和多层网络之间的问题 

        多层网络可以有更多的隐藏层，也就会有更多的非线性变换，网络复杂度会随之更加复杂，因此有能力处理更加复杂的任务，但是不是越复杂越好，容易过拟合，如何理解复杂度，就类似二维平面找类别的分界线，简单的任务一条直线就可划分，困难的需要画出一个或多个极为复杂的不规则多边形才能正确分类，深层神经网络或者深度学习可以很好的处理这种非线性问题，当然深度学习的可解释性是非常差的，它是通过对网络的设计loss的设计加上基于训练得来的。
单节点输出和多节点输出

        输出节点可以根据任务来定，例如多分类就是多个节点，参照上边softmax来看，如果是回归任务：

         当然sigmoid的二分类也是单节点输出。
多层网络的误差逆传播算法BP(BackPropagation)
多层网络的理解 

         多层网络的学习能力很强，但是训练多层网络就得用BP算法了。

        1986年，Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。

 

        对于给定数据集D={(x1,y1),(x2,y2),...,(xm,ym)}，其中，再定义一些变量：

         下图为一个拥有d个输入神经元，L个输出神经元和q个隐含神经元的多层前馈神经网络, 其中输出层第j个神经元的阈值用表示，隐层第h个神经元的阈值用表示，隐层和输出层的激活函数都用sigmoid函数：

         对于训练样本，假设神经网络输出为，即：

         则网络在这个样本上的均方误差为：

 

         任意参数v的更新公式和上边感知机类似：

 
推导 

         下面以图中隐含层到输出层的连接权来进行推导。

         BP算法基于梯度下降的策略，以目标的负梯度方向对参数进行调整，对于误差和给定学习率η，有：

 

        这里一定要注意，求每个参数w变化是需要用相同的loss去对每个w求导得出梯度。 

         我们注意到先影响输出层第 j 个神经元的输入值，然后再进一步影响其输出，最后影响到均方误差。所以根据复合函数链式求导法则有：

 

         根据的定义可知：

 

         因为我们前边说了激活函数使用的sigmoid，sigmoid有如下性质：

         结合前边条件和可得： 

 

         综合得：

 

        类似可以得到：

        其中表示的是隐含层的梯度：

         BP算法的目标是最小化训练集上的累计误差：

         学习率 η控制每一轮迭代更新得步长，太大容易震荡，太小更新太慢，参数更新过程是迭代进行，达到某个预设定得条件即可停止，一般精度够用即可。
 标准BP算法和累计误差逆传播(accumulated error backpropagation)算法

        上边我们推导得思路是标准BP算法，是基于一个训练样例的参数优化：

         这样会使得参数更新很频繁，并且参数间容易出现抵消现象，因此有了基于积累误差最小化的更新规则，就得到了积累误差逆传播（accumulated error backpropagation）算法。累积BP算法直接针对累计误差最小化，读取整个训练集D一遍后才对参数进行更新。
 两种策略来缓解BP网络的过拟合：
        1.早停(early stopping)

        将数据分成训练集和验证集，训练集用来计算梯度、更新连接权和阈值，验证集用来估计误差，若训练集误差降低但验证集误差升高，则停止训练，同时返回具有最小验证集误差的连接权和阈值。
        2.正则化(regularization)
 全局最小和局部最小

        我们训练的最终目的就是找到一组使得误差最小的参数，梯度下降是沿着负梯度方向找值，如果全局不止一个”谷底“，则我们可能陷入局部最小而不是全局最小，实际上我们停止训练的条件就是够用即可，不追求全局最小，但是我们到达的某个局部最小可能不满足停止条件，有几种方式可以跳出局部最小：

        1.以多组不同的参数值初始化多个神经网络，按标准化方法训练后，取其中误差最小的解作为最终的参数，这相当于从多个不同的初始点开始进行探索，这样就可能陷入不同的局部极小，从中进行有可能获得更接近全局最小的结果.
        2. 使用“模拟退火”技术,“模拟退火”在每一步都会以一定的概率接受比当前更差的结果，从而有助于“跳出”局部最小，在每一步的迭代的过程中，接受“次优解”的概率要随着时间的推移而不断的降低。

        以下图为例，模拟退火算法在搜索到局部最优解B后，会以一定的概率接受向右继续移动。也许经过几次这样的不是局部最优的移动后会到达B 和C之间的D点，于是就跳出了局部最小值B。

        3.使用“随机梯度下降”,与标准梯度下降精确计算梯度不一样，随机梯度下降法在计算梯度时加入随机的因素，于是即便其陷入到局部的极小值点，他计算的梯度仍可能不为0，这样就有可能跳出局部的极小值而继续进行搜索。 
训练两个阶段

        神经网络训练和后边我们要说的深度神经网络训练一样都是根据梯度下降或者其变种来求参的，分为两个阶段：
前向传播

        根据初始化的参数W(或者上一轮迭代得出的参数)和数据计算yhat 得出总loss
后向传播

        根据loss求出每个w参数的梯度，并更新，完成一轮迭代。
神经网络到深度学习的发展历史

        但是神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。

　　90年代中期，由Vapnik等人发明的SVM（Support Vector Machines，支持向量机）算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM迅速打败了神经网络算法成为主流。

 

        神经网络的研究再次陷入了冰河期。当时，只要你的论文中包含神经网络相关的字眼，非常容易被会议和期刊拒收，研究界那时对神经网络的不待见可想而知。

        在被人摒弃的10年中，有几个学者仍然在坚持研究。这其中的棋手就是加拿大多伦多大学的Geoffery Hinton教授。

　　2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词--“深度学习”。

 　　很快，深度学习在语音识别领域暂露头角。接着，2012年，深度学习技术又在图像识别领域大展拳脚。Hinton与他的学生在ImageNet竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率15%的好成绩，这个成绩比第二名高了近11个百分点，充分证明了多层神经网络识别效果的优越性。

　　在这之后，关于深度神经网络的研究与应用不断涌现。

        目前，深度神经网络在人工智能界占据统治地位。但凡有关人工智能的产业报道，必然离不开深度学习。神经网络界当下的四位引领者除了前文所说的Ng，Hinton以外，还有CNN的发明人Yann Lecun，以及《Deep Learning》的作者Bengio。

        多层神经网络的研究仍在进行中。现在最为火热的研究技术包括RNN，LSTM等，研究方向则是图像理解方面。图像理解技术是给计算机一幅图片，让它用语言来表达这幅图片的意思。ImageNet竞赛也在不断召开，有更多的方法涌现出来，刷新以往的正确率。

        神经网络的三起三落：

         下面说一下神经网络为什么能这么火热？简而言之，就是其学习效果的强大。随着神经网络的发展，其表示性能越来越强。

　　从单层神经网络，到两层神经网络，再到多层神经网络，下图说明了，随着网络层数的增加，以及激活函数的调整，神经网络所能拟合的决策分界平面的能力。

 

         可以看出，随着层数增加，其非线性分界拟合能力不断增强。图中的分界线并不代表真实训练出的效果，更多的是示意效果。

　　神经网络的研究与应用之所以能够不断地火热发展下去，与其强大的函数拟合能力是分不开关系的。

        当然，光有强大的内在能力，并不一定能成功。一个成功的技术与方法，不仅需要内因的作用，还需要时势与环境的配合。神经网络的发展背后的外在原因可以被总结为：更强的计算性能，更多的数据，以及更好的训练方法。只有满足这些条件时，神经网络的函数拟合能力才能得已体现，见下图

        之所以在单层神经网络年代，Rosenblat无法制作一个双层分类器，就在于当时的计算性能不足，Minsky也以此来打压神经网络。但是Minsky没有料到，仅仅10年以后，计算机CPU的快速发展已经使得我们可以做两层神经网络的训练，并且还有快速的学习算法BP。

　　但是在两层神经网络快速流行的年代。更高层的神经网络由于计算性能的问题，以及一些计算方法的问题，其优势无法得到体现。直到2012年，研究人员发现，用于高性能计算的图形加速卡（GPU）可以极佳地匹配神经网络训练所需要的要求：高并行性，高存储，没有太多的控制需求，配合预训练等算法，神经网络才得以大放光彩。

　　互联网时代，大量的数据被收集整理，更好的训练方法不断被发现。所有这一切都满足了多层神经网络发挥能力的条件。

        关于神经网络中的不同类别：

        具体到前馈神经网络中，就有了本文中所分别描述的三个网络：单层神经网络，双层神经网络，以及多层神经网络。深度学习中的CNN属于一种特殊的多层神经网络。 

       其实神经网络隐藏层多了 ，即很深层的神经网络就是深度学习了，参数越多模型复杂度越高，就可以处理更加复杂的任务，这就是深度学习的优势，深度学习所具有的”学习能力“主要归功于激活函数这个非线性变换上。

        神经网络属于机器学习算法，深度学习属于机器学习的分支，下一篇我们详细讨论深度学习。
————————————————
版权声明：本文为CSDN博主「RayChiu_Labloy」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/RayChiu757374816/article/details/121351693



# CNN模型
CNN: Convolutional Neural Networks; 卷积神经网络
卷积的操作就是提取图像特征，这些特征就是输入传统神经网络输入层的数据，也就是相当于在多层感知器网络的前面加一个卷积神经网络。

* 卷积层：通过在原始图像上平移来提取特征
* 池化层：通过特征后稀疏参数来减少学习的参数，降低网络的复杂度，（最大池化和平均池化）
* 激活层：增加非线性分割能力
* 如果是分类任务，还会加上一个全连接层(FC)也就是最后的输出层，进行损失计算分类，如果不是分类任务就不需要加。



https://zhuanlan.zhihu.com/p/440893420

**复变函数中的定义**：设f(x)和g(x) 是定义在无穷区间上的两个连续时间信号，则将积分$\int_{−\infty}^{+\infty}f(\tau)g(t-\tau)d\tau$   定义为 f(x)和g(x)的卷积（Convolution），记为 $f(x)\ast g(x)$

**物理含义**：系统某一时刻的输出是由多个输入共同作用（叠加）的结果。系统对当前时刻之前的所有输入的响应的叠加。函数 f和 g 在 t时刻的卷积表征为函数 f与经过翻转和平移的 g 的重叠部分的面积。卷积即为面积、面积就是积分、积分便为叠加。所以卷积即为叠加，这解释了卷积为什么叫“积”。

图中重叠部分的面积就相当于t处的卷积
![动图](https://pic1.zhimg.com/v2-edda9924652a3d3d0268bd4d2752a6f4_b.webp)
放在图像分析里，f(x) 可以理解为**原始像素点(source pixel)**，所有的原始像素点叠加起来，就是原始图了。g(x)可以称为作用点，所有作用点合起来我们称为**卷积核（Convolution kernel）** 卷积核上所有作用点依次作用于原始像素点后（即乘起来），线性叠加的输出结果，即是最终卷积的输出，也是我们想要的结果，我们称为destination pixel.
![](https://pic4.zhimg.com/80/v2-c9b00043ba326451979abda5417bfcdf_1440w.webp)



## LeNet-5
2019：第一个成功应用于数字识别问题的卷积神经网络模型；
是卷积+池化串联的基本网络结构
论文：Lécun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.

![](https://img-blog.csdnimg.cn/img_convert/d974e13977fa6dd645725a36510def11.png)
![Image](https://mmbiz.qpic.cn/mmbiz_png/AmjGbfdONykhjgkE6jea7d6LH7xhfZZYoddxB8hz2QpWbTQxX2gibC9FV2vDbt1lFqqjS3XuuNk4CK8ibb1bY7Ww/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)
LeNet5有3个卷基层，2个池化层，2个全连接层。卷基层的卷积核都为5\*5，stride=1, 池化层都为Max pooling, 激活函数为Sigmoid

我们先约定一些叫法，比如featuremap为28\*28\*6，卷积参数大小为(5\*5\*1)\*6。其中28\*28是featuremap的高度，宽度，6是featuremap的通道数。(5\*5\*1)\*6卷积核表示5\*5的高度，宽度，通道数为1的卷积核有6个。你可以把(5\*5\*1)想象成一个厚度为1，长度，宽度各为5的卷积块，以下依此类推。

1.Input
   输入图像统一归一化为32\*32。
2.C1卷积层
   经过(5\*5\*1)\*6卷积核，stride=1, 生成featuremap为28\*28\*6。
3.S2池化层
   经过(2\*2)采样核，stride=2，生成featuremap为14\*14\*6。
4.C3卷积层
   经过(5\*5\*6)\*16卷积核，stride=1，生成featuremap为10\*10\*16。
5.S4池化层
    经过(2\*2)采样核，stride=2，生成featuremap为5\*5\*16。
6.C5卷积层    
   经过(5\*5\*16)\*120卷积核，stride=1， 生成featuremap为1\*1\*120。
7.F6全连接层
   输入为1\*1\*120，输出为1\*1\*84，总参数量为120\*84。
8.Output全连接层 。  
   输入为1\*1\*84，输出为1\*1\*10，总参数量为84\*10。10就是分类的类别数。
## AlexNet
2012：第二个经典CNN网络模型，是CNN向大规模应用的起点，宣告神经网络的归来；夺得ImageNet2012的分类冠军,奠定深度学习在图像识别中的优势地位。
相比LeNet5:激活函数从sigmoid变为relu，加入了Dropout层。
是卷积+池化串联的基本网络结构
Paper: https://mp.weixin.qq.com/s?__biz=MzUxMDkyOTU3Nw==&mid=2247483778&idx=1&sn=5753089d30045631e3e379518d235027&chksm=f97a3e44ce0db752e7bdbc6bb7f8d4d015a4c16b70d433c21a4c4b0df3c9ecc72a90eccdc411&scene=21#wechat_redirect

Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems. Curran Associates Inc. 2012:1097-1105.

![](https://img-blog.csdnimg.cn/20190108174140141.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RhaWx5X0R1YW4=,size_16,color_FFFFFF,t_70)

![Image](https://mmbiz.qpic.cn/mmbiz_png/AmjGbfdONykhjgkE6jea7d6LH7xhfZZYumvx78kpRHHhKiblvE7lIh62hjWt7Ns7JlNMzFpIviaZB95BWcutiatyQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

1.Input
   输入图像为224*224*3。

2.Conv1
   经过(11*11*3)*96卷积核，stride=4， (224-11)/4+2=55，生成featuremap为55*55*96。

3.Pool1

   经过3*3的池化核，stride=2，(55-3)/2+1=27，生成featuremap为27*27*96。

4.Norm1

   local_size=5，生成featuremap为27*27*96。

5.Conv2

   经过(5*5*96)*256的卷积核，pad=2，group=2，(27+2*2-5)/1+1=27，生成featuremap为27*27*256。

6.Pool2

   经过3*3的池化核，stride=2，(27-3)/2+1=13，生成featuremap为13*13*256。

7.Norm2

   local_size=5, 生成featuremap为13*13*256。

8.Conv3

   经过(3*3*256)*384卷积核，pad=1， (13+1*2-3)/1+1=13，生成featuremap为13*13*384。

9.Conv4

   经过(3*3*384)*384卷积核，pad=1，(13+1*2-3)/1+1=13，生成featuremap为13*13*384。

10.Conv5

     经过(3*3*384)*256卷积核，pad=1，(13+1*2-3)/1+1=13，生成featuremap为13*13*256。

11.Pool5

     经过(3*3)的池化核，stride=2，(13-3)/2+1=6，生成featuremap为6*6*256。

12.Fc6

     输入为(6*6*256)*4096全连接，生成featuremap为1*1*4096。

13.Dropout6

     在训练的时候以1/2概率使得隐藏层的某些神经元的输出为0，这样就丢掉了一半节点的输出，BP的时候也不更新这些节点，以下Droupout同理。

14.Fc7

     输入为1*1*4096，输出为1*1*4096，总参数量为4096*4096。

15.Dropout7

     生成featuremap为1*1*4096。

16.Fc8

     输入为1*1*4096，输出为1000，总参数量为4096*1000。

  

总结：

1.网络比LeNet更深，包括5个卷积层和3个全连接层。

2.使用relu激活函数，收敛很快，解决了Sigmoid在网络较深时出现的梯度弥散问题。

3.加入了dropout层，防止过拟合。

4.使用了LRN归一化层，对局部神经元的活动创建竞争机制，抑制反馈较小的神经元放大反应大的神经元，增强了模型的泛化能力。

5.使用裁剪翻转等操作做数据增强，增强了模型的泛化能力。预测时使用提取图片四个角加中间五个位置并进行左右翻转一共十幅图片的方法求取平均值，这也是后面刷比赛的基本使用技巧。

6.分块训练，当年的GPU没有这么强大，Alexnet创新地将图像分为上下两块分别训练，然后在全连接层合并在一起。

7.总体的数据参数大概为240M。

## VGG Net
证明用很小的卷积核(3\*3*)并增加网络深度(16-19层)也可有效提升模型效果，这些发现也是参加2014年ImageNet比赛的基础，并且在这次比赛中，分别在定位和分类跟踪任务中取得第一名和第二名。对其他数据集有较好泛化能力；在提出的若干年内在CV领域都成为最广泛使用的benchmark
是卷积+池化串联的基本网络结构
相比AlexNet：深度是其的2倍，参数量大小也是2倍多。
paper：Simonyan K, Zisserman A. Very Deep Convolutional Networks for Large-Scale Image Recognition[J]. Computer Science, 2014.
![](https://img-blog.csdnimg.cn/2019010817415714.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RhaWx5X0R1YW4=,size_16,color_FFFFFF,t_70)

![Image](https://mmbiz.qpic.cn/mmbiz_png/AmjGbfdONykhjgkE6jea7d6LH7xhfZZYk3wxbDXqo9TiblDsLaMajjtNfUXicXRpYRUHvW2ncGWVSbaxIezmbJdA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

类型从A到E。此处重点讲解VGG16。也就是图中的类型D。如图中所示，共有13个卷积层，3个全连接层。其全部采用3*3卷积核，步长为1，和2*2最大池化核，步长为2。

  

1.Input层

   输入图片为224*224*3。

2.CONV3-64

   经过（3*3*3）*64卷积核，生成featuremap为224*224*64。

3.CONV3-64

   经过（3*3*64）*64卷积核，生成featuremap为224*224*64。

4.Max pool

   经过（2*2）max pool核，生成featuremap为112*112*64。

5.CONV3-128。

   经过（3*3*64）*128卷积核，生成featuremap为112*112*128。

6. CONV3-128

    经过（3*3*128）*128卷积，生成featuremap为112*112*128。

7.Max pool

   经过（2*2）maxpool，生成featuremap为56*56*128。

8.CONV3-256

   经过（3*3*128）*256卷积核，生成featuremap为56*56*256。

9.CONV3-256

   经过（3*3*256）*256卷积核，生成featuremap为56*56*256。

10.CONV3-256

     经过（3*3*256）*256卷积核，生成featuremap为56*56*256。

11.Max pool

     经过（2*2）maxpool，生成featuremap为28*28*256

12.CONV3-512

     经过（3*3*256）*512卷积核，生成featuremap为28*28*512

13.CONV3-512

     经过（3*3*512）*512卷积核，生成featuremap为28*28*512。

14.CONV3-512

     经过（3*3*512）*512卷积核，生成featuremap为28*28*512。

15.Max pool

     经过（2*2）maxpool,生成featuremap为14*14*512。

16.CONV3-512

     经过（3*3*512）*512卷积核，生成featuremap为14*14*512。

17.CONV3-512

    经过（3*3*512）*512卷积核，生成featuremap为14*14*512。

18.CONV3-512

    经过（3*3*512）*512卷积核，生成featuremap为14*14*512。

19.Max pool

    经过2*2卷积，生成featuremap为7*7*512。

20.FC-4096

    输入为7*7*512，输出为1*1*4096，总参数量为7*7*512*4096。

21.FC-4096

    输入为1*1*4096，输出为1*1*4096，总参数量为4096*4096。

22.FC-1000

    输入为1*1*4096，输出为1000，总参数量为4096*1000。

  

总结：

1. 共包含参数约为550M。

2. 全部使用3*3的卷积核和2*2的最大池化核。

3. 简化了卷积神经网络的结构。

## GoogleNet
GoogLeNet夺得ImageNet2014年分类冠军，也被称为**Inception V1**。Inception V1有22层深，参数量为5M。同一时期的VGGNet性能和Inception V1差不多，但是参数量却远大于Inception V1。Inception的优良特性得益于Inception Module，结构如下图：

![Image](https://mmbiz.qpic.cn/mmbiz_png/AmjGbfdONyl7uiacPC0OAWu3lic1x0MjBTy5JnB7Qgibpm86QicycAKhbzalqr6WQXpzxS7VUly2c971nEe7z1Plww/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

  

Inception Module基本组成结构有四个成分。由1\*1卷积，3\*3卷积，5\*5卷积，3\*3最大池化四个并行通道运算结果进行融合，提取图像不同尺度的信息。最后对四个成分运算结果进行通道上组合。这就是Inception Module的核心思想。**通过多个卷积核提取图像不同尺度的信息，最后进行融合，可以得到图像更好的表征**。如果说VGG是以深度取胜，那么GoogLeNet可以说是以宽度取胜，当然1\*1卷积起到了很大的作用，这一点在SqueezeNet中也很关键。详细解读如下

如上图所示，假设我们要提取猫脸特征，而上面两张图的猫脸占比显然不一样，那么我们就得用不同卷积核提取不同信息。信息分布比较全局性的图像采用大卷积核，信息分布比较局部性的图像采用小卷积核。  

图b是对图a的改进，即在3*3卷积，5*5卷积前加1*1卷积，目的是为了先进行降维，相比较于原来结构减少了较多参数。而把1*1卷积放在3*3最大池化之后，相比较放在前面，也是为了参数量的减少。

  

由Inception Module组成的GoogLeNet如下图：

![Image](https://mmbiz.qpic.cn/mmbiz_jpg/AmjGbfdONylaJI4AfmXHiaKcAgFrU5m2cRVFJncaaqmZia4MYnjCKIKpTaGlb4klRrhy7WEBDIDhGNpicjt6o0ia6g/640?wx_fmt=jpeg&wxfrom=5&wx_lazy=1&wx_co=1)

对上图做如下说明：

1.  采用模块化结构，方便增添和修改。其实网络结构就是叠加Inception Module。

2.采用Network in Network中用Averagepool来代替全连接层的思想。实际在最后一层还是添加了一个全连接层，是为了大家做finetune。

3.依然使用Dropout层，防止过拟合。

4.另外增加了两个辅助的softmax分支，作用有两点，一是为了避免梯度消失，用于向前传导梯度。反向传播时如果有一层求导为0，链式求导结果则为0。二是将中间某一层输出用作分类，起到模型融合作用。最后的loss=loss_2 + 0.3 * loss_1 + 0.3 * loss_0。实际测试时，这两个辅助softmax分支会被去掉。

  

02

  

Inception V2【2】

1.学习VGGNet的特点，用两个3*3卷积代替5*5卷积，可以降低参数量。

2.提出BN算法。BN算法是一个正则化方法，可以提高大网络的收敛速度。简单介绍一下BN算法。就是对输入层信息分布标准化处理，使得规范化为N(0,1)的高斯分布，收敛速度大大提高。

  

03

  

Inception V3【3】

学习Factorization into small convolutions的思想，将一个二维卷积拆分成两个较小卷积，例如将7*7卷积拆成1*7卷积和7*1卷积。这样做的好处是降低参数量。paper中指出，通过这种非对称的卷积拆分，比对称的拆分为几个相同的卷积效果更好，可以处理更多，更丰富的空间特征。

  

本来还有Inception V4【4】的，考虑到借鉴了微软的ResNet网络结构思想，在后面介绍Resnet中的残差结构时再做介绍。

【1】Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[C]// IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2015:1-9.

Ioffe S, Szegedy C. 

【2】Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift[J]. 2015:448-456.

【3】Szegedy C, Vanhoucke V, Ioffe S, et al. Rethinking the Inception Architecture for Computer Vision[J]. 2015:2818-2826.

【4】Szegedy C, Ioffe S, Vanhoucke V, et al. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning[J]. 2016.

## ResNet
网络深度增加时网络准确度出现饱和甚至下降，ResNet提出残差学习解决退化问题

## 总结

LeNet5，AlexNet，VGGNet，这三个网络结构本质上都是（卷积+池化）堆叠的网络结构，是深度学习复兴以来的第一个有重大工程意义的网络设计系列。

时序模型在语音，视频以及自然语言处理等领域有不可替代的作用，虽然相比普通的CNN，模型的复杂度和训练难度都增加了不少，但是在进阶之路上也是需要好好掌握的

# RNN模型

卷积神经网络使用固定大小的矩阵作为输入（比如一张图片），然后输出一个固定大小的向量（比如不同分类的概率），适合于图像分类，目标检测，图像分割等。但是除了图像外，还有非常多的信息是非固定长度或者大小的，比如视频，语音，此时更加适合用来处理这些时序信号的网络就是一些时间序列模型。

常见的时间序列模型包括RNN，LSTM等。


我们通常所说的RNN实际上有两种，一种是Recurrent Neural Networks，即**循环神经网络**，一种是Recursive Neural Networks，即**递归神经网络**。

  

循环神经网络是首先被提出的，它是一种**时间上进行线性递归**的神经网络，也就是我们通常所说的RNN。

  

递归神经网络（recursive neural network）被视为循环神经网络（recurrent neural network）的推广，这是一种在**结构上进行递归**的神经网络，常用于自然语言处理中的序列学习，它的输入数据本质不一定是时序的，但结构却往往更加复杂，我们这里只说循环神经网络。  

  

一个RNN的结构如下：

![Image](https://mmbiz.qpic.cn/mmbiz_png/AmjGbfdONyk4KrbeVHgpQcXnV9rapkBImAJBeCrNodqrWaFOmD3BIunLplTXdlMLmj8B6ThR5cOicdt0Tc1WEfw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

左侧就是模型的基本结构，右侧就是它在时间上进行展开的示意图。xt是时刻t的输入，相应的ht，ot分别是对应时刻t的隐藏层和输出层。

  

上面我们可以看出，一个RNN的输入包括了两个：一个是**当前时刻输入xt，用于实时更新状态，另一个是****上一时刻隐藏层的状态ht-1，用于记忆状态，而不同时刻的网络共用的是同一套参数。**

  

RNN中常用的激活函数是tanh，所以上面的式子写成公式，就是：

![Image](https://mmbiz.qpic.cn/mmbiz_png/AmjGbfdONyk4KrbeVHgpQcXnV9rapkBIoOWMLXwSH0x0RRrpf0ic2PFziavD2uH1sKyyEZrHh8EG0MibT5vvngibNA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

w就是要学习的权重，用几句代码表示RNN就是。

class RNN: 

   def step(self, x): 

      self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x)) #更新隐藏层

      y = np.dot(self.W_hy, self.h) #得到输出

   return y

  

普通卷积神经网络的优化使用的是反向传播，那么RNN使用的是什么呢？最常见的还是反向传播，不过是带时序的版本，即**BPFT（backpropagation through time）**，它与BP的原理是完全一样的，只不过计算过程与时间有关。

  

与普通的反向传播算法一样，它重复地使用链式法则，区别在于损失函数不仅依赖于当前时刻的输出层，也依赖于下一时刻。所以参数W在更新梯度时，必须考虑当前时刻的梯度和下一时刻的梯度，传播示意图如下；

![Image](https://mmbiz.qpic.cn/mmbiz_png/AmjGbfdONyk4KrbeVHgpQcXnV9rapkBIOibTxw4hHmgwInCBibjO67eTSPWxdPVnsG7iaejPa8KYRkJHMrBEiaJcTA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

具体的公式我们就不编辑了，大家可以找书看，之所以有后续的LSTM等，就是因为RNN有大问题：因为t时刻的导数会传播到t-1，t-2，... ，1时刻，这样就有了连乘的系数。

  

连乘一直带来了两个问题：**梯度爆炸和消失**。而且，在前向过程中，开始时刻的输入对后面时刻的影响越来越小，这就是长距离依赖问题。这样一来，就失去了**“记忆”**的能力，要知道生物的神经元拥有对过去时序状态很强的记忆能力。

# LSTM

前面说的RNN有两个问题，长短期记忆（Long short-term memory, LSTM）就是要解决这两个问题，通过引入若干门来解决，相比RNN多了一个**状态cell state**。

  

这个cell state承载着之前所有状态的信息，每到新的时刻，就有相应的操作来决定舍弃什么旧的信息以及添加什么新的信息。这个状态与隐藏层状态h不同，在更新过程中，它的更新是缓慢的，而隐藏层状态h的更新是迅速的。

  

LSTM的网络结构图如下，输入包括ht-1，xt，输出ht，状态为ct-1，ct。

![Image](https://mmbiz.qpic.cn/mmbiz_png/AmjGbfdONyk4KrbeVHgpQcXnV9rapkBIVlbvQz9sjYRjJTJia2jeXB5qxtg5fc0sxoPNEFVIKfcsu63myT5RDfw/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**2.1  遗忘门与遗忘阶段**  

遗忘门决定了要从上一个状态中舍弃什么信息，它输入上一状态的输出ht-1、当前状态输入信息xt到一个Sigmoid函数中，产生一个介于0到1之间的数值，与上一个时刻的状态ct-1相乘之后来确定舍弃（保留）多少信息。0 表示“完全舍弃”，1 表示“完全保留”，这个阶段完成了对上一个节点cell state进行选择性忘记，遗忘门和它的输出公式如下：  

![Image](https://mmbiz.qpic.cn/mmbiz_png/AmjGbfdONyk4KrbeVHgpQcXnV9rapkBILTFeIZrq6tELQrKcicicG1q6pYVRBxM1lxicrPtayjBibeG2pqibVia3wic0w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**2.2 输入门与选择记忆阶段**

选择记忆阶段，也就是对输入有选择性地进行“记忆”，重要的记录下来，不重要的少记一些，它决定了要往当前状态中保存什么新的信息。它输入上一状态的输出ht-1、当前输入信息xt到一个Sigmoid函数中，产生一个介于0到1之间的数值it来确定需要保留多少的新信息。

  

“候选新信息”则通过输入上一状态的输出、当前状态输入信息和一个tanh激活函数生成。有了遗忘门和输入门之后，就得到了完整的下一时刻的状态Ct，它将用于产生下一状态的隐藏层ht，也就是当前单元的输出。

![Image](https://mmbiz.qpic.cn/mmbiz_png/AmjGbfdONyk4KrbeVHgpQcXnV9rapkBIPNj4s7alzgicZx3fyPlErd3xaMuia1nx3tB3LebSlPYZccdSnVDcL5Ag/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![Image](https://mmbiz.qpic.cn/mmbiz_png/AmjGbfdONyk4KrbeVHgpQcXnV9rapkBIPpMQwDxwNd5EOQPFiajZr9dTn5YCWDprw8tUMp12eh1eDz1yERymhzg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

![Image](https://mmbiz.qpic.cn/mmbiz_png/AmjGbfdONyk4KrbeVHgpQcXnV9rapkBIOwJmMSr02WZNb0MrHYz7qqvdMdiciaKBzicNHicfbbkWD7KNo1wF98TXlA/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

**2.3 输出门与输出阶段**

输出门决定了要从cell state中输出什么信息。与之前类似，会先有一个Sigmoid函数产生一个介于0到1之间的数值Ot来确定我们需要输出多少cell state中的信息。cell state的信息在与Ot相乘时首先会经过一个tanh层进行“激活”，得到的就是这个LSTM block的输出信息ht。

![Image](https://mmbiz.qpic.cn/mmbiz_png/AmjGbfdONyk4KrbeVHgpQcXnV9rapkBIy2jlFkVvmQic59BGf26RftIfVB5tZVYuzBBQQlqaGYsgx6ulxIkGZ6w/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

  

以上就是LSTM的基本原理，它通过门控状态来对信息进行选择性的记忆，满足了需要长时间记忆信息和遗忘信息的需求。

  

当然，随之而来的就是大量的参数，因此后续就有了GRU。另外，RNN和LSTM不止有单向的，还有双向的，这些就留给读者自己去学习了。
# 深度生成式模型

## GAN
GAN: Generative Adversarial Networks; 生成式对抗网络; 无监督学习; [Paper](https://arxiv.org/abs/1406.2661)

**核心原理**：
GAN的基本原理其实非常简单，这里以生成图片为例进行说明。
假设有两个网络，G (Generator）和D (Discriminator）。正如它的名字所暗示的那样，它们的功能分别是：
* G是一个生成图片的网络，它接收一个随机的噪声z，通过这个噪声生成图片，记做G(Z)。
* D是一个判别网络，判别一张图片是不是”真实的”。它的输入参数是x，x代表一张图片，输出D(x）代表x为真实图片的概率，如果为1，就代表100%是真实的图片，而输出为0，就代表不可能是真实的图片。
在训练过程中，生成网络G的目标就是尽量生成真实的图片去欺骗判别网络D。而D的目标就是尽量把G生成的图片和真实的图片分别开来。这样，G和D构成了一个动态的”博弈过程”
最后博弈的结果是什么？在最理想的状态下，G可以生成足以“以假乱真”的图片G(z)。对于D来说，它难以判定G生成的图片究竟是不是真式的，因此D(G(Z))= 0.5。
这样我们的目的就达成了：我们得到了一个生成式的模型G，它可以用来生成图片。
原始 GAN 理论中，并不要求G 和D 都是神经网络，只需要是能拟合相应生成和判别的西数即可。但实用中一般均使用深度神经网络作为G和D。一个优秀的GAN应用需要有良好的训练方法，否则可能由于神经网络模型的自由性而导致输出不理想。
**生成器**：
对于生成器，输入需要一个n维度向量，输出为图片像素大小的图片。因而首先我们需要得到输入的向量。这里的生成器可以是任意可以输出图片的模型，比如最简单的全连接神经网络，又或者是反卷积网络等。这里输入的向量我们将其视为携带输出的某些信息，比如说手写数字为数字几，手写的潦草程度等等。由于这里我们对于输出数字的具体信息不做要求，只要求其能够最大程度与真实手写数字相似（能骗过判别器）即可。所以我们使用随机生成的向量来作为输入即可，这里面的随机输入最好是满足常见分布比如均值分布，高斯分布等。
假如我们后面需要获得具体的输出数字等信息的时候，我们可以对输入向量产生的输出进行分析，获取到哪些
维度是用手控制数宇编号等信息的即可以得到具体的输出。而在训练之前往往不会去规定它。
![](https://img-blog.csdnimg.cn/20200329123216560.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RGQ0VE,size_16,color_FFFFFF,t_70)
判别器：
对于判别器不用多说，往往是常见的判别器，输入为图片，输出为图片的真伪标签。同理，判别器与生成器一样，可以是任意的判别器模型，比如全连接网络，或者是包含卷积的网络等
![](https://img-blog.csdnimg.cn/20200329123337357.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RGQ0VE,size_16,color_FFFFFF,t_70)

**数学描述**：
![image](https://pic1.zhimg.com/80/v2-f98f1d3caabbca9b6baa4235c40150b4_1440w.webp)

- 整个式子由两项构成。x表示真实图片，z表示输入G网络的噪声，而G(z)表示G网络生成的图片。
- D(x)表示D网络判断**真实图片是否真实**的概率（因为x就是真实的，所以对于D来说，这个值越接近1越好）。而D(G(z))是**D网络判断G生成的图片的是否真实的概率。**
- G的目的：D(G(z))是**D网络判断G生成的图片是否真实的概率**，G应该希望自己生成的图片“越接近真实越好”。也就是说，G希望D(G(z))尽可能得大，这时V(D, G)会变小。因此我们看到式子的最前面的记号是min_G。
- D的目的：D的能力越强，D(x)应该越大，D(G(x))应该越小。这时V(D,G)会变大。因此式子对于D来说是求最大(max_D)


图片描述：
![](https://pic4.zhimg.com/80/v2-95c709a87749e0778248fc8fdd289b83_1440w.webp)
**GAN的训练：**
![](https://img-blog.csdnimg.cn/20200329123951264.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0RGQ0VE,size_16,color_FFFFFF,t_70)
那么如何用随机梯度下降法训练D和G？论文中也给出了算法：

1.在噪声数据分 布中随机采样，输入生成模型，得到一组假数据，记为D(z)
2.在真实数据分布中随机采样，作为真实数据，记做x;
将前两步中某一步产生的数据作为判别网络的输入（因此判别模型的输入为两类数据，真/假），判别网络的
输出值为该输入属于真实数据的概率，real为1， fake为0.
3.然后根据得到的概率值计算损失函数；
4. 根据判别模型和生成模型的损失函数，可以利用反向传播算法，更新模型的参数。（先更新判别模型的参
数，然后通过再采样得到的噪声数据更新生成器的参数）

![](https://pic3.zhimg.com/80/v2-78851777a659db4821695242cd39b42e_1440w.webp)

这里红框圈出的部分是我们要额外注意的。第一步我们训练D，D是希望V(G, D)越大越好，所以是加上梯度(ascending)。第二步训练G时，V(G, D)越小越好，所以是减去梯度(descending)。整个训练过程交替进行。

- Ian Goodfellow对GAN一系列工作总结的ppt，确实精彩，推荐：[独家 | GAN之父NIPS 2016演讲现场直击：全方位解读生成对抗网络的原理及未来（附PPT）](https://link.zhihu.com/?target=http%3A//it.sohu.com/20161210/n475485860.shtml)
- GAN论文汇总，包含code：[zhangqianhui/AdversarialNetsPapers](https://link.zhihu.com/?target=https%3A//github.com/zhangqianhui/AdversarialNetsPapers)


GAN的损失函数

GAN算法的不足：
1.可解释性差,生成模型的分布 Pg(G) 没有显式的表达
2.比较难训练，D与G之间需要很好的同步（例如D更新k次而G更新一次)，GAN模型被定义为极小极大问题，
没有损失西数，在训练过程中很难区分是否正在取得进展。GAN的学习过程可能发生崩溃问题 (collapse
problem），生成器开始退化，总是生成同样的样本点，无法继续学习。当生成模型崩溃时，判别模型也
会对相似的样本点指向相似的方向，训练无法继续。
3. 网络难以收敛，目前所有的理论都认为GAN应该在纳什均衡上有很好的表现，但梯度下降只有在凸函数的
情况下才能保证实玧纳什均衡。
4. 训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到-还没有找到很好的达到纳什均衡
的方法,所以训练GAN相比VAE或者PixeIRNN是不稳定的,但在实践中它还是比训练玻尔兹曼机稳定的多
5. 它很难去学习生成离散的数据,就像文本
6. 相比玻尔兹曼机,GANs很难根据一个像素值去猜测另外一个像素值，GANs天生就是做一件事的,那就是一次
产生所有像素，你可以用BiGAN来修正这个特性,它能让你像使用玻尔兹曼机一样去使用Gibbs采样来猜测
缺失值
## DCGAN
我们知道深度学习中对图像处理应用最好的模型是CNN，那么如何把CNN与GAN结合？DCGAN是这方面最好的尝试之一（论文地址：[[1511.06434] Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1511.06434)）

DCGAN的原理和GAN是一样的，这里就不在赘述。它只是把上述的G和D换成了两个卷积神经网络（CNN）。但不是直接换就可以了，DCGAN对卷积神经网络的结构做了一些改变，以提高样本的质量和收敛的速度，这些改变有：
- 取消所有pooling层。G网络中使用转置卷积（transposed convolutional layer）进行上采样，D网络中用加入stride的卷积代替pooling。
- 在D和G中均使用batch normalization
- 去掉FC层，使网络变为全卷积网络
- G网络中使用ReLU作为激活函数，最后一层使用tanh
- D网络中使用LeakyReLU作为激活函数

DCGAN中的G网络示意：

![](https://pic2.zhimg.com/80/v2-1c06594f38b896e8d15154592bae0309_1440w.webp)

## Diffusion Model
综述：https://m.thepaper.cn/baijiahao_19930487
马尔可夫链


Prompt: 生成式AI模型的输入源，能够控制输入的内容

foundation model

应用
* CV：图像补全修复Repaint
* NLP
* 多模态：从文本生成图像
* 分子图生成：药物分子和蛋白质分子生成





# Transformer
https://pypi.org/project/transformers/
论文：[Attention is all your need](https://arxiv.org/abs/1706.03762),是谷歌TPU推荐的参考模型。
Project; Code
NLP领域主要存在三种特征处理器：CNN，RNN, LSTM和Transformer。RNN和LSTM是主要用于处理序列类型数据的经典模型。当前Transformer流行程序已大过CNN和RNN，它抛弃传统CNN，RNN的神经网络。

整个网络结构完全由Attention机制及前馈神经网络组成。Transformer模型是一种用于处理序列数据的架构。采用自注意力机制(slef-attention)建立输入序列中各个元素之间的关联性。通过对输入序列中的单词或标记进行编码和解码，transformer能捕捉到单词之间的语义和语法关系，从而生成连贯的，上下文相关的回答。

我们可以下载这些大语言模型，通过微调在自己的自然语言处理任务上使用他们。如语音识别，文本分类，情感分类，命名实体识别，机器翻译，文本摘要，文本生成等
应用：GPT
最早是用于机器翻译
https://zhuanlan.zhihu.com/p/82312421
![](image/Transformer模型架构.png)
![](https://pic3.zhimg.com/80/v2-24f6ff6ca0b79a5a6adc5bfd083be8a2_1440w.webp)
**输入层：** Encoder(编码) 和 Decoder(解码) 的输入都是**单词的 Embedding 向量** 和 **位置编码**（Positional Encoding，为了像 RNN 那样捕获输入序列的顺序信息）；不同的是，Encoder 的**初始输入**是训练集的 X ，Decoder 的**初始输入**是训练集的标签 Y ，并且需要整体右移（Shifted Right）一位（原因后文具体介绍）。此外在 Decoder 中，**第二子层**的输入为 **Encoder 的输出**（key 向量和 value 向量）以及**前一子层的输出**（query 向量）。

**Encoder**：该模块可以分为两部分： **Self-Attention 层和全连接层**；此外又加了一些额外的处理，如**残差连接（residual connection）、Layer Normalization层**。为了便于残差连接，作者将所有层的输出维度都定义为 ������=512 （包括 Embedding 层的输出和位置编码的维度）。这个结构可以循环 � 次（文中 �=6 ）。

**Decoder**：该模块可以分成三部分：第一部分是 Self-Attention 层 （这里添加了masking 操作，以防止时间穿越，后文会详细讲解），第二部分是 **Encoder-Decoder Attention**（这是因为输入中 key，value 向量来源于 Encoder，query 向量来源于Decoder），第三部分是全连接层；也用了残差连接和 Normalization。同样，该结构可以循环 � 次。

**输出层**：最后的输出要通过Linear层（全连接层），再通过 softmax 做预测。

## GPT
GPT：Generative Pre-trained Transformer，即预训练生成模型。
GPT是一种基于Transformer模型的生成式预训练语言模型，旨在通过学习大量语料库来生成合理、连贯的文本。
应用：chatgpt。ChatGPT的应用场景非常广泛，比如对话机器人、自动翻译、自动摘要、文本生成等等。

chatglm
**Github:[https://github.com/THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)**  
**Hugging Face Hub(模型地址):[https://huggingface.co/THUDM/chatglm-6b](https://huggingface.co/THUDM/chatglm-6b)**


## 大语言模型
中文通用大模型综合性基准: https://github.com/CLUEbenchmark/SuperCLUE

## 大语言模型微调方法
* Adapter Tuning \[2019\]
* **LoRA** \[2021\] 🌟
* Prefix-tuning \[2021\]
* Prompt-tuning \[2021\]
* p-tuning \[2022\]
* p-tuning v2 \[2022\]
* AdaLora \[2023\]
## LlaMa

https://github.com/facebookresearch/llama
https://github.com/michael-wzhu/Chinese-LlaMA2
https://github.com/ymcui/Chinese-LLaMA-Alpaca-2

# Foundation model

foundation model :  广泛使用的基础模型，用海量数据金额计算资源基础上训练出来的通用性较强的深度学习模型
## BERT
340M参数