{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKuNhLV58cOjAVMVnuNHXR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MingxiaGuo/Artifical_Intelligence/blob/main/ER_NeRF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ER-NeRF\n",
        "\n",
        "Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis\n",
        "\n",
        "ICCV23 | [Github](https://github.com/Fictionarry/ER-NeRF) | [Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Li_Efficient_Region-Aware_Neural_Radiance_Fields_for_High-Fidelity_Talking_Portrait_Synthesis_ICCV_2023_paper.html) | [Project](https://github.com/Fictionarry/ER-NeRF) | [ArXiv](https://arxiv.org/abs/2307.09323) | [Video](https://arxiv.org/abs/2307.09323)"
      ],
      "metadata": {
        "id": "Pb3oD6b737KC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Clone repo"
      ],
      "metadata": {
        "id": "Y7ta65rS378K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Fictionarry/ER-NeRF.git\n",
        "%cd ER-NeRF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txhIi_CInlZ4",
        "outputId": "3b93e296-b26a-4c28-e4df-9ee50c9104de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ER-NeRF'...\n",
            "remote: Enumerating objects: 344, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 344 (delta 31), reused 17 (delta 8), pack-reused 284\u001b[K\n",
            "Receiving objects: 100% (344/344), 24.98 MiB | 31.74 MiB/s, done.\n",
            "Resolving deltas: 100% (152/152), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Download models"
      ],
      "metadata": {
        "id": "ZdniARnK3XGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare face-parsing model.\n",
        "!wget https://github.com/YudongGuo/AD-NeRF/blob/master/data_util/face_parsing/79999_iter.pth?raw=true -O data_utils/face_parsing/79999_iter.pth\n",
        "# Prepare the 3DMM model for head pose estimation.\n",
        "!mkdir data_utils/face_tracking/3DMM\n",
        "!wget https://github.com/YudongGuo/AD-NeRF/blob/master/data_util/face_tracking/3DMM/exp_info.npy?raw=true -O data_utils/face_tracking/3DMM/exp_info.npy\n",
        "!wget https://github.com/YudongGuo/AD-NeRF/blob/master/data_util/face_tracking/3DMM/keys_info.npy?raw=true -O data_utils/face_tracking/3DMM/keys_info.npy\n",
        "!wget https://github.com/YudongGuo/AD-NeRF/blob/master/data_util/face_tracking/3DMM/sub_mesh.obj?raw=true -O data_utils/face_tracking/3DMM/sub_mesh.obj\n",
        "!wget https://github.com/YudongGuo/AD-NeRF/blob/master/data_util/face_tracking/3DMM/topology_info.npy?raw=true -O data_utils/face_tracking/3DMM/topology_info.npy\n"
      ],
      "metadata": {
        "id": "X09-MXiZr93t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare 3DMM model from Basel Face Model 2009\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/01_MorphableModel.mat data_utils/face_tracking/3DMM/01_MorphableModel.mat\n",
        "%cd /content/ER-NeRF/data_utils/face_tracking\n",
        "!python convert_BFM.py"
      ],
      "metadata": {
        "id": "1AwjVo04sF9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12e85afe-7ea3-4e7a-a73b-ad89611c8216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Install dependencies"
      ],
      "metadata": {
        "id": "YPtaZqbR3ODd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI7r7M0hr5Cm"
      },
      "outputs": [],
      "source": [
        "\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-py310_23.10.0-1-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-py310_23.10.0-1-Linux-x86_64.sh -b -f -p /usr/local\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.10/site-packages/')\n",
        "!conda create --name ERNeRF python=3.10 -y\n",
        "!source activate ERNeRF\n",
        "!conda install pytorch==1.12.1 torchvision==0.13.1 cudatoolkit=11.3 -c pytorch\n",
        "!apt-get install portaudio19-dev\n",
        "!apt-get install python3-all-dev\n",
        "!pip install -r requirements.txt\n",
        "!pip install \"git+https://github.com/facebookresearch/pytorch3d.git\"\n",
        "!pip install tensorflow-gpu==2.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Optional: Prepare dateset"
      ],
      "metadata": {
        "id": "DrvjwKd230UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo Datasets\n",
        "!mkdir -p data/obama\n",
        "!wget https://github.com/YudongGuo/AD-NeRF/blob/master/dataset/vids/Obama.mp4?raw=true -O data/obama/obama.mp4"
      ],
      "metadata": {
        "id": "XsOpnR_HzjmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Pre-processing video/audio"
      ],
      "metadata": {
        "id": "uruCxWk22DuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ER-NeRF\n",
        "#  Pre-process video & audio\n",
        "# 4.1 Put training video under data/<ID>/<ID>.mp4\n",
        "#     The video must be 25FPS, with all frames containing the talking person.\n",
        "#     The resolution should be about 512x512, and duration about 1-5 min.\n",
        "# 4.2 Run script to process the video. (may take several hours)\n",
        "!python data_utils/process.py data/<ID>/<ID>.mp4\n",
        "\n",
        "# 4.3 Obtain AU45 for eyes blinking\n",
        "#     Run FeatureExtraction in OpenFace, rename and move the output CSV file to data/<ID>/au.csv\n",
        "\n",
        "#  Pre-process audio\n",
        "# !python data_utils/deepspeech_features/extract_ds_features.py --input data/<name>.wav # save to data/<name>.npy\n",
        "# Borrowed from GeneFace. English pre-trained.\n",
        "# !python data_utils/hubert.py --wav data/<name>.wav # save to data/<name>_hu.npy"
      ],
      "metadata": {
        "id": "LyYpCuhC2UIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Training"
      ],
      "metadata": {
        "id": "_hg7-pS32qJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.1 train (head and lpips finetune, run in sequence)\n",
        "!python main.py data/obama/ --workspace trial_obama/ -O --iters 100000\n",
        "!python main.py data/obama/ --workspace trial_obama/ -O --iters 125000 --finetune_lips --patch_size 32\n",
        "\n",
        "# 7.2 train (torso)\n",
        "# <head>.pth should be the latest checkpoint in trial_obama\n",
        "!python main.py data/obama/ --workspace trial_obama_torso/ -O --torso --head_ckpt <head>.pth --iters 200000"
      ],
      "metadata": {
        "id": "iRKoxFvK2slg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Testing"
      ],
      "metadata": {
        "id": "RVEdltrT3Acl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test on the test split\n",
        "python main.py data/obama/ --workspace trial_obama/ -O --test # only render the head and use GT image for torso\n",
        "python main.py data/obama/ --workspace trial_obama_torso/ -O --torso --test # render both head and torso"
      ],
      "metadata": {
        "id": "51ZswizP2-JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. inference with target audio"
      ],
      "metadata": {
        "id": "PkL64re43IeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding \"--smooth_path\" may help decrease the jitter of the head, while being less accurate to the original pose.\n",
        "python main.py data/obama/ --workspace trial_obama_torso/ -O --torso --test --test_train --aud <audio>.npy"
      ],
      "metadata": {
        "id": "PyF21zQH3KLc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}