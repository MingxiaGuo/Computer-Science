{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MingxiaGuo/Artifical_Intelligence/blob/main/OpenCV_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jjjm2dv4IHT"
   },
   "source": [
    "# OpenCV\n",
    "\n",
    "[Documentation](https://docs.opencv.org/4.x/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2fxA4lp-3u2"
   },
   "source": [
    "## Install OpenCV and imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KAxFA8bi39C1",
    "outputId": "a3104092-f9ea-4aeb-ca28-12b1e0d1d477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python in /Users/gmx/anaconda3/lib/python3.11/site-packages (4.8.0.76)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/gmx/anaconda3/lib/python3.11/site-packages (from opencv-contrib-python) (1.24.3)\n",
      "Requirement already satisfied: imutils in /Users/gmx/anaconda3/lib/python3.11/site-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-contrib-python # Install OpenCV\n",
    "!pip install imutils #Install image processing tools, https://github.com/jrosebr1/imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and displaying an image\n",
    "\n",
    "All images consist of pixels which are the raw building blocks of images. Images are made of pixels in a grid. A 640 x 480 image has 640 columns (the width) and 480 rows (the height). There are 640 * 480 = 307200 pixels in an image with those dimensions.\n",
    "\n",
    "Each pixel in a grayscale image has a value representing the shade of gray. In OpenCV, there are 256 shades of gray — from 0 to 255. So a grayscale image would have a grayscale value associated with each pixel.\n",
    "\n",
    "Pixels in a color image have additional information. There are several color spaces that you’ll soon become familiar with as you learn about image processing. For simplicity let’s only consider the RGB color space.\n",
    "\n",
    "In OpenCV color images in the RGB (Red, Green, Blue) color space have a 3-tuple associated with each pixel: (B, G, R) .\n",
    "\n",
    "Notice the ordering is BGR rather than RGB. This is because when OpenCV was first being developed many years ago the standard was BGR ordering. Over the years, the standard has now become RGB but OpenCV still maintains this “legacy” BGR ordering to ensure no existing code breaks.\n",
    "\n",
    "Each value in the BGR 3-tuple has a range of [0, 255] . How many color possibilities are there for each pixel in an RGB image in OpenCV? That’s easy: 256 * 256 * 256 = 16777216 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "40LtSKZE-xcl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width=1011, height=1409, depth=3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "import imutils\n",
    "import cv2\n",
    "# load the input image and show its dimensions, keeping in mind that\n",
    "# images are represented as a multi-dimensional NumPy array with\n",
    "# shape no. rows (height) x no. columns (width) x no. channels (depth)\n",
    "# We describe matrices by # of rows x # of columns, The number of rows is our height, And the number of columns is our width\n",
    "# Depth is the number of channels — in our case this is three since we’re working with 3 color channels: Blue, Green, and Red\n",
    "image = cv2.imread(\"TITANIC.JPG\")\n",
    "(h, w, d) = image.shape\n",
    "print(\"width={}, height={}, depth={}\".format(w, h, d))\n",
    "# display the image to our screen -- we will need to click the window\n",
    "# open by OpenCV and press a key on our keyboard to continue execution\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing individual pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R=187, G=207, B=182\n"
     ]
    }
   ],
   "source": [
    "# access the RGB pixel located at x=50, y=100, keepind in mind that\n",
    "# OpenCV stores images in BGR order rather than RGB\n",
    "(B, G, R) = image[100, 50]\n",
    "print(\"R={}, G={}, B={}\".format(R, G, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array slicing and cropping\n",
    "\n",
    "Extracting “regions of interest” (ROIs) is an important skill for image processing.\n",
    "\n",
    "Say, for example, you’re working on recognizing faces in a movie. First, you’d run a face detection algorithm to find the coordinates of faces in all the frames you’re working with. Then you’d want to extract the face ROIs and either save them or process them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract a 150x150 pixel square ROI (Region of Interest) from the\n",
    "# input image starting at x=500,y=400 at ending at x=650,y=550\n",
    "roi = image[400:550, 500:650] # image[startY:endY, startX:endX]\n",
    "cv2.imshow(\"ROI\", roi)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resizing images\n",
    "\n",
    "Resizing images is important for a number of reasons. First, you might want to resize a large image to fit on your screen. Image processing is also faster on smaller images because there are fewer pixels to process. In the case of deep learning, we often resize images, ignoring aspect ratio, so that the volume fits into a network which requires that an image be square and of a certain dimension.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# resize the image to 200x200px, ignoring aspect ratio\n",
    "resized = cv2.resize(image, (200, 200))\n",
    "cv2.imshow(\"Fixed Resizing\", resized)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fixed resizing and distort aspect ratio so let's resize the width\n",
    "# to be 300px but compute the new height based on the aspect ratio\n",
    "r = 300.0 / w\n",
    "dim = (300, int(h * r))\n",
    "resized = cv2.resize(image, dim)\n",
    "cv2.imshow(\"Aspect Ratio Resize\", resized)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually computing the aspect ratio can be a pain so let's use the\n",
    "# imutils library instead\n",
    "resized = imutils.resize(image, width=300)\n",
    "cv2.imshow(\"Imutils Resize\", resized)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotating an image\n",
    "\n",
    "Rotating an image about the center point requires that we first calculate the center (x, y)-coordinates of the image.\n",
    "\n",
    "We use // to perform integer math (i.e., no floating point values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's rotate an image 45 degrees clockwise using OpenCV by first\n",
    "# computing the image center, then constructing the rotation matrix,\n",
    "# and then finally applying the affine warp\n",
    "center = (w // 2, h // 2) # compute the center point using the image width and height\n",
    "M = cv2.getRotationMatrix2D(center, -45, 1.0) # compute a rotation matrix with cv2.getRotationMatrix2D, \n",
    "# The -45 means that we’ll rotate the image 45 degrees clockwise. \n",
    "# Recall from your middle/high school geometry class about the unit circle and you’ll be able to remind yourself that positive angles are counterclockwise and negative angles are clockwise.\n",
    "rotated = cv2.warpAffine(image, M, (w, h)) # use the rotation matrix to warp the image with cv2.warpAffine.\n",
    "cv2.imshow(\"OpenCV Rotation\", rotated)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation can also be easily accomplished via imutils with less code\n",
    "rotated = imutils.rotate(image, -45)\n",
    "cv2.imshow(\"Imutils Rotation\", rotated)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenCV doesn't \"care\" if our rotated image is clipped after rotation\n",
    "# so we can instead use another imutils convenience function to help\n",
    "# us out\n",
    "rotated = imutils.rotate_bound(image, 45)\n",
    "cv2.imshow(\"Imutils Bound Rotation\", rotated)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing an image\n",
    "\n",
    "In many image processing pipelines, we must blur an image to reduce high-frequency noise, making it easier for our algorithms to detect and understand the actual contents of the image rather than just noise that will “confuse” our algorithms. Blurring an image is very easy in OpenCV and there are a number of ways to accomplish it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply a Gaussian blur with a 11x11 kernel to the image to smooth it,\n",
    "# useful when reducing high frequency noise\n",
    "blurred = cv2.GaussianBlur(image, (11, 11), 0)\n",
    "cv2.imshow(\"Blurred\", blurred)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing on an image\n",
    "\n",
    "We’re going to draw a rectangle, circle, and line on an input image. We’ll also overlay text on an image as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw a 2px thick red rectangle surrounding the face\n",
    "output = image.copy()\n",
    "# input image starting at x=500,y=400 at ending at x=650,y=550\n",
    "cv2.rectangle(output, (500, 400), (650, 550), (0, 0, 255), 2) # (500, 400): top-left; (650, 550): bottom-right; (0, 0, 255):线条颜色；2:线条粗细，(a negative value will make a solid rectangle).\n",
    "cv2.imshow(\"Rectangle\", output)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Since we are using OpenCV’s functions rather than NumPy operations \n",
    "# we can supply our coordinates in (x, y) order rather than (y, x) \n",
    "# since we are not manipulating or accessing the NumPy array directly \n",
    "# — OpenCV is taking care of that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw a blue 20px (filled in) circle on the image centered at\n",
    "# x=300,y=150\n",
    "output = image.copy()\n",
    "cv2.circle(output, (300, 150), 20, (255, 0, 0), -1) # (300, 150)：circle's center coorfinate; 20: circle radius\n",
    "cv2.imshow(\"Circle\", output)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw a 5px thick red line from x=60,y=20 to x=400,y=200\n",
    "output = image.copy()\n",
    "cv2.line(output, (60, 20), (400, 200), (0, 0, 255), 5)\n",
    "cv2.imshow(\"Line\", output)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# draw green text on the image\n",
    "# available fonts: https://docs.opencv.org/3.4.1/d0/de1/group__core.html#ga0f9314ea6e35f99bb23f29567fc16e11\n",
    "output = image.copy()\n",
    "cv2.putText(output, \"OpenCV + TITANIC!!!\", (10, 25), # (10, 25):start point;\n",
    "\tcv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 5)  # cv2.FONT_HERSHEY_SIMPLEX:font; 0.7: font size;(0, 255, 0): text color; 2: The thickness of the stroke in pixels.\n",
    "cv2.imshow(\"Text\", output)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting objects\n",
    "\n",
    " count the number of Tetris blocks in the following image\n",
    " ![](tetris_blocks.webp)\n",
    "\n",
    "Along the way we’ll be:\n",
    "* Learning how to convert images to grayscale with OpenCV\n",
    "* Performing edge detection\n",
    "* Thresholding a grayscale image\n",
    "* Finding, counting, and drawing contours\n",
    "* Conducting erosion and dilation\n",
    "* Masking an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import argparse # a command line arguments parsing package which comes with all installations of Python.\n",
    "import imutils\n",
    "import cv2\n",
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--image\", required=True,\n",
    "\thelp=\"path to input image\")\n",
    "# args = vars(ap.parse_args()) # 命令行执行脚本传参数时使用\n",
    "args = vars(ap.parse_args(['--image', 'tetris_blocks.webp'])) # 用在jupyter中\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting an image to grayscale\n",
    "\n",
    "We’re going to be thresholding and detecting edges in the image shortly. Therefore we convert the image to grayscale\n",
    "\n",
    "![](opencv_tetris_gray.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the input image (whose path was supplied via command line\n",
    "# argument) and display the image to our screen\n",
    "image = cv2.imread(args[\"image\"])\n",
    "cv2.imshow(\"Image\", image)\n",
    "cv2.waitKey(0)\n",
    "# convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imshow(\"Gray\", gray)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edge detection\n",
    "\n",
    "Edge detection is useful for finding boundaries of objects in an image — it is effective for segmentation purposes.\n",
    "\n",
    "![](opencv_tetris_edge.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying edge detection we can find the outlines of objects in images\n",
    "# cv2.Canny(img, minVal, maxVal, aperture_size)\n",
    "# aperture_size : The Sobel kernel size. By default this value is 3 and hence is not shown \n",
    "# 30: A minimum threshold, 150: The maximum threshold\n",
    "# Different values for the minimum and maximum thresholds will return different edge maps.\n",
    "edged = cv2.Canny(gray, 30, 150) # Using the popular Canny algorithm (developed by John F. Canny in 1986), we can find the edges in the image.\n",
    "cv2.imshow(\"Edged\", edged)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thresholding\n",
    "\n",
    "[Offical thresholding doc](https://docs.opencv.org/3.4.0/d7/d1b/group__imgproc__misc.html#ggaa9e58d2860d4afa658ef70a9b1115576a19120b1a11d8067576cc24f4d2f03754)\n",
    "\n",
    "Image thresholding is an important intermediary step for image processing pipelines. Thresholding can help us to remove lighter or darker regions and contours of images.\n",
    "\n",
    "Segmenting foreground from background with a binary image is critical to finding contours (our next step).\n",
    "\n",
    "![](opencv_tetris_thresh.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# threshold the image by setting all pixel values less than 225\n",
    "# to 255 (white; foreground) and all pixel values >= 225 to 0\n",
    "# (black; background), thereby segmenting the image\n",
    "thresh = cv2.threshold(gray, 225, 255, cv2.THRESH_BINARY_INV)[1]\n",
    "cv2.imshow(\"Thresh\", thresh)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting and drawing contours\n",
    "\n",
    "![](opencv_tetris_contours.gif)\n",
    "![](opencv_tutorial_object_counting.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find contours (i.e., outlines) of the foreground(white) objects in the thresholded image\n",
    "cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\n",
    "\tcv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts) # This is very important accounting for the fact that cv2.findContours implementation changed between OpenCV 2.4, OpenCV 3, and OpenCV 4. This compatibility line is present on the blog wherever contours are involved.\n",
    "output = image.copy() # copy of the original image so that we can draw contours\n",
    "# loop over the contours\n",
    "for c in cnts:\n",
    "\t# draw each contour on the output image with a 3px thick purple\n",
    "\t# outline, then display the output contours one at a time\n",
    "\tcv2.drawContours(output, [c], -1, (240, 0, 159), 3) # (240, 0, 159): 紫色\n",
    "\tcv2.imshow(\"Contours\", output)\n",
    "\tcv2.waitKey(0)\n",
    "\t\n",
    "# draw the total number of contours found in purple\n",
    "text = \"I found {} objects!\".format(len(cnts))\n",
    "cv2.putText(output, text, (10, 25),  cv2.FONT_HERSHEY_SIMPLEX, 0.7,\n",
    "\t(240, 0, 159), 2)\n",
    "cv2.imshow(\"Contours\", output)\n",
    "cv2.waitKey(0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erosions and dilations\n",
    "\n",
    "Erosions and dilations are typically used to reduce noise in binary images (a side effect of thresholding).\n",
    "\n",
    "To reduce the size of foreground objects we can erode away pixels given a number of iterations\n",
    "\n",
    "![](opencv_tutorial_tetris_erosion.jpg)\n",
    "\n",
    "![](opencv_tutorial_tetris_dilation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using OpenCV we can erode contours, effectively making them smaller or causing them to disappear completely with sufficient iterations. This is typically useful for removing small blobs in mask image.\n",
    "# we apply erosions to reduce the size of foreground objects\n",
    "\n",
    "mask = thresh.copy()\n",
    "mask = cv2.erode(mask, None, iterations=5)\n",
    "cv2.imshow(\"Eroded\", mask)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In an image processing pipeline if you ever have the need to connect nearby contours, you can apply dilation to the image. Shown in the figure is the result of dilating contours with five iterations, but not to the point of two contours becoming one.\n",
    "# similarly, dilations can increase the size of the ground objects\n",
    "mask = thresh.copy()\n",
    "mask = cv2.dilate(mask, None, iterations=5)\n",
    "cv2.imshow(\"Dilated\", mask)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking and bitwise operations\n",
    "\n",
    "Masks allow us to “mask out” regions of an image we are uninterested in. We call them “masks” because they will hide regions of images we do not care about.\n",
    "\n",
    "![](opencv_tutorial_tetris_bitwise_masking.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a typical operation we may want to apply is to take our mask and\n",
    "# apply a bitwise AND to our input image, keeping only the masked\n",
    "# regions\n",
    "mask = thresh.copy()\n",
    "output = cv2.bitwise_and(image, image, mask=mask)\n",
    "cv2.imshow(\"Output\", output)\n",
    "cv2.waitKey(0)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMeNnL83KPhvIBPwrGwe6ZH",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
