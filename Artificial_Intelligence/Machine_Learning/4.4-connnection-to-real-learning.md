我们刚刚的推导一直都是对于某一个hypothesis的h来说，而真实的Learning算法则是从众多的hypothesis set中选择出g，对于不同的数据，选择出来的g肯定会不一样，也就是返回的超平面会不一样。而这时如果强制Learning的算法返回固定的h，由于不是Learning算法选择出来的g，那么抽样中的错误率通常都不会很小，也就是说这个h通常都不会和f 约等于。
所以什么是一个好的学习呢？

好的学习是算法选择一个h作为g，这个h的Ein(h)刚好又很小，那么就是一个好的学习了。但是如果一个学习算法强迫你选择一个h，那么很有可能不是一个好的学习。

真正的h需要做选择。
![](assets/图49.png)

不好的样本bad samples：Ein is far away Eout
有选择的时候，选择会恶化不好的情形。
Hoffding不等式告诉我们：
![](assets/图50.png)
![](assets/图51.PNG)
我们一把一把的抽，对于一个h，行中BAD的格很少，也就是 bad samples很少，就是随便抽一把都能预测整体的情况。
多个h的情况。首先什么是不好的样本？好的算法呢是可以自由选择h，那么不好的就是不能自由的选择，也就是可能对于某个D，h会是BAD的。所以对于一个假设集，只要有一列上有一个BAD那么这个资料就是不好的。那么对于所有假设集的 BAD DATA是什么呢？

![](assets/图51.jpg)

![](assets/图52.jpg)

如上式计算，得出不好的资料的概率，要是不好资料的概率很小，也就是任何资料都可以，那么我们有了两个条件：
- 可以随便选择资料；
- 可以自由选择h。
那么就得到了一个好的学习。


### The 'statistical' learning flow
![](assets/图52.PNG)
![](assets/图53.PNG)

所以机器学习是有可能做得到的。PLA的线有无限条，我们刚才证明的只是在h set是有限的情况下，但如果h是无限，该怎么办？

M= ∞ ？(like perceotrons)——see you in the next lectures
![](assets/图54.PNG)