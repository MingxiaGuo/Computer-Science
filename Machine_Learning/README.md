

# Content

* [Introduction](INTRODUCTION.md)
* a）When Can Machines Learn? (illustrative + technical)
    *  [1 The Learning Problem](1-The-Learning-Problem.md)
        * 1.0 Course Introduction
        * [1.1 What is Machine Learning](1.1-What-is-Machine-Learning.md)
        * [1.2 Applications of Machine Learning](1.2-applications-of-machine-learning.md)
        * [1.3 Components of Machine Learning](13-components-of-machine-learning.md)
        * [1.4 Machine Learning and Other Fields](1.4-Machine-Learning-and-Other-Fields.md)
    * [2 Learning to Answer Yes/No](2-learning-to-answer-yesno.md)
        * [2.1 Percetron Hypothesis Set](2.1-percetron-hypothesisi-set.md)
        * [2.2 Percetron Learning Algorithm: PLA](2.2-percetron-learning-algorithm.md)
        * [2.3 Guarantee of PLA](2.3-guarantee-of-pla.md)
        * [2.4 Non-Separable Data: Pocket Algorithm](2.4-non-separable-data.md)
    * [3 Types of Machine Learning](3-Types-of-Machine-Learning.md)
        * [3.1 Learning with Different Output Space Y](3.1-learning-with-different-output-space-y.md)
        * [3.2 Learning with DIfferent Data Label $$y_n$$](3.2-learning-with-different-data-label.md)
        * [3.3 Learning with Different Protocal f: $$f =(X_n,y_n)$$](3.3-learning-with-different-protocal-f.md)
        * [3.4 Learning with Different Input Space X](3.4-learning-with-different-input-space-x.md)
    * [4 Feasibility of Learning](4-feasibility-of-learning.md)
        * [4.1 Learning is Impossible?](4.1-learning-is-impossible.md)
        * [4.2 Probability to the Rescue](4.2-Probability-to-the-Rescue.md)
        * [4.3 Connection to Learning](4.3-connection-to-learning.md)
        * [4.4 Connnection to Real Learning](4.4-connnection-to-real-learning.md)
* b) Why Can Machines Learn?(theoretical + illustrative)
    * [5 Training versus Testing](5-training-versus-testing.md)
        * [5.1 Recap and Preview](5.1-recap-and-preview.md)
        * [5.2 Effective Number of Lines](5.2-effective-number-of-lines.md)
        * [5.3 Effective Number of Hypotheses](5.3-effective-number-of-hypotheses.md)
    * [6 Theory of Generalization](6-theory-of-generalization.md)
    * [7 The vc dimension](7-the-vc-dimension.md)
    * 8 The Noise and Error
    * [9 线性回归Linear Regression](9-xian-xing-hui-gui.md)
    * 10 逻辑回归Logisitic Regression
    * 11 Linear Models for Classification
    * 12 Nolinear Transform
    * 13 Hazard of Overfitting
    * 14 Regularization
    * 15 Validation
    * 16 Three Learning Principles

* c) How Can Machines Learn?(technical + practical)

* d) WHy Can Machines Learn Better?(practical + theoretical)

- Embedding Numerous Features: kernel Models
  - Lecture 1: Linear Support Vector Machine
    - Large-margin Separating Hyperplane
    - Standard Large-margin Problem
    - Support Vector Machine
    - Reasons behind Large-Margin Hyperplane
- Conbining Predictive Features: Aggregation Models
- Distilling Implicit Features: Extraction Models

* [机器学习技法](ji-qi-xue-xi-ji-fa.md)
* [Linear Support Vector Machine](支持向量机svm.md)
  * [Large-Margin Separating Hyperplane](支持向量机svm/large-margin-separating-hyperplane.md)
  * [Standard Large-Margin Problem](支持向量机svm/standard-large-margin-problem.md)
  * Support Vector Machine
  * Reasons behind Large-Margin Hyperplane
* DualSupportVectorMachine
* KernelSupportVectorMachine
* Soft-MarginSupportVectorMachine
* KernalLogisticRegression
* [支持向量回归SVR](zhi-chi-xiang-liang-hui-gui-svr.md)
* Blending and Bagging
* Adaptive Boosting
* 决策树
* [机器学习中的数学](机器学习中的数学.md)
  * [1. 法向量为什么是平面方程的系数](法向量为什么是平面方程的系数.md)
* 深度学习

