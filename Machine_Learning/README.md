# content

* When Can Machines Learn? \(illustrative + technical\)

  * Lecture 1: The Learning Problem

    * Course Introduction
    * What is Machine Learning
    * Applications of Machine Learning
    * Components of Machine Learning
    * Machine Learning and Other Fields

  * Lecture 2: Learning to Answer Yes\/No

    * Perceptron Hypothesis Set
    * **Perceptron Learning Algorithm 感知机算法**
    * Guarantee of PLA
    * Non-Separable Data: **Pocket Algorithm**

  * Lecture 3: Types of Learning

    * Learning with Different Output Space y
    * Learning with Different Data Label $$y_n$$
    * Learning with Different Protocol f =&gt; \($$X_n,y_n$$\)
    * Learning with Different Input Space x
    
  * Lecture 4: Feasibility of Learning
    * Learning is Impoosible?
    * Probability to the Tescue
    * Connection to Learning
    * Connection to Real Learning
* WHy Can Machines Learn?\(theoretical + illustrative\)

  - Lecture 5: Training versus Testing
    - Recap and Preview
    - Effective Number of Lines
    - Effective Number of Hypotheses
    - Break Point

* How Can Machines Learn?\(technical + practical\)

* WHy Can Machines Learn Better?\(practical + theoretical\)

- Embedding Numerous Features: kernel Models
  - Lecture 1: Linear Support Vector Machine
    - Large-margin Separating Hyperplane
    - Standard Large-margin Problem
    - Support Vector Machine
    - Reasons behind Large-Margin Hyperplane
- Conbining Predictive Features: Aggregation Models
- Distilling Implicit Features: Extraction Models

